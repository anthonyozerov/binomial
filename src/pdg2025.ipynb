{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdg\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from measurement_dist import measurement_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# point matplotlib ticks inwards\n",
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "# add top and right ticks\n",
    "# plt.rcParams[\"axes.spines.top\"] = True\n",
    "# plt.rcParams[\"axes.spines.right\"] = True\n",
    "plt.rcParams[\"xtick.top\"] = True\n",
    "plt.rcParams[\"ytick.right\"] = True\n",
    "# use tex\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdg_data import load_pdg_data\n",
    "pdg2025_stat_dfs, pdg2025_both_dfs, pdg2025_stat_quantities, pdg2025_both_quantities = load_pdg_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dist = measurement_dist(pdg2025_stat_dfs)\n",
    "both_dist = measurement_dist(pdg2025_both_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_dist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "zspace = stat_dist['zspace']\n",
    "colors = {'stat': 'grey', 'both': 'red'}\n",
    "dists = {'stat': stat_dist, 'both': both_dist}\n",
    "plt.plot(zspace, stat_dist['pair'], label='stat', color='grey')\n",
    "plt.plot(zspace, stat_dist['norm'], color='black', linestyle='dashed', linewidth=2, label='$|N(0,1)|$')\n",
    "# plt.text(0.95, 0.5, r'$z=\\frac{|y_1-y_2|}{\\sqrt{\\sigma_1^2+\\sigma_2^2}}$', transform=plt.gca().transAxes, ha='right')\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.xlim(0, 2.5)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$P(Z>z)$')\n",
    "plt.title('Empirical distribution of differences\\nbetween studies in 2025 PDG data')\n",
    "\n",
    "plt.savefig('figs/pdg_pairdists_stat.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.plot(zspace, both_dist['pair'], label='stat+syst', color='red')\n",
    "plt.legend(frameon=False)\n",
    "for sig in [0.5, 1, 2, 5]:\n",
    "    for which in ['stat', 'both']:\n",
    "        val = dists[which]['pair' + str(sig)][0]\n",
    "        ci = np.array([dists[which]['pair_boot_ci_' + str(sig)]]).T\n",
    "        ci[0,:] = val - ci[0,:]\n",
    "        ci[1,:] = ci[1,:] - val\n",
    "        plt.errorbar([sig], [val], yerr=ci, color=colors[which])\n",
    "\n",
    "\n",
    "plt.savefig('figs/pdg_pairdists_statvsboth.png', dpi=300, bbox_inches='tight')\n",
    "plt.xlim(0, 10)\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-4, 1)\n",
    "plt.savefig('figs/pdg_pairdists_statvsboth_full.png', dpi=300, bbox_inches='tight')\n",
    "plt.plot(zspace, both_dist['h'], label=r'stat+syst, $h$', color='lightblue')\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig('figs/pdg_pairdists_statvsboth_h.png', dpi=300, bbox_inches='tight')\n",
    "plt.plot(zspace, both_dist['hprime'], label=r'stat+syst, $h^\\prime$', color='blue')\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig('figs/pdg_pairdists_statvsboth_hprime.png', dpi=300, bbox_inches='tight')\n",
    "# plt.xlim(0, 2.5)\n",
    "# plt.yscale('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import birge, fixed_effect, random_effects_pm\n",
    "from collections import defaultdict\n",
    "\n",
    "widths = defaultdict(list)\n",
    "intervals_birge = []\n",
    "intervals_re = []\n",
    "for df in pdg2025_both_dfs:\n",
    "    values = np.array(df.value)\n",
    "    sigmas = np.array(df.uncertainty)\n",
    "\n",
    "    interval, _, _ = fixed_effect(values, sigmas, coverage=0.6827)\n",
    "    widths['fe'].append(interval[1]-interval[0])\n",
    "    interval, _, _, _ = birge(values, sigmas, coverage=0.6827, pdg=True)\n",
    "    widths['birge'].append(interval[1]-interval[0])\n",
    "    intervals_birge.append(interval)\n",
    "    interval, _, _, _ = random_effects_pm(values, sigmas, coverage=0.6827)\n",
    "    widths['pm'].append(interval[1]-interval[0])\n",
    "    intervals_re.append(interval)\n",
    "for key, value in widths.items():\n",
    "    widths[key] = np.array(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(widths['birge']/widths['fe'], widths['pm']/widths['fe'], color='black', s=1, alpha=0.3)\n",
    "plt.ylabel('Random Effects interval width / FE')\n",
    "plt.xlabel('Birge Ratio interval width / FE')\n",
    "plt.plot([0,20],[0,20], color='red')\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(0,10)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.savefig('figs/pdg2025_width_scatter.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.scatterplot(x=widths['pm']/widths['fe'], y=widths['birge']/widths['fe'], hue=data_types, s=5)\n",
    "# plt.gca().set_aspect('equal')\n",
    "# plt.xlim(0,10)\n",
    "# plt.ylim(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(widths['birge']/widths['fe'], widths['pm']/widths['birge'], color='black', s=1, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "width_ratios = widths['pm']/widths['birge']\n",
    "same = np.isclose(width_ratios,1)\n",
    "same_width_prop = np.mean(same)\n",
    "\n",
    "plt.hist(width_ratios[~np.isclose(width_ratios,1)], bins=np.linspace(0,5,50), color='grey')\n",
    "# data = {'widths': width_ratios[~np.isclose(width_ratios,1)], 'datatype': np.array(data_types)[~same]}\n",
    "# sns.histplot(data, x='widths', hue='datatype', multiple='stack', bins=np.linspace(0,5,50))\n",
    "plt.text(0.8, 0.5, f'Same width (not shown): {same_width_prop*100:2.1f}'+r'\\%', transform=plt.gca().transAxes, ha='right')\n",
    "plt.xlabel('Random Effects width / Birge Ratio width')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 5)\n",
    "plt.savefig('figs/pdg2025_width_hist.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(width_ratios))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
    "pdgids = []\n",
    "for i, idx in enumerate([np.argmin(width_ratios), np.argmax(width_ratios)]):\n",
    "    ax = axs[i]\n",
    "    df = pdg2025_both_dfs[idx]\n",
    "    pdgids.append(df['pdgid'].iloc[0])\n",
    "    ax.errorbar(df['value'], np.arange(len(df)), xerr=df['uncertainty'], color='black', fmt='.')\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    # ax.set_xticks([])\n",
    "    \n",
    "    ax.axvspan(intervals_re[idx][0], intervals_re[idx][1], color='lightgrey')\n",
    "    ax.axvspan(intervals_birge[idx][0], intervals_birge[idx][1], color='red', alpha=0.2, linewidth=0)\n",
    "# print(descriptions)\n",
    "axs[0].set_title(r'RE $\\ll$ Birge'+'\\n'+pdgids[0])\n",
    "axs[1].set_title(r'Birge $\\ll$ RE'+'\\n'+pdgids[1])\n",
    "plt.savefig('figs/pdg2025_br_re_extremes.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdg_methods import birge_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.abs(wm1s-wm0s)/meanerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wm0s[1141])\n",
    "print(wm1s[1141])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg2025_both_dfs[1141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "norm.cdf(-5,0,1.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect(\"data/pdgall-2025-v0.2.1.sqlite\")\n",
    "cur = con.cursor()\n",
    "res = cur.execute(\"SELECT * FROM pdgdoc WHERE table_name='PDGID' AND column_name='DATA_TYPE'\")\n",
    "data = res.fetchall()\n",
    "datatype_map = {row[3]: row[5] for row in data}\n",
    "datatype_map[''] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "df_groups = defaultdict(list)\n",
    "data_types = []\n",
    "for i, df in enumerate(pdg2025_both_dfs):\n",
    "    data_type = df['data_type'].iloc[0]\n",
    "    df_groups[data_type].append(df)\n",
    "    data_types.append(datatype_map[data_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = {}\n",
    "for data_type, dfs in df_groups.items():\n",
    "    dists[data_type] = measurement_dist(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "zspace = dists['G']['zspace']\n",
    "for data_type, dist in dists.items():\n",
    "    plt.plot(zspace, dist['pair'], label=datatype_map[data_type])\n",
    "plt.plot(zspace, dists['G']['norm'], color='black')\n",
    "plt.xlim(0, 2.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(pdg2025_both_dfs)):\n",
    "    # drop rows where stat_error_positive or stat_error_negative is NaN\n",
    "    df = pdg2025_both_dfs[i].dropna(subset=['stat_error_positive', 'stat_error_negative'])\n",
    "    # drop rows where error_positive or error_negative is NaN\n",
    "    df = df.dropna(subset=['error_positive', 'error_negative'])\n",
    "    # apply to the pdg2025_both_dfs\n",
    "    pdg2025_both_dfs[i] = df\n",
    "\n",
    "pdg2025_both_dfs[20]\n",
    "\n",
    "props = []\n",
    "techniques = []\n",
    "data_types = []\n",
    "years = []\n",
    "descriptions = []\n",
    "for df in pdg2025_both_dfs:\n",
    "    props += list(df['stat_error_positive']**2/df['error_positive']**2)\n",
    "    props += list(df['stat_error_negative']**2/df['error_negative']**2)\n",
    "    techniques += list(df['technique'])*2\n",
    "    years += list(df['year'])*2\n",
    "    data_types += list(df['data_type'])*2\n",
    "    descriptions += list(df['pdgid.description'])*2\n",
    "data_types = [datatype_map[dt] for dt in data_types]\n",
    "props = np.array(props)\n",
    "years = np.array(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg2025_stat_dfs[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "techniques = pd.Series(techniques)\n",
    "top_categories = techniques.value_counts().nlargest(9).index\n",
    "data_types = pd.Series(data_types)\n",
    "\n",
    "techniques_other = techniques.where(techniques.isin(top_categories), 'Other')\n",
    "data = {'prop': props, 'technique': techniques_other, 'data_type': data_types, 'year': years, 'description': descriptions}\n",
    "df = pd.DataFrame(data)\n",
    "# sns.histplot(data=df, x='prop', hue='technique', multiple=\"stack\", bins=51)\n",
    "# sns.histplot(data=df[~df['data_type'].isin(['branching ratio', 'Other'])], x='prop', hue='data_type', multiple=\"stack\", bins=21)\n",
    "sns.histplot(data=df, x='prop', hue='data_type', multiple=\"stack\", bins=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['prop'], df['year'], s=2, alpha=0.1, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "syst = np.random.uniform(0, 1, 100000)\n",
    "stat = np.random.uniform(0, 1, 100000)\n",
    "\n",
    "plt.hist(stat**2 / (stat**2 + syst**2), bins=101)\n",
    "plt.xlim(0.4, 0.6)\n",
    "plt.show()\n",
    "\n",
    "# round, keeping 2 decimal places if below 0.355, 1 decimal place if below 0.950, and round to 1 otherwise\n",
    "def round_err(x):\n",
    "    x[x < 0.355] = np.round(x[x < 0.355], 2)\n",
    "    x[(x>=0.355) & (x < 0.950)] = np.round(x[(x>=0.355) & (x < 0.950)], 1)\n",
    "    x[x >= 0.950] = np.round(x[x >= 0.950], 0)\n",
    "    return x\n",
    "\n",
    "syst = round_err(syst)\n",
    "stat = round_err(stat)\n",
    "\n",
    "plt.hist(stat**2 / (stat**2 + syst**2), bins=101)\n",
    "plt.xlim(0.4, 0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['data_type'] == 'Other']['description'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.hist(props, bins=101, range=(0, 1), density=True, color='grey')\n",
    "# plt.xlim(0.4, 0.6)\n",
    "plt.title('Proportion of variance which is statistical in PDG data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(props==0.5)\n",
    "props[(props<0.5-1e-10) & (props>0.49)]\n",
    "# np.mean((props<0.45) & (props>0.44))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg2025_both_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api = pdg.connect(\"sqlite:///data/pdgall-2025-v0.2.0.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api.doc_value_type_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"data/pdgall-2025-v0.2.0.sqlite\")\n",
    "cur = con.cursor()\n",
    "command = \"\"\"\n",
    "SELECT pdgid.description, pdgmeasurement.pdgid, pdgdata.value_type, pdgdata.in_summary_table, pdgdata.value, pdgmeasurement_values.value, pdgmeasurement_values.error_positive, pdgmeasurement_values.error_negative\n",
    "FROM pdgmeasurement_values\n",
    "     JOIN pdgmeasurement ON pdgmeasurement.id = pdgmeasurement_values.pdgmeasurement_id\n",
    "     JOIN pdgid ON pdgid.id = pdgmeasurement.pdgid_id\n",
    "     JOIN pdgdata ON pdgdata.pdgid_id = pdgid.id\n",
    "--     JOIN pdgparticle ON pdgparticle.pdgid = pdgid.parent_pdgid\n",
    "WHERE pdgmeasurement_values.used_in_average AND pdgmeasurement_values.value IS NOT NULL AND pdgdata.edition = '2025' AND pdgdata.value_type = 'AC'\n",
    "\"\"\"\n",
    "res = cur.execute(command)\n",
    "data = res.fetchall()  # WHERE\n",
    "columns = [col[0] for col in res.description]\n",
    "print(len(data), \"measurements\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.execute(\"SELECT * FROM pdgmeasurement\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"pdgid.description\",\n",
    "        \"pdgid\",\n",
    "        \"type\",\n",
    "        \"insummary\",\n",
    "        \"avg\",\n",
    "        \"measurement\",\n",
    "        \"error_positive\",\n",
    "        \"error_negative\",\n",
    "    ],\n",
    ")\n",
    "df[\"error\"] = (df[\"error_positive\"] + df[\"error_negative\"]) / 2\n",
    "df[\"std_resid\"] = (df[\"measurement\"] - df[\"avg\"]) / df[\"error\"]\n",
    "# only keep rows where there are at least 3 measurements\n",
    "df = df.groupby(\"pdgid\").filter(lambda x: len(x) >= 3)\n",
    "print(\"Number of properties:\", len(df[\"pdgid\"].unique()))\n",
    "print(\"Number of measurements:\", len(df))\n",
    "\n",
    "\n",
    "# for each pdgid, do some operations on each row with that pdgid\n",
    "def process_group(group):\n",
    "    n = len(group)\n",
    "    sigma = np.array(group[\"error\"])\n",
    "    sigma2 = sigma**2\n",
    "    # sigma2 = np.ones(n)\n",
    "\n",
    "    S = np.sum(1 / sigma2)\n",
    "\n",
    "    Xbar = np.sum(group[\"measurement\"] / sigma2) / S\n",
    "    # print(Xbar, group['avg'].iloc[0])\n",
    "    std = np.sqrt(sigma2 * (1 - 1 / (sigma2 * S)) ** 2 + (S - 1 / sigma2) / (S**2))\n",
    "    # print(std)\n",
    "    group[\"std_resid_adj\"] = (group[\"measurement\"] - group[\"avg\"]) / std\n",
    "    # group['std_resid_adj'] = (group['measurement'] - group['avg']) / sigma\n",
    "    # print(group)\n",
    "    return group\n",
    "\n",
    "\n",
    "# process_group(df[df['pdgid'] == 'Q007TP'])\n",
    "df_gb = df.groupby(\"pdgid\", group_keys=False)\n",
    "dfs = [df_gb.get_group(x) for x in df_gb.groups]\n",
    "df = df.groupby(\"pdgid\").apply(process_group, include_groups=False)\n",
    "df\n",
    "# df = df[df['pdgid.description'].str.contains('MASS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import birge, random_effects_dl_base, random_effects_mle, I2, errscale_test\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "brs = []\n",
    "taus = []\n",
    "I2s = []\n",
    "errscale_ps = []\n",
    "brs_cont = []\n",
    "taus_cont = []\n",
    "I2s_cont = []\n",
    "errscale_ps_cont = []\n",
    "# birge_logprobs = []\n",
    "# re_logprobs = []\n",
    "# fe_logprobs = []\n",
    "# mix_logprobs = []\n",
    "bad = [\n",
    "    \"M047R7\",\n",
    "    \"M002R19\",\n",
    "    \"M049R52\",\n",
    "    \"M055R6\",\n",
    "    \"M053R02\",\n",
    "    \"M052R4\",\n",
    "    \"M056R4\",\n",
    "    \"M057R4\",\n",
    "    \"M070R24\",\n",
    "    \"M070R50\",\n",
    "    \"M070R60\",\n",
    "    \"M070R7\",\n",
    "    \"M070R82\",\n",
    "    \"M070R83\",\n",
    "    \"M070R84\",\n",
    "    \"M070R86\",\n",
    "    \"M070R87\",\n",
    "    \"M070R9\",\n",
    "    \"M070S6\",\n",
    "    \"M071R22\",\n",
    "    \"M071R28\",\n",
    "    \"M071S10\",\n",
    "    \"S040R11\",\n",
    "    \"S041B24\",\n",
    "    \"S041B41\",\n",
    "    \"S041C5\",\n",
    "    \"S041R3\",\n",
    "    \"S041R39\",\n",
    "    \"S041R90\",\n",
    "    \"S041S47\",\n",
    "    \"S041R65\",\n",
    "    \"S041S50\",\n",
    "    \"S041T03\",\n",
    "    \"S042B26\",\n",
    "    \"S042B27\",\n",
    "    \"S042B43\",\n",
    "    \"S042B47\",\n",
    "    \"S042B58\",\n",
    "    \"S042P59\",\n",
    "    \"S042R2\",\n",
    "    \"S042R20\",\n",
    "    \"S042R22\",\n",
    "    \"S042R23\",\n",
    "    \"S042R3\",\n",
    "    \"S042R47\",\n",
    "    \"S042R48\",\n",
    "    \"S042S24\",\n",
    "    \"S042S59\",\n",
    "    \"S049R21\",\n",
    "    \"S049S7\",\n",
    "    \"S049R24\",\n",
    "    \"S042S88\",\n",
    "    \"S086R3\",\n",
    "    \"S086R33\",\n",
    "    \"S086R32\",\n",
    "    \"S086R8\",\n",
    "    \"S086R34\",\n",
    "    \"S086R6\",\n",
    "]\n",
    "\n",
    "birge_loglikes = []\n",
    "re_loglikes = []\n",
    "fe_loglikes = []\n",
    "ns = []\n",
    "\n",
    "for i, property in tqdm(enumerate(dfs), total=len(dfs)):\n",
    "    if property[\"pdgid\"].iloc[0] in bad:\n",
    "        continue\n",
    "    values = np.array(property[\"measurement\"])\n",
    "    sigmas = np.array(property[\"error\"])\n",
    "    # values = values-np.mean(values)\n",
    "    scaler = np.std(values)\n",
    "    if scaler == 0:\n",
    "        continue\n",
    "    values = values / scaler\n",
    "    sigmas = sigmas / scaler\n",
    "\n",
    "    # sigmas = sigmas/np.mean(sigmas)\n",
    "    _, muhat_birge, _, chat = birge(values, sigmas, coverage=0.6827)\n",
    "    brs.append(chat)\n",
    "    mean_sigma = np.mean(sigmas)\n",
    "    # muhat_re, _, tau = random_effects_dl_base(values, sigmas)\n",
    "    # taus.append(np.mean(tau/sigmas))\n",
    "    _, muhat_re, _, tau = random_effects_mle(values, sigmas, coverage=0.6827)\n",
    "    taus.append(np.mean(tau / sigmas))\n",
    "    I2s.append(I2(values, sigmas))\n",
    "\n",
    "    # generate values with same sigmas but no unaccounted for errors.\n",
    "    # to be used as a control when analyzing the distribution of chat and tau\n",
    "    values_control = np.random.normal(loc=0, scale=sigmas)\n",
    "    _, _, _, chat_cont = birge(values_control, sigmas, coverage=0.6827)\n",
    "    brs_cont.append(chat_cont)\n",
    "    _, _, _, tau_cont = random_effects_mle(values_control, sigmas, coverage=0.6827)\n",
    "    taus_cont.append(np.mean(tau_cont / sigmas))\n",
    "    I2s_cont.append(I2(values_control, sigmas))\n",
    "\n",
    "    # errscale_ps.append(errscale_test(values, sigmas))\n",
    "    # errscale_ps_cont.append(errscale_test(values_control, sigmas))\n",
    "\n",
    "    birge_loglikes.append(\n",
    "        np.log(np.prod(norm.pdf(values, loc=muhat_birge, scale=sigmas * chat)))\n",
    "    )\n",
    "    if any(np.array(birge_loglikes) == -np.inf):\n",
    "        print(i)\n",
    "        break\n",
    "    re_loglikes.append(\n",
    "        np.log(\n",
    "            np.prod(norm.pdf(values, loc=muhat_re, scale=np.sqrt(sigmas**2 + tau**2)))\n",
    "        )\n",
    "    )\n",
    "    fe_loglikes.append(np.log(np.prod(norm.pdf(values, loc=muhat_birge, scale=sigmas))))\n",
    "    ns.append(len(property))\n",
    "    # birge_probs = []\n",
    "    # re_probs = []\n",
    "    # fe_probs = []\n",
    "    # mix_probs = []\n",
    "\n",
    "    # for j in range(400):\n",
    "    #     spike = np.random.rand() < 0.5\n",
    "    #     if spike:\n",
    "    #         br = 1\n",
    "    #         tau = 0\n",
    "    #     else:\n",
    "    #         br = np.random.exponential(1)+1\n",
    "    #         tau = np.random.exponential(1)\n",
    "    #     mu = np.random.standard_cauchy()\n",
    "    #     birge_probs.append(np.prod(norm.pdf(values, loc=mu, scale=sigmas*br)))\n",
    "    #     # if np.any(np.log(norm.pdf(values, loc=mu, scale=sigmas*br))==-np.inf):\n",
    "    #     #     print(i)\n",
    "    #     #     print(property)\n",
    "    #     #     print('BR:', br)\n",
    "    #     #     print(mu)\n",
    "    #     #     print(values)\n",
    "    #     #     print(sigmas*br)\n",
    "    #     #     print(norm.pdf(values, loc=mu, scale=sigmas*br))\n",
    "    #     #     raise ValueError(\"Log probability is -inf, check values and sigmas.\")\n",
    "    #     # print(norm.pdf(values, loc=mu, scale=sigmas*br))\n",
    "    #     re_probs.append(np.prod(norm.pdf(values, loc=mu, scale=np.sqrt(sigmas**2+tau**2))))\n",
    "    #     fe_probs.append(np.prod(norm.pdf(values, loc=mu, scale=sigmas)))\n",
    "\n",
    "    #     mix_probs.append(np.prod(norm.pdf(values, loc=mu, scale=np.sqrt((br*sigmas)**2 + tau**2))))\n",
    "    # if np.mean(birge_probs) == 0:\n",
    "    #     print(i, property)\n",
    "    #     raise ValueError(\"Mean of birge_probs is zero, check values and sigmas.\")\n",
    "\n",
    "    # birge_logprobs.append(np.log(np.mean(birge_probs)))\n",
    "    # re_logprobs.append(np.log(np.mean(re_probs)))\n",
    "    # fe_logprobs.append(np.log(np.mean(fe_probs)))\n",
    "    # mix_logprobs.append(np.log(np.mean(mix_probs)))\n",
    "\n",
    "# birge_logprobs = np.array(birge_logprobs)\n",
    "# re_logprobs = np.array(re_logprobs)\n",
    "# fe_logprobs = np.array(fe_logprobs)\n",
    "birge_loglikes = np.array(birge_loglikes)\n",
    "re_loglikes = np.array(re_loglikes)\n",
    "fe_loglikes = np.array(fe_loglikes)\n",
    "ns = np.array(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(birge_logprobs - re_logprobs, bins=100, color='grey')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(I2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(birge_loglikes - re_loglikes, bins=100, color=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs[np.argmax(birge_logprobs - re_logprobs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('log probabilities')\n",
    "# print(np.sum(birge_logprobs))\n",
    "# print(np.sum(re_logprobs))\n",
    "# print(np.sum(fe_logprobs))\n",
    "# print(np.sum(mix_logprobs))\n",
    "print(\"log likelihoods\")\n",
    "loglikes = np.array([np.sum(birge_loglikes), np.sum(re_loglikes), np.sum(fe_loglikes)])\n",
    "print(loglikes)\n",
    "birge_bics = 2 * np.log(ns) - 2 * birge_loglikes\n",
    "re_bics = 2 * np.log(ns) - 2 * re_loglikes\n",
    "fe_bics = 1 * np.log(ns) - 2 * fe_loglikes\n",
    "birge_aics = 2 * 2 - 2 * birge_loglikes\n",
    "re_aics = 2 * 2 - 2 * re_loglikes\n",
    "fe_aics = 2 * 1 - 2 * fe_loglikes\n",
    "\n",
    "col1 = loglikes\n",
    "col2 = np.array([np.mean(birge_bics), np.mean(re_bics), np.mean(fe_bics)])\n",
    "col3 = np.array([np.mean(birge_aics), np.mean(re_aics), np.mean(fe_aics)])\n",
    "colnames = [\"log-likelihood\", \"BIC\", \"AIC\"]\n",
    "rownames = [\"Birge Ratio\", \"Random Effects\", \"Fixed Effects\"]\n",
    "df = pd.DataFrame(np.array([col1, col2, col3]).T, columns=colnames, index=rownames)\n",
    "\n",
    "# print in latex format\n",
    "print(df.to_latex(index=True, float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    birge_loglikes, re_loglikes, marker=\".\", s=4, edgecolor=\"none\", c=ns, vmin=0\n",
    ")\n",
    "ymin = np.min([np.min(birge_loglikes), np.min(re_loglikes)])\n",
    "ymax = np.max([np.max(birge_loglikes), np.max(re_loglikes)])\n",
    "plt.plot(\n",
    "    [ymin - 1, ymax + 1], [ymin - 1, ymax + 1], color=\"red\", linewidth=1, linestyle=\":\"\n",
    ")\n",
    "# set aspect ratio to 1\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.xlim(ymin - 0.2, ymax + 0.2)\n",
    "plt.ylim(ymin - 0.2, ymax + 0.2)\n",
    "plt.xlabel(\"Birge Ratio log-likelihood\")\n",
    "plt.ylabel(\"Random Effects log-likelihood\")\n",
    "plt.colorbar(label=\"Number of measurements\")\n",
    "re_better_percent = np.round(np.mean(re_loglikes > birge_loglikes) * 100, 1)\n",
    "birge_better_percent = np.round(np.mean(birge_loglikes > re_loglikes) * 100, 1)\n",
    "plt.text(\n",
    "    0.1,\n",
    "    0.9,\n",
    "    f\"Random Effects better ({re_better_percent}\\\\%)\",\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"black\",\n",
    "    ha=\"left\",\n",
    "    va=\"top\",\n",
    ")\n",
    "plt.text(\n",
    "    0.9,\n",
    "    0.1,\n",
    "    f\"Birge Ratio better ({birge_better_percent}\\\\%)\",\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"black\",\n",
    "    ha=\"right\",\n",
    "    va=\"bottom\",\n",
    ")\n",
    "plt.title(\"Random Effects and Birge Ratio \\n MLE log-likelihoods for each property\")\n",
    "plt.savefig(\"figs/pdg_loglike.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.hist(re_loglikes - birge_loglikes, bins=100, density=True, color=\"grey\")\n",
    "plt.axvline(0, color=\"black\")\n",
    "plt.axvline(np.mean(re_loglikes - birge_loglikes), color=\"red\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.log(ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(re_loglikes == birge_loglikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(re_loglikes > birge_loglikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2 * len(ns) * np.log(np.sum(ns)) - 2 * np.sum(birge_loglikes))\n",
    "print(2 * len(ns) * np.log(np.sum(ns)) - 2 * np.sum(re_loglikes))\n",
    "print(1 * len(ns) * np.log(np.sum(ns)) - 2 * np.sum(fe_loglikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(errscale_ps)[np.array(I2s) > 0])\n",
    "plt.show()\n",
    "plt.hist(np.array(errscale_ps_cont)[np.array(I2s_cont) > 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "brs = np.array(brs)\n",
    "taus = np.array(taus)\n",
    "brs_cont = np.array(brs_cont)\n",
    "taus_cont = np.array(taus_cont)\n",
    "\n",
    "brs_big = brs[brs > 1]\n",
    "taus_big = taus[taus > 0]\n",
    "brs_cont_big = brs_cont[brs_cont > 1]\n",
    "taus_cont_big = taus_cont[taus_cont > 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].hist(\n",
    "    brs_big, range=(1, 4), bins=30, color=\"grey\", label=\"PDG data\"\n",
    ")  # , weights=np.ones(len(brs_big))/len(brs_big))\n",
    "axs[0].hist(\n",
    "    brs_cont_big,\n",
    "    range=(1, 4),\n",
    "    bins=30,\n",
    "    color=\"black\",\n",
    "    histtype=\"step\",\n",
    "    label=f\"Control experiment\\n(no systematics)\\n(${int(np.mean(brs_cont==1)*100)}\\%=1$)\",\n",
    ")\n",
    "axs[0].set_title(\n",
    "    rf\"Non-unity Birge ratios within each property (${int(np.mean(brs==1)*100)}\\%=1$)\"\n",
    ")\n",
    "axs[0].set_xlim(1, 4)\n",
    "axs[0].set_xlabel(r\"Estimated Birge ratio of a property\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "axs[0].legend(frameon=False)\n",
    "\n",
    "axs[1].hist(\n",
    "    taus_big, range=(0, 3), bins=30, color=\"grey\", label=\"PDG data\"\n",
    ")  # , weights=np.ones(len(taus_big))/len(taus_big))\n",
    "axs[1].hist(\n",
    "    taus_cont_big,\n",
    "    range=(0, 3),\n",
    "    bins=30,\n",
    "    color=\"black\",\n",
    "    histtype=\"step\",\n",
    "    label=f\"Control experiment\\n(no systematics)\\n(${int(np.mean(taus_cont==0)*100)}\\%=0$)\",\n",
    ")\n",
    "axs[1].set_title(\n",
    "    rf\"Non-zero mean ratios $\\hat\\tau/\\sigma_i$ within each property (${int(np.mean(taus==0)*100)}\\%=0$)\"\n",
    ")\n",
    "axs[1].set_xlim(0, 3)\n",
    "axs[1].set_xlabel(r\"Mean ratio $\\hat\\tau/\\sigma_i$ within a property\")\n",
    "axs[1].set_ylabel(\"Count\")\n",
    "axs[1].legend(frameon=False)\n",
    "\n",
    "plt.savefig(\"figs/pdg_birge_re.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "brs_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(brs) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(taus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(taus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(taus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(taus)\n",
    "dfs[1231]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdgid.description\"][df[\"pdgid.description\"].str.contains(\"MASS\")].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"limit\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"std_resid\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    df[\"std_resid_adj\"],\n",
    "    bins=100,\n",
    "    range=(-5, 5),\n",
    "    density=True,\n",
    "    color=\"grey\",\n",
    "    label=\"Standardized residuals\",\n",
    ")\n",
    "# plot normal pdf\n",
    "from scipy.stats import norm\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.xlim(-5, 5)\n",
    "plt.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.plot(x, norm.pdf(x, 0, 1), color=\"red\", label=\"Standard Normal PDF\")\n",
    "plt.title(\"Standardized residuals of PDG measurements\")\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(\"figs/pdg_std_residuals.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## qq plot\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot(df[\"std_resid_adj\"])\n",
    "plt.ylim(-7, 7)\n",
    "plt.xlim(-7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks test\n",
    "from scipy.stats import kstest\n",
    "\n",
    "ks_stat, ks_pvalue = kstest(df[\"std_resid_adj\"], \"norm\")\n",
    "ks_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "data = np.random.normal(0, 1, (100000, n))\n",
    "avg = np.mean(data, axis=1)\n",
    "resid = data - avg[:, None]\n",
    "plt.hist(resid.flatten(), bins=100, range=(-5, 5), density=True)\n",
    "plt.plot(x, norm.pdf(x, 0, np.sqrt((n - 1) / n)), color=\"red\", label=\"Normal PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM pdgdata\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle = api.get_particle_by_name(\"t\")\n",
    "measurement = list(particle.mass_measurements())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(particle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"data/pdgall-2025-v0.2.0.sqlite\")\n",
    "cur = con.cursor()\n",
    "command = \"\"\"\n",
    "SELECT pdgid.description, pdgmeasurement.pdgid, pdgdata.value_type, pdgdata.in_summary_table, pdgdata.value, pdgmeasurement_values.value, pdgmeasurement_values.error_positive, pdgmeasurement_values.error_negative\n",
    "FROM pdgmeasurement_values\n",
    "     JOIN pdgmeasurement ON pdgmeasurement.id = pdgmeasurement_values.pdgmeasurement_id\n",
    "     JOIN pdgid ON pdgid.id = pdgmeasurement.pdgid_id\n",
    "     JOIN pdgdata ON pdgdata.pdgid_id = pdgid.id\n",
    "--     JOIN pdgparticle ON pdgparticle.pdgid = pdgid.parent_pdgid\n",
    "WHERE pdgmeasurement_values.value IS NOT NULL AND pdgdata.edition = '2025'\n",
    "\"\"\"\n",
    "res = cur.execute(command)\n",
    "data = res.fetchall()  # WHERE\n",
    "columns = [col[0] for col in res.description]\n",
    "print(len(data), \"measurements\")\n",
    "print(columns)\n",
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"pdgid.description\",\n",
    "        \"pdgid\",\n",
    "        \"type\",\n",
    "        \"insummary\",\n",
    "        \"avg\",\n",
    "        \"measurement\",\n",
    "        \"error_positive\",\n",
    "        \"error_negative\",\n",
    "    ],\n",
    ")\n",
    "df[\"error\"] = (df[\"error_positive\"] + df[\"error_negative\"]) / 2\n",
    "df[\"std_resid\"] = (df[\"measurement\"] - df[\"avg\"]) / df[\"error\"]\n",
    "# only keep rows where there are at least 3 measurements\n",
    "df = df.groupby(\"pdgid\").filter(lambda x: len(x) >= 3)\n",
    "print(\"Number of properties:\", len(df[\"pdgid\"].unique()))\n",
    "print(\"Number of measurements:\", len(df))\n",
    "df_gb = df.groupby(\"pdgid\", group_keys=False)\n",
    "dfs = [df_gb.get_group(x) for x in df_gb.groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = []\n",
    "for df in dfs:\n",
    "    ns.append(len(df))\n",
    "biggest = np.argpartition(ns, -10)[-10:]\n",
    "for idx in biggest:\n",
    "    print(dfs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[np.argmax(ns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "from pymc_extras.inference import fit_dadvi\n",
    "import random\n",
    "# to_include=100\n",
    "to_include = np.inf\n",
    "with pm.Model() as model:\n",
    "    N = min(len(pdg2025_both_dfs), to_include)\n",
    "    # dataset_perm = random.sample(pdg2025_both_dfs, N)\n",
    "    dataset_perm = pdg2025_both_dfs\n",
    "\n",
    "    all_ys = []\n",
    "    all_sigmas = []\n",
    "    ds_idx = []\n",
    "    scales = []\n",
    "    for i, df in enumerate(dataset_perm):\n",
    "        # scale = np.median(df['uncertainty'])\n",
    "        scale = np.mean(df['uncertainty'])\n",
    "        # shift = np.median(df['value'])\n",
    "        shift = np.mean(df['value'])\n",
    "        ys = (np.array(df['value']) - shift) / scale\n",
    "        sigmas = np.array(df['uncertainty']) / scale\n",
    "        all_ys.append(ys)\n",
    "        all_sigmas.append(sigmas)\n",
    "        ds_idx.append(np.ones(len(ys)) * i)\n",
    "        scales.append(scale)\n",
    "    all_ys = np.concatenate(all_ys)\n",
    "    all_sigmas = pm.Data('sigmas', np.concatenate(all_sigmas))\n",
    "    ds_idx = np.concatenate(ds_idx).astype(int)\n",
    "    # ds_idx = pm.Data('ds_idx', ds_idx, dims=\"obs\")\n",
    "    print(all_ys.shape, all_sigmas.shape, ds_idx.shape)\n",
    "\n",
    "    alpha = pm.Exponential('alpha', 1)\n",
    "    beta = pm.Exponential('beta', 1)\n",
    "    c = pm.Gamma('c', alpha, beta, shape=N)+1\n",
    "    theta = pm.Normal('theta', 0, 100, shape=N)\n",
    "    # random_effect = pm.Normal('random_effect', 0, tau[ds_idx])\n",
    "    # theta_obs = pm.Normal('theta_obs', theta[ds_idx] + random_effect[ds_idx], tau[ds_idx])\n",
    "    y_obs = pm.Normal('y_obs', theta[ds_idx], all_sigmas*c[ds_idx], observed=all_ys)\n",
    "\n",
    "    print('model defined')\n",
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    res = pm.sample(tune=4000, draws=4000, target_accept=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import birge\n",
    "brs = []\n",
    "intervals = []\n",
    "for df in pdg2025_both_dfs:\n",
    "    interval, _, _, br = birge(df.value, df.uncertainty, coverage=0.6827)\n",
    "    brs.append(br)\n",
    "    intervals.append(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = res.posterior\n",
    "for i in range(20):\n",
    "    # plt.figure(figsize=(3,2))\n",
    "    fig, axs = plt.subplots(1,2,figsize=(5,2))\n",
    "    idx = np.random.choice(len(pdg2025_both_dfs))\n",
    "    df = pdg2025_both_dfs[idx]\n",
    "    plt.title(df['pdgid'].iloc[0] + ': n='+str(len(df)))\n",
    "    axs[0].hist(pos['c'].values[:,:,idx].flatten()+1, bins=np.linspace(1,10,200), color='grey')\n",
    "    axs[0].axvline(brs[idx], color='red')\n",
    "    axs[0].set_xlim(0.8,5)\n",
    "\n",
    "    theta_quantiles = np.quantile(pos['theta'].values[:,:,idx].flatten(), q=[0.15865,0.84135])\n",
    "    theta_quantiles = theta_quantiles * np.mean(df['uncertainty']) + np.mean(df['value'])\n",
    "\n",
    "    axs[1].errorbar(df.value, np.arange(len(df)), xerr=df.uncertainty, fmt='.', color='black')\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_xticks([])\n",
    "    axs[1].axvspan(intervals[idx][0], intervals[idx][1], color='grey', alpha=1, linewidth=0)\n",
    "    axs[1].axvspan(theta_quantiles[0], theta_quantiles[1], color='red', alpha=0.3, linewidth=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1, 4, 100)\n",
    "ns = [2, 3, 4, 5, 100]\n",
    "for n in ns:\n",
    "    plt.plot(x, chi2.cdf((n-1) * x**2, df=n-1), label=n)\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.xlabel('Inferred Scale Factor')\n",
    "plt.ylabel('CDF')\n",
    "plt.title('Inferred scale factors when truth is 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1, 4, 100)\n",
    "ns = [2, 3, 5, 10, 100]\n",
    "truth = 2\n",
    "for n in ns:\n",
    "    plt.plot(x, chi2.cdf((n-1) * x**2, df=n-1, scale=truth**2), label=n)\n",
    "plt.ylim(0,1)\n",
    "plt.axvline(truth, color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Inferred Scale Factor')\n",
    "plt.ylabel('CDF')\n",
    "plt.title(f'Inferred scale factors when truth is {truth}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
