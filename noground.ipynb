{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdg\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# point matplotlib ticks inwards\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "# add top and right ticks\n",
    "plt.rcParams['axes.spines.top'] = True\n",
    "plt.rcParams['axes.spines.right'] = True\n",
    "# use tex\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get PDG 2025 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = pdg.connect('sqlite:///data/pdgall-2025-v0.2.0.sqlite')\n",
    "\n",
    "con = sqlite3.connect('data/pdgall-2025-v0.2.0.sqlite')\n",
    "cur = con.cursor()\n",
    "command = \"\"\"\n",
    "SELECT pdgid.description, pdgmeasurement.pdgid, pdgdata.value_type, pdgdata.in_summary_table, pdgdata.value, pdgmeasurement_values.value, pdgmeasurement_values.error_positive, pdgmeasurement_values.error_negative, pdgmeasurement_values.stat_error_positive, pdgmeasurement_values.stat_error_negative\n",
    "FROM pdgmeasurement_values\n",
    "     JOIN pdgmeasurement ON pdgmeasurement.id = pdgmeasurement_values.pdgmeasurement_id\n",
    "     JOIN pdgid ON pdgid.id = pdgmeasurement.pdgid_id\n",
    "     JOIN pdgdata ON pdgdata.pdgid_id = pdgid.id\n",
    "--     JOIN pdgparticle ON pdgparticle.pdgid = pdgid.parent_pdgid\n",
    "WHERE pdgmeasurement_values.used_in_average AND pdgmeasurement_values.value IS NOT NULL AND pdgdata.edition = '2025' AND pdgdata.value_type = 'AC'\n",
    "\"\"\"\n",
    "res = cur.execute(command)\n",
    "data = res.fetchall() #WHERE \n",
    "columns = [col[0] for col in res.description]\n",
    "print(len(data), 'measurements')\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['pdgid.description', 'pdgid', 'type', 'insummary', 'avg', 'measurement', 'error_positive', 'error_negative', 'stat_error_positive', 'stat_error_negative'])\n",
    "df['error'] = (df['error_positive'] + df['error_negative'])/2\n",
    "df['staterr'] = (df['stat_error_positive'] + df['stat_error_negative'])/2\n",
    "# replace NaN staterr with total error\n",
    "staterr = np.array(df['staterr'])\n",
    "staterr[np.isnan(staterr)] = np.array(df['error'])[np.isnan(staterr)]\n",
    "# replace zero staterr with smallest nonzero staterr\n",
    "\n",
    "# see https://pdg.lbl.gov/encoder_listings/s035.pdf, S035C19 LEE, 0.003% star. error * 8.42\n",
    "staterr[(staterr == 0) & np.array(df['pdgid'] == 'S035C19')] = 0.0002526\n",
    "\n",
    "# staterr[staterr == 0] = np.min(staterr[staterr > 0])/2\n",
    "df['staterr'] = staterr\n",
    "\n",
    "\n",
    "\n",
    "df['std_resid'] = (df['measurement'] - df['avg']) / df['error']\n",
    "# only keep rows where there are at least 3 measurements\n",
    "df = df.groupby('pdgid').filter(lambda x: len(x) >= 3)\n",
    "print('Number of properties:', len(df['pdgid'].unique()))\n",
    "print('Number of measurements:', len(df))\n",
    "\n",
    "df['value'] = df['measurement']\n",
    "# to_drop = ['measurement', 'error_positive', 'error_negative', 'stat_error_positive', 'stat_error_negative']\n",
    "# df = df.drop(columns=to_drop)\n",
    "\n",
    "pdg2025_stat = df.copy()\n",
    "pdg2025_stat['uncertainty'] = pdg2025_stat['staterr']\n",
    "del pdg2025_stat['staterr'], pdg2025_stat['error']\n",
    "df_gb = pdg2025_stat.groupby('pdgid', group_keys=False)\n",
    "pdg2025_stat_dfs = [df_gb.get_group(x).copy() for x in df_gb.groups]\n",
    "\n",
    "pdg2025_both = df.copy()\n",
    "pdg2025_both['uncertainty'] = pdg2025_both['error']\n",
    "del pdg2025_both['staterr'], pdg2025_both['error']\n",
    "df_gb = pdg2025_both.groupby('pdgid', group_keys=False)\n",
    "pdg2025_both_dfs = [df_gb.get_group(x).copy() for x in df_gb.groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_from_folder(folder):\n",
    "    path_list = os.listdir(folder)\n",
    "    files = [path for path in path_list if path.endswith('.csv')]\n",
    "    files.sort()\n",
    "    dfs = [pd.read_csv(f'{folder}/{file}', comment='#') for file in files]\n",
    "    return dfs, files\n",
    "manylabs2_dfs, manylabs2_files = dfs_from_folder('data/psymetadata')\n",
    "bipm_radionuclide_dfs, bipm_radionuclide_files = dfs_from_folder('data/bipm-radionuclide')\n",
    "baker_medical_dfs, baker_medical_files = dfs_from_folder('data/baker-medical/clean')\n",
    "for df in baker_medical_dfs:\n",
    "    df['uncertainty'] = np.sqrt(df['sigma2'])\n",
    "baker_pdg2011_both_dfs, baker_pdg2011_both_files = dfs_from_folder('data/baker-pdg2011-both')\n",
    "baker_pdg2011_stat_dfs, baker_pdg2011_stat_files = dfs_from_folder('data/baker-pdg2011-stat')\n",
    "\n",
    "datasets = {\n",
    "    'pdg2025-stat': pdg2025_stat_dfs,\n",
    "    'pdg2025-both': pdg2025_both_dfs,\n",
    "    'baker-pdg2011-stat': baker_pdg2011_stat_dfs,\n",
    "    'baker-pdg2011-both': baker_pdg2011_both_dfs,\n",
    "    'bipm-radionuclide': bipm_radionuclide_dfs,\n",
    "    'manylabs2': manylabs2_dfs,\n",
    "    'baker-medical': baker_medical_dfs,\n",
    "}\n",
    "files = {\n",
    "    'baker-pdg2011-stat': baker_pdg2011_stat_files,\n",
    "    'baker-pdg2011-both': baker_pdg2011_both_files,\n",
    "    'bipm-radionuclide': bipm_radionuclide_files,\n",
    "    'manylabs2': manylabs2_files,\n",
    "    'baker-medical': baker_medical_files,\n",
    "}\n",
    "nice_names = {\n",
    "    'pdg2025-stat': 'PDG 2025 (stat)',\n",
    "    'pdg2025-both': 'PDG 2025 (stat + syst)',\n",
    "    'baker-pdg2011-stat': 'Baker 2013: PDG 2011 (stat)',\n",
    "    'baker-pdg2011-both': 'Baker 2013: PDG 2011 (stat + syst)',\n",
    "    'bipm-radionuclide': 'BIPM Radionuclide',\n",
    "    'manylabs2': 'Many Labs 2',\n",
    "    'baker-medical': 'Baker 2013: medical',\n",
    "}\n",
    "nice_names_break = {\n",
    "    'pdg2025-stat': 'PDG 2025 (stat)',\n",
    "    'pdg2025-both': 'PDG 2025 (stat + syst)',\n",
    "    'baker-pdg2011-stat': 'Baker 2013\\nPDG 2011 (stat)',\n",
    "    'baker-pdg2011-both': 'Baker 2013\\nPDG 2011 (stat + syst)',\n",
    "    'bipm-radionuclide': 'BIPM Radionuclide',\n",
    "    'manylabs2': 'Many Labs 2',\n",
    "    'baker-medical': 'Baker 2013\\nmedical',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pdgid, do some operations on each row with that pdgid\n",
    "def process_group(name, df):\n",
    "\n",
    "    sigma = np.array(df['uncertainty'])\n",
    "    sigma2 = sigma**2\n",
    "    if not np.all(sigma2 > 0):\n",
    "        print(name)\n",
    "        print(df)\n",
    "\n",
    "    S = np.sum(1/sigma2)\n",
    "\n",
    "    Xbar = np.sum(df['value'] / sigma2) / S\n",
    "    std = np.sqrt(sigma2*(1-1/(sigma2*S))**2 + (S-1/sigma2)/(S**2))\n",
    "    df['std_resid_adj'] = (df['value'] - Xbar) / std\n",
    "\n",
    "from methods import I2\n",
    "I2s = {}\n",
    "for name, dataset in datasets.items():\n",
    "    I2s[name] = []\n",
    "    for df in dataset:\n",
    "        process_group(name, df)\n",
    "        I2s[name].append(I2(df['value'], df['uncertainty']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "x = np.linspace(0, 5, 100)\n",
    "\n",
    "\n",
    "# plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Standard Normal PDF')\n",
    "# plt.title('Standardized residuals of PDG measurements')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.savefig('figs/pdg_std_residuals.pdf', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(8, 4), sharex=True, sharey=True, gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "n_datasets = len(datasets)\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    ax = axs.flatten()[i]\n",
    "    std_resids = []\n",
    "    for df in dataset:\n",
    "        std_resids += list(df['std_resid_adj'])\n",
    "    std_resids = np.abs(std_resids)\n",
    "    ax.hist(std_resids, bins=30, range=(0, 5), density=True, color='grey', label='Standardized residuals')\n",
    "    ax.axvline(0, color='black', linestyle='--')\n",
    "    ax.plot(x, norm.pdf(x, 0, 1)*2, color='red', label='Standard Normal PDF')\n",
    "    ax.set_xlim(0, 4.5)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    # put title inside axis in top right corner\n",
    "    #ax.set_title(nice_names[name])\n",
    "    ax.text(0.95, 0.95, nice_names_break[name], transform=ax.transAxes, ha='right', va='top')\n",
    "    # add top and right ticks\n",
    "    ax.tick_params(top=True, right=True)\n",
    "    ax.set_xlabel('Standardized residuals')\n",
    "    ax.set_ylabel('Density')\n",
    "    # remove y tick labels if axis is on the right\n",
    "    # if i % 2 == 1:\n",
    "    #     ax.set_yticklabels([])\n",
    "\n",
    "\n",
    "empty_axs = axs.flatten()[i+1:]\n",
    "for ax in empty_axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Standardized residuals of study results with no ground truth')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/noground-std-resids.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import birge, random_effects_mle, I2\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "brs = defaultdict(list)\n",
    "taus = defaultdict(list)\n",
    "I2s = defaultdict(list)\n",
    "errscale_ps = defaultdict(list)\n",
    "brs_cont = defaultdict(list)\n",
    "taus_cont = defaultdict(list)\n",
    "I2s_cont = defaultdict(list)\n",
    "errscale_ps_cont = defaultdict(list)\n",
    "\n",
    "birge_loglikes = defaultdict(list)\n",
    "re_loglikes = defaultdict(list)\n",
    "fe_loglikes = defaultdict(list)\n",
    "ns = defaultdict(list)\n",
    "exponent_loglikes = defaultdict(list)\n",
    "exponent_grid = np.linspace(0, 1, 101, endpoint=True)\n",
    "# print(exponent_grid)\n",
    "\n",
    "pdg2025bad = ['M047R7', 'M002R19', 'M049R52', 'M055R6', 'M053R02', 'M052R4', 'M056R4', 'M057R4', 'M070R24', 'M070R50', 'M070R60', 'M070R7', 'M070R82', 'M070R83', 'M070R84', 'M070R86', 'M070R87','M070R9', 'M070S6', 'M071R22', 'M071R28','M071S10', 'S040R11', 'S041B24', 'S041B41', 'S041C5', 'S041R3', 'S041R39', 'S041R90', 'S041S47', 'S041R65', 'S041S50', 'S041T03', 'S042B26', 'S042B27', 'S042B43', 'S042B47', 'S042B58', 'S042P59', 'S042R2', 'S042R20', 'S042R22', 'S042R23', 'S042R3', 'S042R47', 'S042R48', 'S042S24', 'S042S59', 'S049R21', 'S049S7', 'S049R24', 'S042S88', 'S086R3', 'S086R33', 'S086R32', 'S086R8', 'S086R34', 'S086R6']\n",
    "\n",
    "names = defaultdict(list)\n",
    "\n",
    "for name, dfs in datasets.items():\n",
    "    # if name not in ['baker-pdg2011-stat', 'baker-medical', 'baker-pdg2011-both']:\n",
    "    #     continue\n",
    "    print(name)\n",
    "    for i, df in tqdm(enumerate(dfs), total=len(dfs)):\n",
    "        \n",
    "        if name in ['pdg2025-stat', 'pdg2025-both']:\n",
    "            if df['pdgid'].iloc[0] in pdg2025bad:\n",
    "                continue\n",
    "            names[name].append(df['pdgid'].iloc[0])\n",
    "        else:\n",
    "            names[name].append(files[name][i])\n",
    "\n",
    "        values = np.array(df['value'])\n",
    "        sigmas = np.array(df['uncertainty'])\n",
    "\n",
    "        scaler = np.std(values)\n",
    "        if scaler == 0:\n",
    "            continue\n",
    "        values = values / scaler\n",
    "        sigmas = sigmas / scaler\n",
    "\n",
    "        # sigmas = sigmas/np.mean(sigmas)\n",
    "        _, muhat_birge, _, chat = birge(values, sigmas, coverage=0.6827, mle=True)\n",
    "        brs[name].append(chat)\n",
    "        mean_sigma = np.mean(sigmas)\n",
    "\n",
    "        _, muhat_re, _, tau = random_effects_mle(values, sigmas, coverage=0.6827)\n",
    "        taus[name].append(np.mean(tau/sigmas))\n",
    "        I2s[name].append(I2(values, sigmas))\n",
    "        # print(files[name][i], len(values), np.round(I2s[name][-1], 2))\n",
    "        # if I2s[name][-1] <= 0:\n",
    "        #     continue\n",
    "        # if files[name][i] == 'RHO770.csv':\n",
    "        #     continue\n",
    "\n",
    "        tau_grid = np.concatenate((np.linspace(0,1.1,100)*np.std(values), np.logspace(-4,4,100)))\n",
    "        \n",
    "        # theta_grid = np.concatenate((np.linspace(muhat_birge, muhat_re, 100), np.linspace(np.min(values), np.max(values), 100)))\n",
    "        exponent_loglike = np.zeros(len(exponent_grid))\n",
    "        for i,exponent in enumerate(exponent_grid):\n",
    "            # scale = np.sqrt(sigmas[:,np.newaxis,np.newaxis]**2 + ((tau_grid)**2) * (sigmas[:,np.newaxis,np.newaxis]**(2*exponent)))\n",
    "            scale = np.sqrt(sigmas[:,np.newaxis]**2 + ((tau_grid)**2) * (sigmas[:,np.newaxis]**(2*exponent)))\n",
    "            \n",
    "            w = 1/scale**2\n",
    "            assert w.shape == (len(values), len(tau_grid))\n",
    "            # print(np.sum(w * values, axis=0).shape)\n",
    "            theta_mle = np.sum(w * values[:,np.newaxis], axis=0) / np.sum(w, axis=0)\n",
    "            assert theta_mle.shape == tau_grid.shape\n",
    "\n",
    "            # print(scale.shape)\n",
    "            loglike = norm.logpdf(values[:,np.newaxis], loc=theta_mle, scale=scale)\n",
    "            assert loglike.shape == (len(values), len(tau_grid))\n",
    "            exponent_loglike[i] = np.max(np.sum(loglike, axis=0))\n",
    "        exponent_loglikes[name].append(exponent_loglike)\n",
    "\n",
    "\n",
    "        # for i in range(10):\n",
    "        #     # generate values with same sigmas but no unaccounted for errors.\n",
    "        #     # to be used as a control when analyzing the distribution of chat and tau\n",
    "        #     values_control = np.random.normal(loc=0, scale=sigmas)\n",
    "        #     _, _, _, chat_cont = birge(values_control, sigmas, coverage=0.6827)\n",
    "        #     brs_cont[name].append(chat_cont)\n",
    "        #     _, _, _, tau_cont = random_effects_mle(values_control, sigmas, coverage=0.6827)\n",
    "        #     taus_cont[name].append(np.mean(tau_cont/sigmas))\n",
    "        #     I2s_cont[name].append(I2(values_control, sigmas))\n",
    "\n",
    "\n",
    "\n",
    "        birge_loglikes[name].append(np.sum(norm.logpdf(values, loc=muhat_birge, scale=sigmas*chat)))\n",
    "        if any(np.array(birge_loglikes[name])==-np.inf):\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "        re_loglikes[name].append(np.sum(norm.logpdf(values, loc=muhat_re, scale=np.sqrt(sigmas**2+tau**2))))\n",
    "        fe_loglikes[name].append(np.sum(norm.logpdf(values, loc=muhat_birge, scale=sigmas)))\n",
    "        ns[name].append(len(df))\n",
    "\n",
    "    birge_loglikes[name] = np.array(birge_loglikes[name])\n",
    "    re_loglikes[name] = np.array(re_loglikes[name])\n",
    "    fe_loglikes[name] = np.array(fe_loglikes[name])\n",
    "    ns[name] = np.array(ns[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(k, np.sum(n)) for k, n in ns.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in exponent_loglikes.keys():\n",
    "    assert len(exponent_loglikes[name]) == len(birge_loglikes[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, theta_mle, _, tau_mle = random_effects_mle(values, sigmas, coverage=0.6827)\n",
    "# theta_mle, tau_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_idx, tau_idx = np.unravel_index(np.argmax(loglike), loglike.shape)\n",
    "# theta_grid[theta_idx], tau_grid[tau_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(norm.logpdf(values, loc=theta_mle, scale=np.sqrt(sigmas**2+tau_mle**2))))\n",
    "# print(np.sum(norm.logpdf(values, loc=theta_grid[theta_idx], scale=np.sqrt(sigmas**2+tau_grid[tau_idx]**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "best_exponents = {}\n",
    "exponent_cis = {}\n",
    "for name, exponent_loglike in exponent_loglikes.items():\n",
    "    exponent_loglike = np.sum(exponent_loglike, axis=0)\n",
    "    best_exponents[name] = exponent_grid[np.argmax(exponent_loglike)]\n",
    "    logratio = 2 * (np.max(exponent_loglike) - exponent_loglike)\n",
    "    within = logratio < chi2.ppf(0.95, df=1)\n",
    "    idx_l = np.argmax(within)\n",
    "    idx_u = -np.argmax(within[::-1]) - 1\n",
    "    low = exponent_grid[idx_l]\n",
    "    high = exponent_grid[idx_u]\n",
    "    exponent_cis[name] = [low, high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for name in best_exponents.keys():\n",
    "    rows.append([r'\\texttt{' + name + r'}', f'${best_exponents[name]}$', f'$[{exponent_cis[name][0]}, {exponent_cis[name][1]:.2f}]$'])\n",
    "    \n",
    "# use np to save latex table with & separator\n",
    "txt = ' \\\\\\\\\\n'.join([' & '.join(row) for row in rows])\n",
    "print(txt)\n",
    "with open('tables/noground-bakermodel.tex', 'w') as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(exponent_grid, exponent_loglikes['baker-pdg2011-both'][4])\n",
    "# for exponent_loglike in exponent_loglikes['baker-pdg2011-stat']:\n",
    "    # plt.plot(exponent_grid, (exponent_loglike - np.max(exponent_loglike))/(np.max(exponent_loglike)-np.min(exponent_loglike)))\n",
    "which = 'baker-pdg2011-stat'\n",
    "for name, exponent_loglike in exponent_loglikes.items():\n",
    "    exponent_loglike = np.sum(np.array(exponent_loglike), axis=0)\n",
    "    plt.plot(exponent_grid, exponent_loglike-np.max(exponent_loglike), label=name)\n",
    "    # print(len(exponent_loglike))\n",
    "    # plt.axvline(exponent_grid[np.argmax(exponent_loglike)], color='black', linestyle='--')\n",
    "plt.ylim(-60, 5)\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exponent_grid, np.sum(np.array(exponent_loglikes['pdg2025-stat']), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(re_loglikes['pdg2025-both'], np.array(exponent_loglikes['pdg2025-both'])[:,0], s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = 'pdg2025-both'\n",
    "diffs = np.array(exponent_loglikes[which])[:,0]-re_loglikes[which]\n",
    "print(np.sum(diffs))\n",
    "bad_idx = np.argmax(diffs)\n",
    "\n",
    "name = names[which][bad_idx]\n",
    "print(name)\n",
    "for df in datasets[which]:\n",
    "    if df['pdgid'].iloc[0] == name:\n",
    "        break\n",
    "\n",
    "values = np.array(df['value'])\n",
    "sigmas = np.array(df['uncertainty'])\n",
    "print(values, sigmas)\n",
    "_, theta_mle, _, tau_mle = random_effects_mle(values, sigmas, coverage=0.6827)\n",
    "print(theta_mle, tau_mle)\n",
    "tau_grid = np.concatenate((np.logspace(-4,4,200)*max(sigmas), np.logspace(-4,4,200)))\n",
    "theta_grid = np.concatenate((np.linspace(muhat_birge, muhat_re, 100), np.linspace(np.min(values), np.max(values), 100)))\n",
    "scale = np.sqrt(sigmas[:,np.newaxis,np.newaxis]**2 + (tau_grid)**2)\n",
    "loglike = np.sum(norm.logpdf(values[:,np.newaxis,np.newaxis], loc=theta_grid[:,np.newaxis], scale=scale), axis=0)\n",
    "\n",
    "theta_idx, tau_idx = np.unravel_index(np.argmax(loglike), loglike.shape)\n",
    "print(theta_grid[theta_idx], tau_grid[tau_idx])\n",
    "\n",
    "print(np.sum(norm.logpdf(values, loc=theta_mle, scale=np.sqrt(sigmas**2+tau_mle**2))))\n",
    "print(np.sum(norm.logpdf(values, loc=theta_grid[theta_idx], scale=np.sqrt(sigmas**2+tau_grid[tau_idx]**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3), sharey=True, gridspec_kw={'wspace': 0.1})\n",
    "for i, data in enumerate([ns, I2s, brs]):\n",
    "    parts = axs[i].violinplot(list(data.values()), vert=False, showmedians=False, showextrema=False)\n",
    "    axs[i].tick_params(top=True, right=True)\n",
    "    for j in range(len(data)):\n",
    "        axs[i].axhline(j+1, color='grey', linewidth=0.5, linestyle=':')\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('grey')\n",
    "        pc.set_edgecolor('none')\n",
    "        pc.set_alpha(1)\n",
    "    # add median\n",
    "    medians = [np.median(data[name]) for name in data.keys()]\n",
    "    print(medians)\n",
    "    axs[i].scatter(medians, np.array(range(len(data)))+1, marker='|', s=50, c='black', alpha=1, linewidths=1)\n",
    "\n",
    "axs[0].set_xlabel('Number of results')\n",
    "axs[1].set_xlabel('$I^2$')\n",
    "axs[2].set_xlabel('Birge Ratio')\n",
    "\n",
    "# add y ticks\n",
    "axs[0].set_yticks(np.array(range(len(datasets)))+1)\n",
    "axs[0].set_yticklabels([nice_names_break[name] for name in datasets.keys()])\n",
    "# flip y axis\n",
    "axs[0].invert_yaxis()\n",
    "axs[2].set_xlim(1, 5)\n",
    "axs[0].set_xlim(0, None)\n",
    "# add right ticks\n",
    "axs[0].tick_params(top=True, right=True)\n",
    "axs[1].tick_params(top=True, right=True)\n",
    "axs[2].tick_params(top=True, right=True)\n",
    "axs[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/noground_violin.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "rows_numeric = []\n",
    "\n",
    "def format_number(x):\n",
    "    if x == -np.inf:\n",
    "        return r'$\\approx-\\infty$'\n",
    "    if x == np.inf:\n",
    "        return r'$\\approx\\infty$'\n",
    "    if x >= 1e3 or x <= -1e3:\n",
    "        return f'${str(int(x))}$'\n",
    "    return r'${0:.3g}$'.format(x)\n",
    "\n",
    "def add_bold(string):\n",
    "    return r'$\\mathbf{' + string.strip('$') + r'}$'\n",
    "\n",
    "for name in datasets.keys():\n",
    "    loglikes = np.array([birge_loglikes[name], re_loglikes[name], fe_loglikes[name]])\n",
    "    total_loglikes = np.sum(loglikes, axis=1)\n",
    "    assert len(total_loglikes) == 3\n",
    "    ks = np.array([2, 2, 1])\n",
    "    \n",
    "    bics = ks[:, np.newaxis] * np.log(np.array(ns[name])) - 2 * loglikes\n",
    "    aics = 2 * ks[:, np.newaxis] - 2 * loglikes\n",
    "    bics = np.mean(bics, axis=1)\n",
    "    aics = np.mean(aics, axis=1)\n",
    "    numbers = list(-total_loglikes) + list(bics) + list(aics)\n",
    "    numbers_str = [format_number(x) for x in numbers]\n",
    "    rows.append([r'\\texttt{' + name + r'}'] + numbers_str)\n",
    "    rows_numeric.append(numbers)\n",
    "rows_numeric = np.array(rows_numeric)\n",
    "\n",
    "best_loglike = np.argmin(rows_numeric[:, :3], axis=1)\n",
    "best_bic = np.argmin(rows_numeric[:, 3:6], axis=1)\n",
    "best_aic = np.argmin(rows_numeric[:, 6:], axis=1)\n",
    "\n",
    "# add bold to the best model\n",
    "for i, row in enumerate(rows):\n",
    "    row[best_loglike[i]+1] = add_bold(row[best_loglike[i]+1])\n",
    "    row[best_bic[i]+4] = add_bold(row[best_bic[i]+4])\n",
    "    row[best_aic[i]+7] = add_bold(row[best_aic[i]+7])\n",
    "\n",
    "# use np to save latex table with & separator\n",
    "txt = ' \\\\\\\\\\n'.join([' & '.join(row) for row in rows])\n",
    "print(txt)\n",
    "with open('tables/noground-loglike.tex', 'w') as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxn = np.max([np.max(n) for n in ns.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 2, figsize=(7, 9), sharex=False, sharey=False, gridspec_kw={'wspace': 0, 'hspace': 0.1})\n",
    "\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    ax = axs.flatten()[i]\n",
    "\n",
    "    sc = ax.scatter(birge_loglikes[name], re_loglikes[name], marker='x', s=10, c=ns[name], vmin=0, vmax=maxn, facecolor='none', linewidths=0.5)\n",
    "    ymin = np.min([np.min(birge_loglikes[name]), np.min(re_loglikes[name])])\n",
    "    ymax = np.max([np.max(birge_loglikes[name]), np.max(re_loglikes[name])])\n",
    "    ax.plot([ymin-1, ymax+1], [ymin-1, ymax+1], color='red', linewidth=1, linestyle=':')\n",
    "    # set aspect ratio to 1\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_xlim(ymin-0.2, ymax+0.2)\n",
    "    ax.set_ylim(ymin-0.2, ymax+0.2)\n",
    "    ax.set_xlabel('BR log-likelihood')\n",
    "    ax.set_ylabel('RE log-likelihood')\n",
    "    # plt.colorbar(label='Number of measurements')\n",
    "    re_better_count = np.sum(re_loglikes[name] > birge_loglikes[name])\n",
    "    birge_better_count = np.sum(birge_loglikes[name] > re_loglikes[name])\n",
    "    total_count = len(birge_loglikes[name])\n",
    "    re_better_frac = f'{re_better_count}/{total_count}'\n",
    "    birge_better_frac = f'{birge_better_count}/{total_count}'\n",
    "    ax.text(0.3, 0.5, f'RE better ({re_better_frac})', fontsize=8, fontweight='bold', color='black', ha='center', va='center', transform=ax.transAxes, rotation=45)\n",
    "    ax.text(0.5, 0.3, f'BR better ({birge_better_frac})', fontsize=8, fontweight='bold', color='black', ha='center', va='center', transform=ax.transAxes, rotation=45)\n",
    "    ax.text(0.05, 0.95, nice_names_break[name], transform=ax.transAxes, ha='left', va='top')\n",
    "fig.suptitle('Random Effects and Birge Ratio MLE log-likelihoods for each set of results')\n",
    "axs.flatten()[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax, label='Number of measurements')\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.savefig('figs/noground_loglike.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    diff = re_loglikes[name] - birge_loglikes[name]\n",
    "    plt.scatter(diff, -np.ones(len(diff))*i, marker='|', s=20, vmin=0, vmax=maxn, facecolor='none', linewidths=0.4, c='black', alpha=0.5)\n",
    "    plt.axhline(-i, color='grey', linewidth=0.3, linestyle=':')\n",
    "\n",
    "    re_better_count = np.sum(re_loglikes[name] > birge_loglikes[name])\n",
    "    birge_better_count = np.sum(birge_loglikes[name] > re_loglikes[name])\n",
    "    total_count = len(birge_loglikes[name])\n",
    "    re_better_frac = f'{re_better_count}/{total_count}'\n",
    "    birge_better_frac = f'{birge_better_count}/{total_count}'\n",
    "    plt.text(1, -i+0.3, f'RE better: {re_better_frac}', fontsize=8, fontweight='bold', color='black', ha='left', va='center')\n",
    "    plt.text(-1, -i+0.3, f'BR better: {birge_better_frac}', fontsize=8, fontweight='bold', color='black', ha='right', va='center')\n",
    "    \n",
    "plt.xlabel('RE log-likelihood - BR log-likelihood')\n",
    "plt.ylabel('Set of results')\n",
    "plt.yticks(range(0, -len(datasets), -1), [nice_names_break[name] for name in datasets.keys()])\n",
    "plt.tight_layout()\n",
    "plt.axvline(0, color='red', linewidth=1, linestyle=':')\n",
    "xmax = max(np.abs(np.array(plt.xlim())))\n",
    "plt.xlim(-xmax, xmax)\n",
    "plt.ylim(-len(datasets)+0.5, 0.8)\n",
    "plt.gca().tick_params(top=True)\n",
    "plt.savefig('figs/noground_loglike_diff.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
