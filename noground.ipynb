{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdg\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# point matplotlib ticks inwards\n",
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "# add top and right ticks\n",
    "plt.rcParams[\"axes.spines.top\"] = True\n",
    "plt.rcParams[\"axes.spines.right\"] = True\n",
    "# use tex\n",
    "plt.rcParams[\"text.usetex\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get PDG 2025 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = pdg.connect(\"sqlite:///data/pdgall-2025-v0.2.0.sqlite\")\n",
    "\n",
    "con = sqlite3.connect(\"data/pdgall-2025-v0.2.0.sqlite\")\n",
    "cur = con.cursor()\n",
    "command = \"\"\"\n",
    "SELECT pdgid.description, pdgmeasurement.pdgid, pdgdata.value_type, pdgdata.in_summary_table, pdgdata.value, pdgmeasurement_values.value, pdgmeasurement_values.error_positive, pdgmeasurement_values.error_negative, pdgmeasurement_values.stat_error_positive, pdgmeasurement_values.stat_error_negative\n",
    "FROM pdgmeasurement_values\n",
    "     JOIN pdgmeasurement ON pdgmeasurement.id = pdgmeasurement_values.pdgmeasurement_id\n",
    "     JOIN pdgid ON pdgid.id = pdgmeasurement.pdgid_id\n",
    "     JOIN pdgdata ON pdgdata.pdgid_id = pdgid.id\n",
    "--     JOIN pdgparticle ON pdgparticle.pdgid = pdgid.parent_pdgid\n",
    "WHERE pdgmeasurement_values.used_in_average AND pdgmeasurement_values.value IS NOT NULL AND pdgdata.edition = '2025' AND pdgdata.value_type = 'AC'\n",
    "\"\"\"\n",
    "res = cur.execute(command)\n",
    "data = res.fetchall()  # WHERE\n",
    "columns = [col[0] for col in res.description]\n",
    "print(len(data), \"measurements\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"pdgid.description\",\n",
    "        \"pdgid\",\n",
    "        \"type\",\n",
    "        \"insummary\",\n",
    "        \"avg\",\n",
    "        \"measurement\",\n",
    "        \"error_positive\",\n",
    "        \"error_negative\",\n",
    "        \"stat_error_positive\",\n",
    "        \"stat_error_negative\",\n",
    "    ],\n",
    ")\n",
    "df[\"error\"] = (df[\"error_positive\"] + df[\"error_negative\"]) / 2\n",
    "df[\"staterr\"] = (df[\"stat_error_positive\"] + df[\"stat_error_negative\"]) / 2\n",
    "# replace NaN staterr with total error\n",
    "staterr = np.array(df[\"staterr\"])\n",
    "staterr[np.isnan(staterr)] = np.array(df[\"error\"])[np.isnan(staterr)]\n",
    "# replace zero staterr with smallest nonzero staterr\n",
    "\n",
    "# see https://pdg.lbl.gov/encoder_listings/s035.pdf, S035C19 LEE, 0.003% star. error * 8.42\n",
    "staterr[(staterr == 0) & np.array(df[\"pdgid\"] == \"S035C19\")] = 0.0002526\n",
    "\n",
    "# staterr[staterr == 0] = np.min(staterr[staterr > 0])/2\n",
    "df[\"staterr\"] = staterr\n",
    "\n",
    "\n",
    "df[\"std_resid\"] = (df[\"measurement\"] - df[\"avg\"]) / df[\"error\"]\n",
    "# only keep rows where there are at least 3 measurements\n",
    "df = df.groupby(\"pdgid\").filter(lambda x: len(x) >= 3)\n",
    "\n",
    "df[\"value\"] = df[\"measurement\"]\n",
    "# to_drop = ['measurement', 'error_positive', 'error_negative', 'stat_error_positive', 'stat_error_negative']\n",
    "# df = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"Number of properties:\", len(df[\"pdgid\"].unique()))\n",
    "print(\"Number of measurements:\", len(df))\n",
    "\n",
    "pdg2025_stat = df.copy()\n",
    "pdg2025_stat[\"uncertainty\"] = pdg2025_stat[\"staterr\"]\n",
    "del pdg2025_stat[\"staterr\"], pdg2025_stat[\"error\"]\n",
    "df_gb = pdg2025_stat.groupby(\"pdgid\", group_keys=False)\n",
    "pdg2025_stat_dfs = [df_gb.get_group(x).copy() for x in df_gb.groups]\n",
    "\n",
    "pdg2025_both = df.copy()\n",
    "pdg2025_both[\"uncertainty\"] = pdg2025_both[\"error\"]\n",
    "del pdg2025_both[\"staterr\"], pdg2025_both[\"error\"]\n",
    "df_gb = pdg2025_both.groupby(\"pdgid\", group_keys=False)\n",
    "pdg2025_both_dfs = [df_gb.get_group(x).copy() for x in df_gb.groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg2025bad = [\n",
    "    \"M047R7\",\n",
    "    \"M002R19\",\n",
    "    \"M049R52\",\n",
    "    \"M055R6\",\n",
    "    \"M053R02\",\n",
    "    \"M052R4\",\n",
    "    \"M056R4\",\n",
    "    \"M057R4\",\n",
    "    \"M070R24\",\n",
    "    \"M070R50\",\n",
    "    \"M070R60\",\n",
    "    \"M070R7\",\n",
    "    \"M070R82\",\n",
    "    \"M070R83\",\n",
    "    \"M070R84\",\n",
    "    \"M070R86\",\n",
    "    \"M070R87\",\n",
    "    \"M070R9\",\n",
    "    \"M070S6\",\n",
    "    \"M071R22\",\n",
    "    \"M071R28\",\n",
    "    \"M071S10\",\n",
    "    \"S040R11\",\n",
    "    \"S041B24\",\n",
    "    \"S041B41\",\n",
    "    \"S041C5\",\n",
    "    \"S041R3\",\n",
    "    \"S041R39\",\n",
    "    \"S041R90\",\n",
    "    \"S041S47\",\n",
    "    \"S041R65\",\n",
    "    \"S041S50\",\n",
    "    \"S041T03\",\n",
    "    \"S042B26\",\n",
    "    \"S042B27\",\n",
    "    \"S042B43\",\n",
    "    \"S042B47\",\n",
    "    \"S042B58\",\n",
    "    \"S042P59\",\n",
    "    \"S042R2\",\n",
    "    \"S042R20\",\n",
    "    \"S042R22\",\n",
    "    \"S042R23\",\n",
    "    \"S042R3\",\n",
    "    \"S042R47\",\n",
    "    \"S042R48\",\n",
    "    \"S042S24\",\n",
    "    \"S042S59\",\n",
    "    \"S049R21\",\n",
    "    \"S049S7\",\n",
    "    \"S049R24\",\n",
    "    \"S042S88\",\n",
    "    \"S086R3\",\n",
    "    \"S086R33\",\n",
    "    \"S086R32\",\n",
    "    \"S086R8\",\n",
    "    \"S086R34\",\n",
    "    \"S086R6\",\n",
    "]\n",
    "\n",
    "pdg2025_stat_dfs = [\n",
    "    df for df in pdg2025_stat_dfs if df[\"pdgid\"].iloc[0] not in pdg2025bad\n",
    "]\n",
    "pdg2025_both_dfs = [\n",
    "    df for df in pdg2025_both_dfs if df[\"pdgid\"].iloc[0] not in pdg2025bad\n",
    "]\n",
    "pdg2025_stat_quantities = [df[\"pdgid\"].iloc[0] for df in pdg2025_stat_dfs]\n",
    "pdg2025_both_quantities = [df[\"pdgid\"].iloc[0] for df in pdg2025_both_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(df[\"stat_error_negative\"])\n",
    "df.iloc[317, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_from_folder(folder):\n",
    "    path_list = os.listdir(folder)\n",
    "    files = [path for path in path_list if path.endswith(\".csv\")]\n",
    "    files.sort()\n",
    "    dfs = [pd.read_csv(f\"{folder}/{file}\", comment=\"#\") for file in files]\n",
    "    return dfs, files\n",
    "\n",
    "\n",
    "manylabs2_dfs, manylabs2_files = dfs_from_folder(\"data/psymetadata\")\n",
    "bipm_radionuclide_dfs, bipm_radionuclide_files = dfs_from_folder(\n",
    "    \"data/bipm-radionuclide\"\n",
    ")\n",
    "baker_medical_dfs, baker_medical_files = dfs_from_folder(\"data/baker-medical/clean\")\n",
    "for df in baker_medical_dfs:\n",
    "    df[\"uncertainty\"] = np.sqrt(df[\"sigma2\"])\n",
    "baker_pdg2011_both_dfs, baker_pdg2011_both_files = dfs_from_folder(\n",
    "    \"data/baker-pdg2011-both\"\n",
    ")\n",
    "baker_pdg2011_stat_dfs, baker_pdg2011_stat_files = dfs_from_folder(\n",
    "    \"data/baker-pdg2011-stat\"\n",
    ")\n",
    "pdg1970_dfs, pdg1970_files = dfs_from_folder(\"data/pdg1970\")\n",
    "bailey_pdg_dfs, bailey_pdg_files = dfs_from_folder(\"data/bailey/CsvData/Particle_Data\")\n",
    "bailey_pdg_stable_dfs, bailey_pdg_stable_files = dfs_from_folder(\n",
    "    \"data/bailey/CsvData/Particle_Data_Stable\"\n",
    ")\n",
    "bailey_constants_dfs, bailey_constants_files = dfs_from_folder(\n",
    "    \"data/bailey/CsvData/Constants_Data\"\n",
    ")\n",
    "bailey_interlab_dfs, bailey_interlab_files = dfs_from_folder(\n",
    "    \"data/bailey/CsvData/Interlab_Data\"\n",
    ")\n",
    "bailey_interlab_key_dfs, bailey_interlab_key_files = dfs_from_folder(\n",
    "    \"data/bailey/CsvData/Interlab_Data_Key\"\n",
    ")\n",
    "bailey_nuclear_dfs, bailey_nuclear_files = dfs_from_folder(\n",
    "    \"data/bailey/CsvData/Nuclear_Data\"\n",
    ")\n",
    "bailey_medical_dfs, bailey_medical_files = dfs_from_folder(\n",
    "    \"data/bailey/CsvData/Medical_Data\"\n",
    ")\n",
    "\n",
    "datasets = {\n",
    "    \"pdg2025-stat\": pdg2025_stat_dfs,\n",
    "    \"pdg2025-both\": pdg2025_both_dfs,\n",
    "    \"baker-pdg2011-stat\": baker_pdg2011_stat_dfs,\n",
    "    \"baker-pdg2011-both\": baker_pdg2011_both_dfs,\n",
    "    \"bailey-pdg\": bailey_pdg_dfs,\n",
    "    \"bailey-pdg-stable\": bailey_pdg_stable_dfs,\n",
    "    \"pdg1970\": pdg1970_dfs,\n",
    "    \"bipm-radionuclide\": bipm_radionuclide_dfs,\n",
    "    \"bailey-nuclear\": bailey_nuclear_dfs,\n",
    "    \"bailey-interlab\": bailey_interlab_dfs,\n",
    "    \"bailey-interlab-key\": bailey_interlab_key_dfs,\n",
    "    \"manylabs2\": manylabs2_dfs,\n",
    "    \"baker-medical\": baker_medical_dfs,\n",
    "    \"bailey-medical\": bailey_medical_dfs,\n",
    "    \"bailey-constants\": bailey_constants_dfs,\n",
    "}\n",
    "quantities = {\n",
    "    \"pdg2025-stat\": pdg2025_stat_quantities,\n",
    "    \"pdg2025-both\": pdg2025_both_quantities,\n",
    "    \"baker-pdg2011-stat\": baker_pdg2011_stat_files,\n",
    "    \"baker-pdg2011-both\": baker_pdg2011_both_files,\n",
    "    \"bailey-pdg\": bailey_pdg_files,\n",
    "    \"bailey-pdg-stable\": bailey_pdg_stable_files,\n",
    "    \"pdg1970\": pdg1970_files,\n",
    "    \"bipm-radionuclide\": bipm_radionuclide_files,\n",
    "    \"bailey-nuclear\": bailey_nuclear_files,\n",
    "    \"bailey-interlab\": bailey_interlab_files,\n",
    "    \"bailey-interlab-key\": bailey_interlab_key_files,\n",
    "    \"manylabs2\": manylabs2_files,\n",
    "    \"baker-medical\": baker_medical_files,\n",
    "    \"bailey-medical\": bailey_medical_files,\n",
    "    \"bailey-constants\": bailey_constants_files,\n",
    "}\n",
    "nice_names = {\n",
    "    \"pdg2025-stat\": \"PDG 2025 (stat)\",\n",
    "    \"pdg2025-both\": \"PDG 2025 (stat + syst)\",\n",
    "    \"baker-pdg2011-stat\": \"Baker 2013: PDG 2011 (stat)\",\n",
    "    \"baker-pdg2011-both\": \"Baker 2013: PDG 2011 (stat + syst)\",\n",
    "    \"bailey-pdg\": \"Bailey 2017: PDG\",\n",
    "    \"bailey-pdg-stable\": \"Bailey 2017: PDG (stable)\",\n",
    "    \"pdg1970\": \"PDG 1970\",\n",
    "    \"bipm-radionuclide\": \"BIPM Radionuclide\",\n",
    "    \"bailey-nuclear\": \"Bailey 2017: nuclear\",\n",
    "    \"bailey-interlab\": \"Bailey 2017: interlab\",\n",
    "    \"bailey-interlab-key\": \"Bailey 2017: interlab (key)\",\n",
    "    \"manylabs2\": \"Many Labs 2\",\n",
    "    \"baker-medical\": \"Baker 2013: medical\",\n",
    "    \"bailey-medical\": \"Bailey 2017: medical\",\n",
    "    \"bailey-constants\": \"Bailey 2017: constants\",\n",
    "}\n",
    "nice_names_break = {\n",
    "    \"pdg1970\": \"PDG 1970\",\n",
    "    \"pdg2025-stat\": \"PDG 2025 (stat)\",\n",
    "    \"pdg2025-both\": \"PDG 2025 (stat + syst)\",\n",
    "    \"baker-pdg2011-stat\": \"Baker 2013\\nPDG 2011 (stat)\",\n",
    "    \"baker-pdg2011-both\": \"Baker 2013\\nPDG 2011 (stat + syst)\",\n",
    "    \"bailey-pdg\": \"Bailey 2017\\nPDG\",\n",
    "    \"bailey-pdg-stable\": \"Bailey 2017\\nPDG (stable)\",\n",
    "    \"bipm-radionuclide\": \"BIPM Radionuclide\",\n",
    "    \"bailey-nuclear\": \"Bailey 2017\\nnuclear\",\n",
    "    \"bailey-interlab\": \"Bailey 2017\\ninterlab\",\n",
    "    \"bailey-interlab-key\": \"Bailey 2017\\ninterlab (key)\",\n",
    "    \"manylabs2\": \"Many Labs 2\",\n",
    "    \"baker-medical\": \"Baker 2013\\nmedical\",\n",
    "    \"bailey-medical\": \"Bailey 2017\\nmedical\",\n",
    "    \"bailey-constants\": \"Bailey 2017\\nconstants\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pdgid, do some operations on each row with that pdgid\n",
    "def process_group(name, df):\n",
    "    sigma = np.array(df[\"uncertainty\"])\n",
    "    sigma2 = sigma**2\n",
    "    if not np.all(sigma2 > 0):\n",
    "        print(name)\n",
    "        print(df)\n",
    "\n",
    "    S = np.sum(1 / sigma2)\n",
    "\n",
    "    Xbar = np.sum(df[\"value\"] / sigma2) / S\n",
    "    std = np.sqrt(sigma2 * (1 - 1 / (sigma2 * S)) ** 2 + (S - 1 / sigma2) / (S**2))\n",
    "    df[\"std_resid_adj\"] = (df[\"value\"] - Xbar) / std\n",
    "\n",
    "\n",
    "from methods import I2\n",
    "\n",
    "I2s = {}\n",
    "for name in datasets.keys():\n",
    "    print(name)\n",
    "    I2s[name] = []\n",
    "    for df, quantity in zip(datasets[name], quantities[name]):\n",
    "        process_group(quantity, df)\n",
    "        I2s[name].append(I2(df[\"value\"], df[\"uncertainty\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "\n",
    "\n",
    "# plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Standard Normal PDF')\n",
    "# plt.title('Standardized residuals of PDG measurements')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.savefig('figs/pdg_std_residuals.pdf', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    4,\n",
    "    4,\n",
    "    figsize=(8, 8),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    gridspec_kw={\"hspace\": 0, \"wspace\": 0},\n",
    ")\n",
    "n_datasets = len(datasets)\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    ax = axs.flatten()[i]\n",
    "    std_resids = []\n",
    "    for df in dataset:\n",
    "        std_resids += list(df[\"std_resid_adj\"])\n",
    "    std_resids = np.abs(std_resids)\n",
    "    ax.hist(\n",
    "        std_resids,\n",
    "        bins=30,\n",
    "        range=(0, 5),\n",
    "        density=True,\n",
    "        color=\"grey\",\n",
    "        label=\"Standardized residuals\",\n",
    "    )\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "    ax.plot(x, norm.pdf(x, 0, 1) * 2, color=\"red\", label=\"Standard Normal PDF\")\n",
    "    ax.set_xlim(0, 4.5)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    # put title inside axis in top right corner\n",
    "    # ax.set_title(nice_names[name])\n",
    "    ax.text(\n",
    "        0.95, 0.95, nice_names_break[name], transform=ax.transAxes, ha=\"right\", va=\"top\"\n",
    "    )\n",
    "    # add top and right ticks\n",
    "    ax.tick_params(top=True, right=True)\n",
    "    ax.set_xlabel(\"Standardized residuals\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    # remove y tick labels if axis is on the right\n",
    "    # if i % 2 == 1:\n",
    "    #     ax.set_yticklabels([])\n",
    "\n",
    "\n",
    "empty_axs = axs.flatten()[i + 1 :]\n",
    "for ax in empty_axs:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Standardized residuals of study results with no ground truth\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/noground-std-resids.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bailey pair analysis\n",
    "from itertools import combinations\n",
    "\n",
    "dataset = datasets[\"pdg2025-both\"]\n",
    "zs = []\n",
    "weights1 = []\n",
    "weights2 = []\n",
    "for df in dataset:\n",
    "    n = len(df)\n",
    "    n_pairs = n * (n - 1) / 2\n",
    "    weight1 = 1 / n_pairs\n",
    "    weight2 = 1 / n\n",
    "    val_list = list(zip(df[\"value\"], df[\"uncertainty\"]))\n",
    "    # print(val_list)\n",
    "    for pair in combinations(val_list, 2):\n",
    "        z = np.abs(pair[0][0] - pair[1][0]) / np.sqrt(pair[0][1] ** 2 + pair[1][1] ** 2)\n",
    "        zs.append(z)\n",
    "        weights1.append(weight1)\n",
    "        weights2.append(weight2)\n",
    "        # if z > 10:\n",
    "        #     print(df)\n",
    "zs = np.array(zs)\n",
    "weights1 = np.array(weights1)\n",
    "weights2 = np.array(weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = 10\n",
    "xspace = np.linspace(0, xmax, 100)\n",
    "plt.hist(zs, bins=np.linspace(0, xmax, 50), density=True, weights=weights1)\n",
    "plt.plot(xspace, norm.pdf(xspace, 0, 1) * 2, color=\"red\", label=\"Standard Normal PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zspace = np.linspace(0, 20, 100)\n",
    "probs_w1 = []\n",
    "probs_w2 = []\n",
    "probs_unweighted = []\n",
    "probs_norm = []\n",
    "denom1 = np.sum(weights1)\n",
    "denom2 = np.sum(weights2)\n",
    "for z in zspace:\n",
    "    above = zs > z\n",
    "    probs_w1.append(np.sum(above * weights1) / denom1)\n",
    "    probs_w2.append(np.sum(above * weights2) / denom2)\n",
    "    probs_unweighted.append(np.mean(above))\n",
    "    # probs.append(np.mean(above))\n",
    "    probs_norm.append(norm.cdf(-z) * 2)\n",
    "plt.plot(zspace, probs_w1)\n",
    "plt.plot(zspace, probs_w2)\n",
    "plt.plot(zspace, probs_unweighted)\n",
    "plt.plot(zspace, probs_norm)\n",
    "plt.ylim(1e-6, 1)\n",
    "plt.xlim(0, None)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t, pareto, genpareto\n",
    "# nu_grid = np.linspace(0.5, 10, 100)\n",
    "# sigma_grid = np.linspace(0.5, 10, 100)\n",
    "# loglikes = np.full([len(nu_grid), len(sigma_grid)], np.nan)\n",
    "# for i,nu in enumerate(nu_grid):\n",
    "#     for j,sigma in enumerate(sigma_grid):\n",
    "#         loglikes[i,j] = np.sum(np.log(t.pdf(zs, df=nu, scale=sigma)*weights1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nu_idx, sigma_idx = np.unravel_index(np.argmax(loglikes), loglikes.shape)\n",
    "# nu_star, sigma_star = nu_grid[nu_idx], sigma_grid[sigma_idx]\n",
    "# # print(nu_grid[nu_idx], sigma_grid[sigma_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(zspace, probs_w1)\n",
    "# plt.plot(zspace, probs_w2)\n",
    "# plt.plot(zspace, norm.cdf(-zspace)*2, label='Normal')\n",
    "# plt.plot(zspace, t.cdf(-zspace, df=nu_star, scale=sigma_star)*2, label='t')\n",
    "# plt.plot(zspace, 1-genpareto.cdf(zspace, *genpareto.fit(zs, floc=0)), label='generalized pareto')\n",
    "# plt.plot(zspace, norm.cdf(-zspace, *norm.fit(zs, floc=0))*2, label='wider Normal')\n",
    "# # plt.plot(zspace, t.cdf(-zspace, df=2.7, scale=1.1)*2)\n",
    "# plt.yscale('log')\n",
    "# plt.ylim(1e-6, 1)\n",
    "# plt.xlim(0, 15)\n",
    "# plt.legend()\n",
    "# print(nu_star, sigma_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dfs = []\n",
    "for df in datasets[\"pdg2025-both\"]:\n",
    "    sigmas = np.array(df[\"uncertainty\"])\n",
    "    tau = np.median(sigmas)\n",
    "    values = norm.rvs(loc=0, scale=np.sqrt(sigmas**2 + tau**2))\n",
    "    synthetic_dfs.append(pd.DataFrame({\"value\": values, \"uncertainty\": sigmas}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    4,\n",
    "    4,\n",
    "    figsize=(8, 8),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    gridspec_kw={\"hspace\": 0, \"wspace\": 0},\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from measurement_dist import measurement_dist\n",
    "import json\n",
    "\n",
    "LIMIT = -1  # TODO SET TO -1\n",
    "for i, (name, dfs) in tqdm(\n",
    "    enumerate((datasets).items())\n",
    "):  # | {'synthetic': synthetic_dfs}).items():\n",
    "    if name == \"pdg1970\":\n",
    "        continue\n",
    "    ax = axs.flatten()[i]\n",
    "\n",
    "    output = measurement_dist(dfs, lists=True)\n",
    "    output[\"name\"] = nice_names_break[name]\n",
    "\n",
    "    # save results for use in measurement_dist.py\n",
    "    with open(f\"results/measurement_dist/{name}.json\", \"w\") as f:\n",
    "        json.dump(output, f)\n",
    "\n",
    "    probs = np.array(output[\"pair\"])\n",
    "    ax.plot(zspace[probs > 0], probs[probs > 0], label=r\"$z_{ij}$\")\n",
    "    probs = np.array(output[\"h\"])\n",
    "    ax.plot(zspace[probs > 0], probs[probs > 0], label=r\"$h_{i}$\")\n",
    "    probs = np.array(output[\"hprime\"])\n",
    "    ax.plot(zspace[probs > 0], probs[probs > 0], label=r\"$h_{i}^\\prime$\")\n",
    "    probs = np.array(output[\"norm\"])\n",
    "    ax.plot(zspace, probs, label=\"Normal\")\n",
    "    ax.legend()\n",
    "    ax.text(\n",
    "        0.95, 0.95, nice_names_break[name], transform=ax.transAxes, ha=\"right\", va=\"top\"\n",
    "    )\n",
    "    ax.set_ylabel(\"$P(Z>z)$\")\n",
    "    ax.set_xlabel(\"$z$\")\n",
    "\n",
    "plt.ylim(1e-4, 1)\n",
    "plt.xlim(0, 10)\n",
    "plt.yscale(\"log\")\n",
    "# plt\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/noground_resids.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "rel_us = defaultdict(list)\n",
    "weights = defaultdict(list)\n",
    "for name, dfs in datasets.items():\n",
    "    for df in dfs:\n",
    "        n = len(df)\n",
    "        weights[name] += [1 / n] * n\n",
    "        rel_us[name] += list(df[\"uncertainty\"] / df[\"value\"])\n",
    "    weights[name] = np.array(weights[name])\n",
    "    weights[name] /= len(dfs)\n",
    "\n",
    "bins = np.logspace(-5, 1, 30)\n",
    "# for name, rel_u in rel_us.items():\n",
    "#     plt.plot(bins, np.bincount(rel_u, bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in datasets.keys():\n",
    "    if name.endswith(\"-stat\"):\n",
    "        continue\n",
    "    plt.hist(\n",
    "        rel_us[name],\n",
    "        bins=bins,\n",
    "        histtype=\"step\",\n",
    "        weights=weights[name],\n",
    "        linewidth=2,\n",
    "        label=name,\n",
    "    )\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(1e-5, None)\n",
    "plt.ylabel(\"Proportion in bin\")\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genpareto.fit(zs, floc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit(zs, floc=0, method=\"MLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import birge, random_effects_mle, I2\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "brs = defaultdict(list)\n",
    "taus = defaultdict(list)\n",
    "I2s = defaultdict(list)\n",
    "errscale_ps = defaultdict(list)\n",
    "brs_cont = defaultdict(list)\n",
    "taus_cont = defaultdict(list)\n",
    "I2s_cont = defaultdict(list)\n",
    "errscale_ps_cont = defaultdict(list)\n",
    "\n",
    "sigma_range = defaultdict(list)\n",
    "\n",
    "birge_loglikes = defaultdict(list)\n",
    "re_loglikes = defaultdict(list)\n",
    "fe_loglikes = defaultdict(list)\n",
    "ns = defaultdict(list)\n",
    "exponent_loglikes = defaultdict(list)\n",
    "exponent_grid = np.linspace(0, 1, 101, endpoint=True)\n",
    "# print(exponent_grid)\n",
    "\n",
    "names = defaultdict(list)\n",
    "\n",
    "for name, dfs in datasets.items():\n",
    "    # if name not in ['baker-pdg2011-stat', 'baker-medical', 'baker-pdg2011-both']:\n",
    "    #     continue\n",
    "    print(name)\n",
    "    for i, df in tqdm(enumerate(dfs), total=len(dfs)):\n",
    "        # if name in ['pdg2025-stat', 'pdg2025-both']:\n",
    "        #     if df['pdgid'].iloc[0] in pdg2025bad:\n",
    "        #         continue\n",
    "        #     names[name].append(df['pdgid'].iloc[0])\n",
    "        # else:\n",
    "        names[name].append(quantities[name][i])\n",
    "\n",
    "        values = np.array(df[\"value\"])\n",
    "        sigmas = np.array(df[\"uncertainty\"])\n",
    "\n",
    "        scaler = np.std(values)\n",
    "        if scaler == 0:\n",
    "            continue\n",
    "        values = values / scaler\n",
    "        sigmas = sigmas / scaler\n",
    "\n",
    "        sigma_range[name].append(np.max(sigmas) / np.min(sigmas))\n",
    "\n",
    "        # sigmas = sigmas/np.mean(sigmas)\n",
    "        _, muhat_birge, _, chat = birge(values, sigmas, coverage=0.6827, mle=True)\n",
    "        brs[name].append(chat)\n",
    "        mean_sigma = np.mean(sigmas)\n",
    "\n",
    "        _, muhat_re, _, tau = random_effects_mle(values, sigmas, coverage=0.6827)\n",
    "        taus[name].append(np.mean(tau / sigmas))\n",
    "        I2s[name].append(I2(values, sigmas))\n",
    "        # print(files[name][i], len(values), np.round(I2s[name][-1], 2))\n",
    "        # if I2s[name][-1] <= 0:\n",
    "        #     continue\n",
    "        # if files[name][i] == 'RHO770.csv':\n",
    "        #     continue\n",
    "\n",
    "        tau_grid = np.concatenate(\n",
    "            (np.linspace(0, 1.1, 100) * np.std(values), np.logspace(-4, 4, 100))\n",
    "        )\n",
    "\n",
    "        # theta_grid = np.concatenate((np.linspace(muhat_birge, muhat_re, 100), np.linspace(np.min(values), np.max(values), 100)))\n",
    "        exponent_loglike = np.zeros(len(exponent_grid))\n",
    "        for i, exponent in enumerate(exponent_grid):\n",
    "            # scale = np.sqrt(sigmas[:,np.newaxis,np.newaxis]**2 + ((tau_grid)**2) * (sigmas[:,np.newaxis,np.newaxis]**(2*exponent)))\n",
    "            scale = np.sqrt(\n",
    "                sigmas[:, np.newaxis] ** 2\n",
    "                + ((tau_grid) ** 2) * (sigmas[:, np.newaxis] ** (2 * exponent))\n",
    "            )\n",
    "\n",
    "            w = 1 / scale**2\n",
    "            assert w.shape == (len(values), len(tau_grid))\n",
    "            # print(np.sum(w * values, axis=0).shape)\n",
    "            theta_mle = np.sum(w * values[:, np.newaxis], axis=0) / np.sum(w, axis=0)\n",
    "            assert theta_mle.shape == tau_grid.shape\n",
    "\n",
    "            # print(scale.shape)\n",
    "            loglike = norm.logpdf(values[:, np.newaxis], loc=theta_mle, scale=scale)\n",
    "            assert loglike.shape == (len(values), len(tau_grid))\n",
    "            exponent_loglike[i] = np.max(np.sum(loglike, axis=0))\n",
    "        exponent_loglikes[name].append(exponent_loglike)\n",
    "\n",
    "        # for i in range(10):\n",
    "        #     # generate values with same sigmas but no unaccounted for errors.\n",
    "        #     # to be used as a control when analyzing the distribution of chat and tau\n",
    "        #     values_control = np.random.normal(loc=0, scale=sigmas)\n",
    "        #     _, _, _, chat_cont = birge(values_control, sigmas, coverage=0.6827)\n",
    "        #     brs_cont[name].append(chat_cont)\n",
    "        #     _, _, _, tau_cont = random_effects_mle(values_control, sigmas, coverage=0.6827)\n",
    "        #     taus_cont[name].append(np.mean(tau_cont/sigmas))\n",
    "        #     I2s_cont[name].append(I2(values_control, sigmas))\n",
    "\n",
    "        birge_loglikes[name].append(\n",
    "            np.sum(norm.logpdf(values, loc=muhat_birge, scale=sigmas * chat))\n",
    "        )\n",
    "        if any(np.array(birge_loglikes[name]) == -np.inf):\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "        re_loglikes[name].append(\n",
    "            np.sum(norm.logpdf(values, loc=muhat_re, scale=np.sqrt(sigmas**2 + tau**2)))\n",
    "        )\n",
    "        fe_loglikes[name].append(\n",
    "            np.sum(norm.logpdf(values, loc=muhat_birge, scale=sigmas))\n",
    "        )\n",
    "        ns[name].append(len(df))\n",
    "\n",
    "    birge_loglikes[name] = np.array(birge_loglikes[name])\n",
    "    re_loglikes[name] = np.array(re_loglikes[name])\n",
    "    fe_loglikes[name] = np.array(fe_loglikes[name])\n",
    "    ns[name] = np.array(ns[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(k, np.sum(n)) for k, n in ns.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in exponent_loglikes.keys():\n",
    "    assert len(exponent_loglikes[name]) == len(birge_loglikes[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit Birge model to the whole pdg2025-both dataset\n",
    "# import pymc as pm\n",
    "# with pm.Model() as model:\n",
    "#     N = 100# len(pdg2025_both_dfs)\n",
    "#     means = np.array([np.mean(df['value']) for df in pdg2025_both_dfs[:N]])\n",
    "#     stds = np.array([np.std(df['value']) for df in pdg2025_both_dfs[:N]])\n",
    "#     theta = pm.Normal('theta', mu=means, sigma=stds*2, shape=N)\n",
    "#     alpha = pm.Uniform('alpha', lower=0, upper=10)\n",
    "#     beta = pm.Uniform('beta', lower=0, upper=10)\n",
    "#     c = pm.Gamma('c', alpha=alpha, beta=beta, shape=N)\n",
    "#     for i in range(N):\n",
    "#         values = np.array(pdg2025_both_dfs[i]['value'])\n",
    "#         sigmas = np.array(pdg2025_both_dfs[i]['uncertainty'])\n",
    "#         scale = c[i]*sigmas\n",
    "#         y = pm.Normal(f'y_{i}', mu=theta[i], sigma=scale, observed=values)\n",
    "#     trace = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, theta_mle, _, tau_mle = random_effects_mle(values, sigmas, coverage=0.6827)\n",
    "# theta_mle, tau_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_idx, tau_idx = np.unravel_index(np.argmax(loglike), loglike.shape)\n",
    "# theta_grid[theta_idx], tau_grid[tau_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(norm.logpdf(values, loc=theta_mle, scale=np.sqrt(sigmas**2+tau_mle**2))))\n",
    "# print(np.sum(norm.logpdf(values, loc=theta_grid[theta_idx], scale=np.sqrt(sigmas**2+tau_grid[tau_idx]**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "best_exponents = {}\n",
    "exponent_cis = {}\n",
    "for name, exponent_loglike in exponent_loglikes.items():\n",
    "    exponent_loglike = np.sum(exponent_loglike, axis=0)\n",
    "    best_exponents[name] = exponent_grid[np.argmax(exponent_loglike)]\n",
    "    logratio = 2 * (np.max(exponent_loglike) - exponent_loglike)\n",
    "    within = logratio < chi2.ppf(0.95, df=1)\n",
    "    idx_l = np.argmax(within)\n",
    "    idx_u = -np.argmax(within[::-1]) - 1\n",
    "    low = exponent_grid[idx_l]\n",
    "    high = exponent_grid[idx_u]\n",
    "    exponent_cis[name] = [low, high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for name in best_exponents.keys():\n",
    "    rows.append(\n",
    "        [\n",
    "            r\"\\texttt{\" + name + r\"}\",\n",
    "            f\"${best_exponents[name]:.2f}$\",\n",
    "            f\"$[{exponent_cis[name][0]:.2f}, {exponent_cis[name][1]:.2f}]$\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# use np to save latex table with & separator\n",
    "txt = \" \\\\\\\\\\n\".join([\" & \".join(row) for row in rows])\n",
    "print(txt)\n",
    "with open(\"tables/noground-bakermodel.tex\", \"w\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(exponent_grid, exponent_loglikes['baker-pdg2011-both'][4])\n",
    "# for exponent_loglike in exponent_loglikes['baker-pdg2011-stat']:\n",
    "# plt.plot(exponent_grid, (exponent_loglike - np.max(exponent_loglike))/(np.max(exponent_loglike)-np.min(exponent_loglike)))\n",
    "which = \"baker-pdg2011-stat\"\n",
    "for name, exponent_loglike in exponent_loglikes.items():\n",
    "    exponent_loglike = np.sum(np.array(exponent_loglike), axis=0)\n",
    "    plt.plot(exponent_grid, exponent_loglike - np.max(exponent_loglike), label=name)\n",
    "    # print(len(exponent_loglike))\n",
    "    # plt.axvline(exponent_grid[np.argmax(exponent_loglike)], color='black', linestyle='--')\n",
    "plt.ylim(-60, 5)\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sum(np.array(exponent_loglikes[\"bailey-pdg-stable\"]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exponent_grid, np.sum(np.array(exponent_loglikes[\"pdg2025-stat\"]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    re_loglikes[\"pdg2025-both\"], np.array(exponent_loglikes[\"pdg2025-both\"])[:, 0], s=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = \"pdg2025-both\"\n",
    "diffs = np.array(exponent_loglikes[which])[:, 0] - re_loglikes[which]\n",
    "print(np.sum(diffs))\n",
    "bad_idx = np.argmax(diffs)\n",
    "\n",
    "name = names[which][bad_idx]\n",
    "print(name)\n",
    "for df in datasets[which]:\n",
    "    if df[\"pdgid\"].iloc[0] == name:\n",
    "        break\n",
    "\n",
    "values = np.array(df[\"value\"])\n",
    "sigmas = np.array(df[\"uncertainty\"])\n",
    "print(values, sigmas)\n",
    "_, theta_mle, _, tau_mle = random_effects_mle(values, sigmas, coverage=0.6827)\n",
    "print(theta_mle, tau_mle)\n",
    "tau_grid = np.concatenate(\n",
    "    (np.logspace(-4, 4, 200) * max(sigmas), np.logspace(-4, 4, 200))\n",
    ")\n",
    "theta_grid = np.concatenate(\n",
    "    (\n",
    "        np.linspace(muhat_birge, muhat_re, 100),\n",
    "        np.linspace(np.min(values), np.max(values), 100),\n",
    "    )\n",
    ")\n",
    "scale = np.sqrt(sigmas[:, np.newaxis, np.newaxis] ** 2 + (tau_grid) ** 2)\n",
    "loglike = np.sum(\n",
    "    norm.logpdf(\n",
    "        values[:, np.newaxis, np.newaxis], loc=theta_grid[:, np.newaxis], scale=scale\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "theta_idx, tau_idx = np.unravel_index(np.argmax(loglike), loglike.shape)\n",
    "print(theta_grid[theta_idx], tau_grid[tau_idx])\n",
    "\n",
    "print(np.sum(norm.logpdf(values, loc=theta_mle, scale=np.sqrt(sigmas**2 + tau_mle**2))))\n",
    "print(\n",
    "    np.sum(\n",
    "        norm.logpdf(\n",
    "            values,\n",
    "            loc=theta_grid[theta_idx],\n",
    "            scale=np.sqrt(sigmas**2 + tau_grid[tau_idx] ** 2),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 4), sharey=True, gridspec_kw={\"wspace\": 0.1})\n",
    "import seaborn as sns\n",
    "\n",
    "for i, data in enumerate([ns, I2s, sigma_range]):\n",
    "    # parts = axs[i].violinplot(list(data.values()), vert=False, showmedians=False, showextrema=False, log=True)\n",
    "    # axs[i].tick_params(top=True, right=True)\n",
    "    # for j in range(len(data)):\n",
    "    #     axs[i].axhline(j+1, color='grey', linewidth=0.5, linestyle=':')\n",
    "    # for pc in parts['bodies']:\n",
    "    #     pc.set_facecolor('grey')\n",
    "    #     pc.set_edgecolor('none')\n",
    "    #     pc.set_alpha(1)\n",
    "    # # add median\n",
    "    # print(medians)\n",
    "    sns.stripplot(\n",
    "        data=data, ax=axs[i], orient=\"h\", size=1.5, jitter=0.3, color=\"black\", alpha=0.4\n",
    "    )\n",
    "\n",
    "    medians = [np.median(data[name]) for name in data.keys()]\n",
    "    print(medians)\n",
    "    axs[i].scatter(\n",
    "        medians,\n",
    "        np.array(range(len(data))),\n",
    "        marker=\"|\",\n",
    "        s=70,\n",
    "        c=\"red\",\n",
    "        alpha=1,\n",
    "        linewidths=1,\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "\n",
    "axs[0].set_xlabel(\"Number of results\")\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[1].set_xlabel(\"$I^2$\")\n",
    "axs[2].set_xlabel(\"$\\sigma_{\\mathrm{max}}/\\sigma_{\\mathrm{min}}$\")\n",
    "\n",
    "# add y ticks\n",
    "axs[0].set_yticks(np.array(range(len(datasets))))\n",
    "axs[0].set_yticklabels([nice_names[name] for name in datasets.keys()])\n",
    "# flip y axis\n",
    "# axs[0].invert_yaxis()\n",
    "axs[2].set_xlim(1, 1e8)\n",
    "axs[2].set_xscale(\"log\")\n",
    "# axs[0].set_xlim(0, None)\n",
    "# add right ticks\n",
    "axs[0].tick_params(top=True, right=True)\n",
    "axs[1].tick_params(top=True, right=True)\n",
    "axs[2].tick_params(top=True, right=True)\n",
    "# add minor ticks to axs[2]\n",
    "axs[2].set_xticks(np.logspace(0, 8, 9))\n",
    "# axs[2].set_xticklabels([f'{x:.0f}' for x in np.logspace(0, 8, 9)])\n",
    "axs[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/noground_violin.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "rows_numeric = []\n",
    "\n",
    "\n",
    "def format_number(x):\n",
    "    if x == -np.inf:\n",
    "        return r\"$\\approx-\\infty$\"\n",
    "    if x == np.inf:\n",
    "        return r\"$\\approx\\infty$\"\n",
    "    if x >= 1e3 or x <= -1e3:\n",
    "        return f\"${str(int(x))}$\"\n",
    "    return r\"${0:.3g}$\".format(x)\n",
    "\n",
    "\n",
    "def add_bold(string):\n",
    "    return r\"$\\mathbf{\" + string.strip(\"$\") + r\"}$\"\n",
    "\n",
    "\n",
    "for name in datasets.keys():\n",
    "    loglikes = np.array([birge_loglikes[name], re_loglikes[name], fe_loglikes[name]])\n",
    "    total_loglikes = np.sum(loglikes, axis=1)\n",
    "    assert len(total_loglikes) == 3\n",
    "    ks = np.array([2, 2, 1])\n",
    "\n",
    "    bics = ks[:, np.newaxis] * np.log(np.array(ns[name])) - 2 * loglikes\n",
    "    aics = 2 * ks[:, np.newaxis] - 2 * loglikes\n",
    "    bics = np.mean(bics, axis=1)\n",
    "    aics = np.mean(aics, axis=1)\n",
    "    numbers = list(-total_loglikes) + list(bics) + list(aics)\n",
    "    numbers_str = [format_number(x) for x in numbers]\n",
    "    rows.append([r\"\\texttt{\" + name + r\"}\"] + numbers_str)\n",
    "    rows_numeric.append(numbers)\n",
    "rows_numeric = np.array(rows_numeric)\n",
    "\n",
    "best_loglike = np.argmin(rows_numeric[:, :3], axis=1)\n",
    "best_bic = np.argmin(rows_numeric[:, 3:6], axis=1)\n",
    "best_aic = np.argmin(rows_numeric[:, 6:], axis=1)\n",
    "\n",
    "# add bold to the best model\n",
    "for i, row in enumerate(rows):\n",
    "    row[best_loglike[i] + 1] = add_bold(row[best_loglike[i] + 1])\n",
    "    row[best_bic[i] + 4] = add_bold(row[best_bic[i] + 4])\n",
    "    row[best_aic[i] + 7] = add_bold(row[best_aic[i] + 7])\n",
    "\n",
    "# use np to save latex table with & separator\n",
    "txt = \" \\\\\\\\\\n\".join([\" & \".join(row) for row in rows])\n",
    "print(txt)\n",
    "with open(\"tables/noground-loglike.tex\", \"w\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxn = np.max([np.max(n) for n in ns.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    4,\n",
    "    2,\n",
    "    figsize=(7, 9),\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    gridspec_kw={\"wspace\": 0, \"hspace\": 0.1},\n",
    ")\n",
    "\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    ax = axs.flatten()[i]\n",
    "\n",
    "    sc = ax.scatter(\n",
    "        birge_loglikes[name],\n",
    "        re_loglikes[name],\n",
    "        marker=\"x\",\n",
    "        s=10,\n",
    "        c=ns[name],\n",
    "        vmin=0,\n",
    "        vmax=maxn,\n",
    "        facecolor=\"none\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    ymin = np.min([np.min(birge_loglikes[name]), np.min(re_loglikes[name])])\n",
    "    ymax = np.max([np.max(birge_loglikes[name]), np.max(re_loglikes[name])])\n",
    "    ax.plot(\n",
    "        [ymin - 1, ymax + 1],\n",
    "        [ymin - 1, ymax + 1],\n",
    "        color=\"red\",\n",
    "        linewidth=1,\n",
    "        linestyle=\":\",\n",
    "    )\n",
    "    # set aspect ratio to 1\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_xlim(ymin - 0.2, ymax + 0.2)\n",
    "    ax.set_ylim(ymin - 0.2, ymax + 0.2)\n",
    "    ax.set_xlabel(\"BR log-likelihood\")\n",
    "    ax.set_ylabel(\"RE log-likelihood\")\n",
    "    # plt.colorbar(label='Number of measurements')\n",
    "    re_better_count = np.sum(re_loglikes[name] > birge_loglikes[name])\n",
    "    birge_better_count = np.sum(birge_loglikes[name] > re_loglikes[name])\n",
    "    total_count = len(birge_loglikes[name])\n",
    "    re_better_frac = f\"{re_better_count}/{total_count}\"\n",
    "    birge_better_frac = f\"{birge_better_count}/{total_count}\"\n",
    "    ax.text(\n",
    "        0.3,\n",
    "        0.5,\n",
    "        f\"RE better ({re_better_frac})\",\n",
    "        fontsize=8,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"black\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        rotation=45,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.3,\n",
    "        f\"BR better ({birge_better_frac})\",\n",
    "        fontsize=8,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"black\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        rotation=45,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.05, 0.95, nice_names_break[name], transform=ax.transAxes, ha=\"left\", va=\"top\"\n",
    "    )\n",
    "fig.suptitle(\n",
    "    \"Random Effects and Birge Ratio MLE log-likelihoods for each set of results\"\n",
    ")\n",
    "# axs.flatten()[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax, label=\"Number of measurements\")\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.savefig(\"figs/noground_loglike.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "xrange = 50\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    diff = re_loglikes[name] - birge_loglikes[name]\n",
    "    plt.scatter(\n",
    "        diff,\n",
    "        -np.ones(len(diff)) * i,\n",
    "        marker=\"|\",\n",
    "        s=20,\n",
    "        vmin=0,\n",
    "        vmax=maxn,\n",
    "        facecolor=\"none\",\n",
    "        linewidths=0.4,\n",
    "        c=\"black\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.axhline(-i, color=\"grey\", linewidth=0.3, linestyle=\":\")\n",
    "\n",
    "    re_better_count = np.sum(re_loglikes[name] > birge_loglikes[name])\n",
    "    birge_better_count = np.sum(birge_loglikes[name] > re_loglikes[name])\n",
    "    total_count = len(birge_loglikes[name])\n",
    "    re_better_frac = f\"{re_better_count}/{total_count}\"\n",
    "    birge_better_frac = f\"{birge_better_count}/{total_count}\"\n",
    "    plt.text(\n",
    "        xrange / 20,\n",
    "        -i + 0.3,\n",
    "        f\"RE better: {re_better_frac}\",\n",
    "        fontsize=8,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"black\",\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "    )\n",
    "    plt.text(\n",
    "        -xrange / 20,\n",
    "        -i + 0.3,\n",
    "        f\"BR better: {birge_better_frac}\",\n",
    "        fontsize=8,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"black\",\n",
    "        ha=\"right\",\n",
    "        va=\"center\",\n",
    "    )\n",
    "    if any(diff > xrange):\n",
    "        num_outliers = np.sum(diff > xrange)\n",
    "        plt.text(\n",
    "            xrange,\n",
    "            -i + 0.3,\n",
    "            f\"{num_outliers}\" + r\"$\\rightarrow$\",\n",
    "            fontsize=8,\n",
    "            fontweight=\"bold\",\n",
    "            color=\"black\",\n",
    "            ha=\"right\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "    if any(diff < -xrange):\n",
    "        num_outliers = np.sum(diff < -xrange)\n",
    "        plt.text(\n",
    "            -xrange,\n",
    "            -i + 0.3,\n",
    "            r\"$\\leftarrow$\" + f\"{num_outliers}\",\n",
    "            fontsize=8,\n",
    "            fontweight=\"bold\",\n",
    "            color=\"black\",\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"RE log-likelihood - BR log-likelihood\")\n",
    "plt.ylabel(\"Set of results\")\n",
    "plt.yticks(range(0, -len(datasets), -1), [nice_names[name] for name in datasets.keys()])\n",
    "plt.tight_layout()\n",
    "plt.axvline(0, color=\"red\", linewidth=1, linestyle=\":\")\n",
    "xmax = max(np.abs(np.array(plt.xlim())))\n",
    "plt.xlim(-xrange, xrange)\n",
    "plt.ylim(-len(datasets) + 0.5, 0.8)\n",
    "plt.gca().tick_params(top=True)\n",
    "plt.savefig(\"figs/noground_loglike_diff.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
