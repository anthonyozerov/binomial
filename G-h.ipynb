{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UQ for G and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from methods import binomial_method, random_effects_dl, random_effects_hksj, birge, binomial_sigmacdf, vniim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "plt.rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravitational Constant\n",
    "g_df = pd.read_csv(\"data/G.csv\")\n",
    "g_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planck Constant\n",
    "h_df = pd.read_csv(\"data/h.csv\")\n",
    "h_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "datasets = {\n",
    "    \"G\": {\n",
    "        \"values\": np.array(g_df[\"value\"]),\n",
    "        \"uncertainties\": np.array(g_df[\"sigma\"]),\n",
    "        \"names\": g_df[\"id\"],\n",
    "        \"xlabel\": \"Gravitational constant $G$ [10$^{-11}$ m$^3$ kg$^{-1}$ s$^{-2}$]\",\n",
    "        \"codata_value\": 6.67430,\n",
    "        \"codata_sigma\": 0.00015,\n",
    "    },\n",
    "    \"h\": {\n",
    "        \"values\": np.array(h_df[\"value\"]),\n",
    "        \"uncertainties\": np.array(h_df[\"sigma\"]),\n",
    "        \"names\": h_df[\"id\"],\n",
    "        \"xlabel\": \"Planck constant $h$ [J s]\",\n",
    "        \"codata_value\": 6.62606957,\n",
    "        \"codata_sigma\": 0.00000029,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, d in datasets.items():\n",
    "    values = d[\"values\"]\n",
    "    uncertainties = d[\"uncertainties\"]\n",
    "    names = d[\"names\"]\n",
    "    # plot figure of the dataframe values and corresponding intervals\n",
    "    # so that each value and interval is in one row\n",
    "    n = len(values)\n",
    "    p = 0.5\n",
    "\n",
    "    for i in range(2):\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.errorbar(values, -np.arange(n), xerr=uncertainties, fmt=\"o\", color=\"black\", markersize=3, linewidth=1)\n",
    "        plt.yticks(-np.arange(n), names)\n",
    "        # point xticks inwards, and add top ticks\n",
    "        plt.tick_params(axis=\"x\", direction=\"in\", top=True)\n",
    "\n",
    "        target = 0.6827\n",
    "        tail_prob = (1 - target) / 2\n",
    "\n",
    "        lower, tail_prob_achieved = binomial_method(\n",
    "            np.sort(values), p=p, target=tail_prob, which=\"lower\"\n",
    "        )\n",
    "        upper, _ = binomial_method(\n",
    "            np.sort(values), p=p, target=tail_prob, which=\"upper\"\n",
    "        )\n",
    "\n",
    "        interval_binomial = [lower, upper]\n",
    "\n",
    "        # for s in range(200):\n",
    "        #     values_b = np.random.normal(values, uncertainties)\n",
    "        #     values_b = np.sort(values_b)\n",
    "        #     lower, _ = binomial_method(values_b, p=p, target=tail_prob, which=\"lower\")\n",
    "        #     upper, _ = binomial_method(values_b, p=p, target=tail_prob, which=\"upper\")\n",
    "        #     plt.axvline(lower, color=\"grey\", alpha=0.05)\n",
    "        #     plt.axvline(upper, color=\"grey\", alpha=0.05)\n",
    "\n",
    "        if i == 0:\n",
    "            z_alpha = norm.ppf(1 - tail_prob)\n",
    "            t_alpha = t.ppf(1 - tail_prob, n - 1)\n",
    "            target_cov = 1 - (2 * tail_prob)\n",
    "        else:\n",
    "            z_alpha = norm.ppf(1 - tail_prob_achieved)\n",
    "            t_alpha = t.ppf(1 - tail_prob_achieved, n - 1)\n",
    "            target_cov = 1 - (2 * tail_prob_achieved)\n",
    "\n",
    "        # calculate using random-effects model (DL)\n",
    "        interval_re_dl, muhat, sigma, _ = random_effects_dl(\n",
    "            values, uncertainties, zalpha=z_alpha\n",
    "        )\n",
    "\n",
    "        # calculate using random-effects model (HKSJ)\n",
    "        interval_re_hksj, muhat, sigma, _ = random_effects_hksj(\n",
    "            values, uncertainties, talpha=t_alpha\n",
    "        )\n",
    "\n",
    "        # calculate using Birge ratio\n",
    "        interval_birge, muhat, sigma, ratio = birge(values, uncertainties, zalpha=z_alpha)\n",
    "        print(ratio)\n",
    "\n",
    "        interval_vniim, muhat = vniim(values, uncertainties, zalpha=z_alpha)\n",
    "\n",
    "\n",
    "        # for s in range(400):\n",
    "        #     values_b = np.random.normal(values, uncertainties)\n",
    "        #     values_b = np.sort(values_b)\n",
    "        #     lower, _ = binomial_method(values_b, p=p, target=tail_prob, which=\"lower\")\n",
    "        #     upper, _ = binomial_method(values_b, p=p, target=tail_prob, which=\"upper\")\n",
    "        #     plt.axvline(lower, color=\"red\", alpha=0.015)\n",
    "        #     plt.axvline(upper, color=\"red\", alpha=0.015)\n",
    "\n",
    "        if i == 0:\n",
    "            z_alpha = norm.ppf(1 - tail_prob)\n",
    "            t_alpha = t.ppf(1 - tail_prob, n - 1)\n",
    "            target_cov = 1 - (2 * tail_prob)\n",
    "        else:\n",
    "            z_alpha = norm.ppf(1 - tail_prob_achieved)\n",
    "            t_alpha = t.ppf(1 - tail_prob_achieved, n - 1)\n",
    "            target_cov = 1 - (2 * tail_prob_achieved)\n",
    "\n",
    "        # calculate using random-effects model (DL)\n",
    "        # CODATA\n",
    "        muhat, sigma = d[\"codata_value\"], d[\"codata_sigma\"]\n",
    "        interval_codata = [muhat - sigma * z_alpha, muhat + sigma * z_alpha]\n",
    "\n",
    "        interval_bsigma, cvg = binomial_sigmacdf(values, uncertainties, p=p, coverage=target_cov)\n",
    "        # print(cvg)\n",
    "\n",
    "        plt.axvline(interval_codata[0], color=\"green\", linestyle=\":\", label=\"CODATA\")\n",
    "        plt.axvline(interval_codata[1], color=\"green\", linestyle=\":\")\n",
    "\n",
    "        plt.axvline(interval_binomial[0], color=\"red\", linestyle=\"--\", label=\"Binomial\")\n",
    "        plt.axvline(interval_binomial[1], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        # plt.axvline(interval_bsigma[0], color=\"green\", linestyle=\"-.\", label=\"Binomial $\\sigma$CDF\")\n",
    "        # plt.axvline(interval_bsigma[1], color=\"green\", linestyle=\"-.\")\n",
    "\n",
    "        plt.axvline(\n",
    "            interval_re_hksj[0],\n",
    "            color=\"blue\",\n",
    "            linestyle=\"-.\",\n",
    "            label=\"Random Effects (RE)\",\n",
    "        )\n",
    "        plt.axvline(interval_re_hksj[1], color=\"blue\", linestyle=\"-.\")\n",
    "\n",
    "        plt.axvline(interval_vniim[0], color='purple', linestyle='--')\n",
    "        plt.axvline(interval_vniim[1], color='purple', linestyle='--')\n",
    "\n",
    "\n",
    "        # plt.axvline(\n",
    "        #     interval_birge[0], color=\"grey\", linestyle=\"--\", label=\"Birge Ratio (BR)\"\n",
    "        # )\n",
    "        # plt.axvline(interval_birge[1], color=\"grey\", linestyle=\"--\")\n",
    "\n",
    "        # plt.legend(frameon=False, loc=\"lower left\")\n",
    "\n",
    "        plt.xlabel(d[\"xlabel\"])\n",
    "        # plt.title(\n",
    "        #     rf\"Aggregating estimates of ${var}$ with target coverage {np.round(target_cov*100, 1)}\\%\"\n",
    "        # )\n",
    "        print(np.round(target_cov*100,1))\n",
    "        plt.savefig(f\"figs/{var}{i}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.savefig(f\"figs/{var}{i}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.read_csv('data/c.csv', comment='#')\n",
    "# c_df = c_df.groupby('author').apply(lambda x: x.sample(1), include_groups=False).reset_index(drop=True)\n",
    "# c_df = c_df.sort_values(by='year')\n",
    "c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "series = np.array(c_df.value > 299792.458).astype(int)\n",
    "acf(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.graphics.tsa.plot_acf(c_df.value, lags=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(c_df.value<299792.458), np.sum(c_df.value>299792.458))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import sign_rank\n",
    "def get_lowers_uppers(df, years, get_flip_covs=False):\n",
    "    lowers, uppers, covs, flip_covs = [], [], [], []\n",
    "    for year in years:\n",
    "        subset = df[df.year <= year]\n",
    "        values = np.sort(subset.value)\n",
    "        # lower, alpha = binomial_method(values, which='lower')\n",
    "        # upper, _ = binomial_method(values, which='upper')\n",
    "        (lower, upper), alpha = sign_rank(values)\n",
    "        lowers.append(lower)\n",
    "        uppers.append(upper)\n",
    "        covs.append(1-2*alpha)\n",
    "        if get_flip_covs:\n",
    "            flip_covs.append(1 - flip_test(values, h0=lower, tail='lower', mode='median') - flip_test(values, h0=upper, tail='upper', mode='median'))\n",
    "    lowers = np.array(lowers)\n",
    "    uppers = np.array(uppers)\n",
    "    covs = np.array(covs)\n",
    "    flip_covs = np.array(flip_covs)\n",
    "    return lowers, uppers, covs, flip_covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowers = []\n",
    "uppers = []\n",
    "years = np.arange(c_df.year.iloc[2], c_df.year.iloc[-1]+1, 0.5)\n",
    "lowers, uppers, covs, flip_covs = get_lowers_uppers(c_df, years, get_flip_covs=True)\n",
    "\n",
    "c = 299792.458\n",
    "fig, axs = plt.subplots(1, 2, figsize=(4.5, 4), sharey=True, width_ratios=(3, 1))\n",
    "# plt.figure(figsize=(4,4))\n",
    "axs[0].plot(lowers-c, years, color='red')\n",
    "axs[0].plot(uppers-c, years, color='red')\n",
    "for i in range(10):\n",
    "    smaller = c\n",
    "    unique_author = c_df.groupby('author').apply(lambda x: x.sample(1), include_groups=False).reset_index(drop=True)\n",
    "    unique_author = unique_author.sort_values(by='year')\n",
    "    lowers, uppers, _, _ = get_lowers_uppers(unique_author, years)\n",
    "    axs[0].fill_betweenx(years, lowers-c, uppers-c, color='red', alpha=0.04)\n",
    "axs[0].plot(c_df.value[2:]-c, c_df.year[2:], 'o', color='black', markersize=3)\n",
    "axs[0].set_xscale('symlog')\n",
    "axs[0].axvline(0, color='black', linestyle='--')\n",
    "axs[0].set_xlabel('Deviation from true value [km/s]')\n",
    "axs[0].set_ylabel('Year')\n",
    "# reverse y axis\n",
    "axs[0].invert_yaxis()\n",
    "axs[1].plot(covs, years, color='black', linewidth=1)\n",
    "axs[1].plot(flip_covs, years, color='grey', linewidth=1)\n",
    "axs[1].set_xlabel('Nominal coverage')\n",
    "# point ticks inwards and add top and right ticks\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)\n",
    "plt.tight_layout()\n",
    "for n, label in enumerate(axs[0].xaxis.get_ticklabels()):\n",
    "    if n % 2 != 0:\n",
    "        label.set_visible(False)\n",
    "plt.savefig('figs/c.pdf', bbox_inches='tight')\n",
    "plt.savefig('figs/c.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import sign_rank_test\n",
    "\n",
    "sign_rank_test(c_df.value, h0_median=299792.458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import sign_rank\n",
    "sign_rank(np.array(c_df.value), coverage=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import flip_test, flip_interval\n",
    "print(flip_test(c_df.value, h0=299784.3, tail='both', mode='median'))\n",
    "print(flip_test(c_df.value, h0=299794.0, tail='both', mode='median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = []\n",
    "in_intervals = []\n",
    "covs = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(200)):\n",
    "    values = np.random.standard_normal(33)\n",
    "    p_values.append(flip_test(values, mode='median'))\n",
    "    [lower, upper], cov = flip_interval(values, mode='median', coverage=0.95, boot=False)\n",
    "    in_intervals.append(lower <= 0 <= upper)\n",
    "    # covs.append(cov)\n",
    "\n",
    "print(np.nanmean(in_intervals))\n",
    "plt.hist(covs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values, lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual 95% confidence interval for mean of normal\n",
    "z = 1.96\n",
    "mean = np.mean(values)\n",
    "lower = mean - z * np.std(values) / np.sqrt(len(values))\n",
    "upper = mean + z * np.std(values) / np.sqrt(len(values))\n",
    "print(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flip_test(values, h0=lower, mode='median', tail='lower'))\n",
    "print(flip_test(values, h0=upper, mode='median', tail='upper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import flip_interval\n",
    "flip_interval(values, mode='mean', coverage=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_values, bins=20, cumulative=True, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_df = pd.read_csv(\"data/rho.csv\", comment='#')\n",
    "\n",
    "datasets = {\n",
    "    'rho': rho_df,\n",
    "    'h': h_df,\n",
    "    'G': g_df,\n",
    "    # 'c': c_df,\n",
    "}\n",
    "truths = {\n",
    "    'rho': 5.513,\n",
    "    'c': 299792.458,\n",
    "    'G': None,\n",
    "    'h': None,\n",
    "}\n",
    "yscales = {\n",
    "    'rho': 'linear',\n",
    "    'c': 'symlog',\n",
    "    'G': 'linear',\n",
    "    'h': 'linear',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(rho_df.value<truths['rho']), np.sum(rho_df.value>truths['rho']))\n",
    "rho_df = rho_df.groupby('author').apply(lambda x: x.sample(1), include_groups=False).reset_index(drop=True)\n",
    "rho_df = rho_df.sort_values(by='year')\n",
    "lowers = []\n",
    "uppers = []\n",
    "years = np.arange(rho_df.year.iloc[2], rho_df.year.iloc[-1]+1, 0.5)\n",
    "covs = []\n",
    "for year in years:\n",
    "    subset = rho_df[rho_df.year <= year]\n",
    "    values = np.sort(subset.value)\n",
    "    lower, alpha = binomial_method(values, which='lower')\n",
    "    upper, _ = binomial_method(values, which='upper')\n",
    "    lowers.append(lower)\n",
    "    uppers.append(upper)\n",
    "    covs.append(1-2*alpha)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(4.5, 4), sharey=True, width_ratios=(3, 1))\n",
    "# plt.figure(figsize=(4,4))\n",
    "axs[0].plot(np.array(lowers), years, color='red')\n",
    "axs[0].plot(np.array(uppers), years, color='red')\n",
    "axs[0].plot(rho_df.value[2:], rho_df.year[2:], 'o', color='black', markersize=3)\n",
    "# axs[0].set_xscale('symlog')\n",
    "axs[0].axvline(truths['rho'], color='black', linestyle='--')\n",
    "axs[0].set_xlabel(r'Estimate of $\\rho_\\bigoplus$')\n",
    "axs[0].set_ylabel('Year')\n",
    "# reverse y axis\n",
    "axs[0].invert_yaxis()\n",
    "axs[1].plot(covs, years, color='black', linewidth=1)\n",
    "axs[1].set_xlabel('Nominal coverage')\n",
    "# point ticks inwards and add top and right ticks\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)\n",
    "plt.tight_layout()\n",
    "for n, label in enumerate(axs[0].xaxis.get_ticklabels()):\n",
    "    if n % 2 != 0:\n",
    "        label.set_visible(False)\n",
    "# plt.savefig('figs/c.pdf', bbox_inches='tight')\n",
    "plt.savefig('figs/rho.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(c_df.value<299792.458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(c_df.year,c_df.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df = pd.read_csv(\"data/G.csv\", comment='#')\n",
    "h_df = pd.read_csv(\"data/h.csv\")\n",
    "rho_df = pd.read_csv(\"data/rho.csv\", comment='#')\n",
    "rho_df = rho_df.groupby('author').apply(lambda x: x.sample(1), include_groups=False).reset_index(drop=True)\n",
    "rho_df = rho_df.sort_values(by='year')\n",
    "# rho_df = rho_df.dropna()\n",
    "h_df = h_df.sort_values(by='year')\n",
    "rho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = 'rho'\n",
    "use_sigma = False\n",
    "dataset = datasets[which]\n",
    "if yscales[which] == 'symlog':\n",
    "    offset = truths[which]\n",
    "else:\n",
    "    offset = 0\n",
    "\n",
    "lowers_b = [binomial_method(np.sort(dataset.value[:i]), which='lower')[0] for i in range(3,len(dataset)+1)]\n",
    "uppers_b = [binomial_method(np.sort(dataset.value[:i]), which='upper')[0] for i in range(3,len(dataset)+1)]\n",
    "if 'sigma' in dataset.columns and use_sigma:\n",
    "    intervals_re = [random_effects_hksj(dataset.value[:i], dataset.sigma[:i], coverage=0.6827)[0] for i in range(3,len(dataset)+1)]\n",
    "    lowers_re = [interval[0] for interval in intervals_re]\n",
    "    uppers_re = [interval[1] for interval in intervals_re]\n",
    "    intervals_birge = [birge(dataset.value[:i], dataset.sigma[:i], coverage=0.6827)[0] for i in range(3,len(dataset)+1)]\n",
    "    lowers_birge = [interval[0] for interval in intervals_birge]\n",
    "    uppers_birge = [interval[1] for interval in intervals_birge]\n",
    "years = dataset.year\n",
    "\n",
    "plt.plot(years[2:], np.array(lowers_b)-offset, color='red')\n",
    "plt.plot(years[2:], np.array(uppers_b)-offset, color='red')\n",
    "\n",
    "if 'sigma' in dataset.columns and use_sigma:\n",
    "    plt.plot(years[2:], np.array(lowers_re)-offset, color='black')\n",
    "    plt.plot(years[2:], np.array(uppers_re)-offset, color='black')\n",
    "    plt.plot(years[2:], np.array(lowers_birge)-offset, color='blue')\n",
    "    plt.plot(years[2:], np.array(uppers_birge)-offset, color='blue')\n",
    "\n",
    "if 'sigma' in dataset.columns and use_sigma:\n",
    "    plt.errorbar(dataset.year, dataset.value-offset, yerr=dataset.sigma, fmt='o', color='grey')\n",
    "else:\n",
    "    plt.errorbar(dataset.year, dataset.value-offset, fmt='o', color='grey')\n",
    "\n",
    "if truths[which] is not None:\n",
    "    plt.axhline(truths[which]-offset, color='red', linestyle='--')\n",
    "if which == 'rho':\n",
    "    plt.axhline(5.5247+0.0013,color='grey',linewidth=0.2)\n",
    "    plt.axhline(5.5247-0.0013,color='grey',linewidth=0.2)\n",
    "if yscales[which] == 'symlog':\n",
    "    plt.yscale('symlog')\n",
    "plt.ylabel('Estimate of $G$')\n",
    "plt.xlabel('Year')\n",
    "plt.title(r'Intervals for $G$ over time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
