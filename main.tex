\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{parskip}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}
\usepackage{microtype}

\usepackage{enumitem}
\setlist{nosep}

\usepackage{minted}

\usepackage{natbib}
\bibliographystyle{unsrtnat}
\renewcommand{\bibsection}{\section{\bibname}} %make bibliography show in TOC

\renewcommand{\listingscaption}{Code Snippet}

\usepackage[font={small},labelfont={bf}]{caption}

\renewcommand{\footnoterule}{\vfill\kern -3pt \hrule width 0.4\columnwidth \kern 2.6pt}

\usepackage{relsize}
\AtBeginEnvironment{quote}{\smaller}

\usepackage{fancyhdr}
\pagestyle{fancy}

\setcounter{tocdepth}{2}

\title{Aggregating experiments}
\author{Anthony Ozerov}
\date{\today}



\begin{document}

\maketitle

\begin{abstract}
Many applied studies seek to aggregate multiple estimates of a target quantity $\theta$ from different experiments/methods, taking into account both systematic errors (or, generally, between-study heterogeneity) and within-study noise. Inference typically involves strong assumptions, such as a normal random-effects model. We apply the sign test from nonparametric statistics, which uses weaker assumption: that each study independently underestimates the true value $\theta$ with probability $1/2$. Producing exact confidence intervals for $\theta$ under this assumption is easy. Our simulations show that the resulting intervals are competitive with intervals from commonly-used procedures like DerSimonian \& Laird and HKSJ. The sign test is further invariant to monotone transformations, robust to outliers, and oddly does not require any information on the within-study noise. We think it might be useful in the physical sciences, and present retrospective case studies using historical estimates of physical constants.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}\label{introduction}

Combining multiple estimates of a quantity into one is common across the sciences. In medicine and the social sciences, meta-analyses find studies/experiments which estimate the same quantity (e.g.~the treatment effect of a medicine) and combine them into one estimate, with associated uncertainties. In Earth science, ``intercomparison'' studies take models from different teams and study their differences, sometimes attempting to reconcile them and produce one better estimate. In physics, the values of fundamental constants and measuremnets of fundamental particles obtained in different experiments must also be somehow reconciled.

One key issue that arises is systematic uncertainty, where an estimate of a quantity may have some error not due to noise which can't be reduced by collecting a larger sample or running the experiment for longer. This error can be due to issues in the experiment like a non-randomly-sampled population, or an instrument which is miscalibrated, or analysis issues like approximations and model imperfections. Empirically, this can be seen when results from different experiments/teams/methods are far outside of each other's error bars. Aggregating multiple estimates must therefore not only account for the modeled noise, but also somehow model the systematic uncertainties in the different estimates. This leads to the counterintuitive fact that combining multiple estimates sometimes \emph{increases} our uncertainty, rather than decreasing it---this is because the level of systematic error is not apparent in only one estimate, and becomes apparent when we see more.

We will begin by discussing and setting up the general problem. Then, we will go through different sets of assumptions which can be used to do inference in this setting, and follow with an evaluation of the assumptions using real-world datasets. Next, we will perform a simulation study to evaluate the performance of different inference methods and their sensitivity to violations of assumptions.

We will present results on two toy examples---estimates of the gravitational constant and of the Planck constant---to illustrate how the different methods work when applied to real data. Finally, we will present two case studies: aggregating estimates of the properties of fundamental particles, and aggregating estimates of Antarctic ice sheet mass loss over time. This will illustrate how the methods presented and discussed here can be used in real scientific problems.


\subsection{Thought experiment}\label{thought-experiment}

Suppose you are given a jar containing a lot of jelly beans, and you want to get an idea of how many jelly beans it contains without counting. You ask five of your friends to try to figure it out, and they give you the following numbers:

\begin{itemize}

\item
  Friend A: 540
\item
  Friend B: 535
\item
  Friend C: 556
\item
  Friend D: 543
\item
  Friend E: 637
\end{itemize}

You want to make an interval that has a roughly 70\% chance of containing the true number of jelly beans. What are some reasonable intervals?

\begin{itemize}

\item
  $[535, 637]$: You think Friends B and E are both smart people with reasonable estimates, and probably the true value is not outside the range of all the estimates.
\item
  $[535, 556]$: You think Friend E is probably overestimating, as your other smart friends are all clustered together.
\item
  $[100, 1000]$: Maybe all of your friends are way off! They could all be underestimating or overestimating the number together if they used a similar approach or suffer from the same bias.
\end{itemize}

Here are some things you may consider when picking an interval:

\begin{itemize}

\item
  Are all of the estimates good, reasonable estimates? Should we discard any? To help decide, you may want to interrogate the methods that your friends used. If Friend E's method looks good, should you do something to resolve the large discrepancy in results with the others?
\item
  How diverse were the methods used? If they all used the same, wrong method then they might all be very wrong. On the other hand, if they all used different methods and arrived at similar results, you may be more inclined to trust them.
\item
  Do the methods of each friend yield values which are centered around the truth?
\item
  Were there any social factors? For example, if Friend D publicly announced their result and is highly respected among the others, the others could have been disinclined to give highly discrepant results.
\end{itemize}

Evidently, combining these five estimates into an interval is not a trivial matter. Indeed, without knowing a lot more details about how the estimates came about from the truth and are related to each other, it will not be possible to design a statistical method to yield a confidence interval or credible interval with the desired probability.

Now, imagine each of your friends also gave a standard deviation associated with their estimate. This would, in fact, complicate the situation further, as we would need to ask the following questions:

\begin{itemize}

\item
  What do the standard deviations mean?
\item
  Which distributions are they the standard deviations of? Do the numbers and the standard deviations together completely describe the distributions?
\item
  Are the distributions frequentist sampling distributions or Bayesian posterior distributions?
\item
  Do the standard deviations account for just statistical noise involved in their method, or some or all parts of their systematic biases?
\end{itemize}

If there is any heterogeneity among the friends in the answers to these questions, our task will become even more complicated. What do we do if Friend A's standard deviation is for just statistical noise but Friend B's also tries to take into account systematic error?

\subsection{Setup}\label{setup}

Suppose we are given $n$ estimates $y_1,\ldots,y_n$ of a number $\theta$, from $n$ experiments or teams or methods. It is possible that each of these estimates has error from two sources: \textbf{systematic} and \textbf{statistical} (i.e.~noise). (For instance, we can have a data-generating process where $y_i$ is actually a noisy estimate of $\theta_i$, which is what experiment $i$ would see if it had no noise; because of systematic error, in general $\theta_i\neq \theta$).

Suppose we are further given a standard deviation $\sigma_i$ for each $y_i$. Experimenter $i$ believes that $\sigma_i$ is a good estimate for the uncertainty in $y_i$ due to noise.

In this setting, how can we learn something about $\theta$? Without making any assumptions, we can't learn anything (for instance, we could have $\theta_i=\theta-a\ \forall i$, in which case we are unable to recover $\theta$ without knowing something about $a$).

This is the problem we will consider in this paper. In some cases, we will suppose that we are not given $\sigma_i$'s. But for simplicity we will suppose being given no more than $(y_i,\sigma_i)$ pairs: no asymmetric uncertainties, no full predictive distributions, and no breaking down of $\sigma_i$ into multiple components.

\subsection{History}\label{sec:history}

The core statistical methods to aggregate data from different observations, and eventually different studies, developed first in the physical sciences. The method of ``least-squares'' (i.e., minimizing the sum of squared residuals to obtain a best-fit) was hinted at in the context of astronomy by Roger Cotes in his posthumously published \emph{Opera Miscellanea} \citeyearpar{cotes1722opera}.\footnote{He described that to calculate the position of an object from several observations with different errors, one must take the center of the positions as weighted by the inverse of the ``limits of the errors.'' This is the original text:
  \begin{quote}Ad eundem fere modum in aliis casibus Limites inveniuntur Errorum qui ex minus accuratis observationibus ortum ducunt, quin \& Positiones ad Observandum commodissimae deprehenduntur: ut mihi vix quidquam ulterius desiderari videatur postquam ostensum fuerit qua ratione Probabilitas maxima in his rebus haberi possit, ubi diversae Observationes, in eundem finem institutae, paullulum diversas ab invicem conclusiones exhibent. Id autem fiet ad modum sequentis Exempli. Sit p locus Objecti alicujus ex Observatione prima definitus, q, r, s ejusdem Objecti loca ex Observationibus subsequentibus; sint insuper P, Q, R, S pondera reciproce proportionalia spatiis Evagationum, per quae se diffundere possint Errores ex Observationibus singulis prodeuntes, quaeque dantur ex datis Errorum Limitibus; \& ad puncta p, q, r, s posita intelligantur pondera P, Q, R, S \& inveniatur eorum gravitatis centrum Z: dico punctum Z fore Locum Objecti maxime probabilem, qui pro vero ejus loco tutissime haberi potest.
  
  \hfill-- \citet[Aestimatio Errorum in Mixta Mathesi, pg. 22]{cotes1722opera}\end{quote}}
Astronomy was indeed a core motiving science for these developments through the 19th century. The simple observational problem of recording the position of an object (say, a planet or asteroid) across multiple days, then computing its orbit, requires combining multiple noisy estimates. Least squares was formalized by \citet[art. 179]{gauss1857theory} and stated in a more recognizable, modern form for simple one-dimensional experiment- or observation-aggregating by \citet[art. 69-71]{airy1861algebraical}. Effectively, this was the Fixed Effect model and method described below, which does not account for any systematic error not modeled by the given $\sigma_i$'s.

As early as 1881, we see FW Clarke, a chemist, using least-squares to aggregate measurements of atomic weights reported by different experimenters who used different methods \citep{clarke1881abstract}. What is lacking from Clarke's work, though, is a statistical evaluation of systematic errors---we see several weighted means with quoted uncertainties that are far too low when considering the spread of the different experimenters' results.\footnote{Consider the following results quoted for the atomic weight of chromium:
  \begin{quote}
    The data studied were by Berlin, Moberg, Lefort, Wildenstein, Kessler, and Siewert. General mean from all, Cr=52.453; $\pm$.015. Berlin's work, considered by itself, gives Cr=52.389; $\pm$.019. Siewert's results give Cr=52.009; $\pm$.025. I regard-the last value as freest from constant errors, and use it in subsequent calculations.
  
    \hfill-- \cite{clarke1881abstract}
  \end{quote}
}

In fundamental physics, statistical meta-analysis can be traced back at least as far as \cite{burgess1902value}, which calculated the error-weighted mean of the gravitational constant $G$. This work suffered from similar issues as Clarke's atomic weights. Finally, \cite{birge1919most} calculated an error-weighted mean of several determinations of Planck's constant $h$ obtained from different experiments. Again, this was under the Fixed Effect model. What goes beyond a simple error-weighted mean, though, is Birge's discussion of systematic uncertainity. In particular, he computed the residuals of each result from the error-weighted mean, and compared those residuals to the given errors $\sigma_i$. He concluded that the results were ``consistent'' \cite{birge1919most}. Thus he provided a quantitative evaluation of the presence of systematic error or underestimation of the experiments' uncertainties.

By the early 20th century, measurements and estimates of various physical constants were becoming increasingly numerous. These physical constants were also key to the work of the experimental physicist, whose experimental results involved calculations with these uncertain physical constants, or were even stated \emph{in terms of} them (as they entered somewhere into the units). Comparing and discussing different results thus became increasingly difficult as physicists did agree on one set of values. Therefore, by 1929, Birge set out to collate different experimental results and produce a ``self-consistent'' set of constants based on all readily available measurements (since many constants are functions of one another, the values for a set of constants need to be internally consistent, which introduces additional difficulties).\footnote{It appears that the first work performing a least-squares adjustment of multiple physical constants at once may be \cite{harkness1891solar}. However, Harkness does not perform sophisticated statistical aggregations of the experiments contributing to each constant: he uses either simple arithmetic means or picks one of the results.} Such a collation/aggregation of results has come to be known as an ``adjustment.''

In Birge's pioneering work \citeyearpar{birge1929probable}, it is apparent that much of the difficulty lay in interpreting the uncertainties which experimental results reported in diverse ways:
\begin{quote}
  One of the greatest difficulties in the present work has been the proper weighting of various results, one of which is stated in terms of ``limit of error,'' another in terms of a ``probable error,'' another possibly with the error undefined, and still another with no definite statement as to accuracy. This is quite aside from the reliability or correctness of the method used by each investigator in arriving at his stated error.

  \hfill -- \cite{birge1929probable}
\end{quote} 

To perform his ``adjustment,'' Birge used a combination of heuristics and statistics. In terms of heuristics, he used domain knowledge or apparent inconsistency to discard poor or outdated experimental results. From the statistical side, he again used the least-squares method or Fixed Effect model. In the context of aggregating physics experiments, ``least-squares'' has come to mean taking an inverse-variance-weighted mean of the $y_i$'s, where the variances are somehow calculated from the $y_i$'s and $\sigma_i$ (in the simplest case, the variances are taken as $1/\sigma_i^2$, and the weights are proportional to $1/\sigma_i^2$). The uncertainty is taken as appropriate for a weighted mean. But Birge was quick to discard least-squares results when they seemed insensible. For example, in his calculation of the charge of the electron, for which only two values were used, he took a weighted mean as usual, but adopted ``as its probable error the smaller of the two individual errors, rather than that given by least squares, since the latter is rather meaningless when only two observations are concerned'' \citep{birge1929probable}.

By 1932, Birge developed what is now known as the ``Birge ratio'' method for least-squares. The first clear statement of it is in \cite{birge1932calculation}. After obtaining the weighted mean by least-squares as usual, he computes the ratio of the error in the weighted mean calculated by ``external consistency'' (using the residuals between experiments and the weighted mean) to that calculated by ``internal consistency'' (using each experiment's own given error). This is a useful quantitative diagnostic tool: if the ratio is large, it indicates that the individual experiments have more spread between them than we would expect from their individual errors. But he goes beyond diagnostics, and, when calculating the error of the weighted mean, he advocates choosing the larger of the ``external consistency'' and ``internal consistency'' errors. This effectively forms the Birge ratio method which will be described more precisely in a later section. However, Birge also writes that when the ratio is large, the errors should be adjusted manually with personal judgment.

Aggregations of fundamental physical constants are now performed by the Committee on Data of the International Science Council (CODATA). But this task of aggregating well-defined physical constants has only expanded, and now includes constants for particle properties in the standard model, performed by the Particle Data Group (PDG) in its frequently-updated \textit{Review of Particle Physics}. Both CODATA and the PDG use slight variations on Birge's method. The challenges of dealing with systematic uncertainty are illustrated by PDG's ``History Plots'' \citep[pg. 19, Figure 1]{navas2024review}, which show how, as PDG incorporated new experiments and discarded old ones over time, the resulting estimates of some particle properties have varied outside of the range of their error bars.

An additional task has also arisen with the development of data- and simulation-driven Earth science. This involves estimates of less well-defined (and sometimes in principle unknowable) quantities. For example, we may consider the amount of global warming in the past or future, or the historical and present-day rate of ice mass loss in polar regions. Here, looking at multiple estimates is sometimes known as ``intercomparison.'' Some intercomparison studies simply seek to review the differences in the methods and results, while others seek to statistically combine multiple estimates into one.

\subsection{Experiment aggregating vs.~forecast ensembling}

Our discussion of meta-analysis / experiment aggregation brings us close to a related area: opinion pooling, or ensemble forecasting. Let's compare:

\begin{enumerate}
\item
  In experiment aggregation, we have been discussing a situation where $n$ different experimenters/teams/studies come up with $n$ different estimates $y_1,\ldots,y_n$ for some quantity $\theta$, possibly with associated uncertainties $\sigma_1,\ldots,\sigma_n$.
\item
  In ensemble forecasting or opinion pooling, we may ask $n$ different teams to predict how much rain will fall tomorrow, and maybe they will each give values $y_1,\ldots,y_n$ with associated uncertainties $\sigma_1,\ldots,\sigma_n$.
\end{enumerate}

These seem like very similar situations. So can we use the same methods for both of them?

Let's give an example from paleoclimatology to further explore the difference. It will illustrate that it is not the act of ``forecasting'' (i.e.~predicting something in the future) itself that leads to any difference, but the effectively irreducible uncertainty of the estimand.

\begin{enumerate}
\item
  $\theta$ is the mean global temperature between 10,000 years ago and 11,000 years ago
\item
  $\theta$ is the air temperature at solar noon on January 1, 10,000 BP in what is now Berkeley, CA.
\end{enumerate}

In the first case, different teams may use different historical temperature proxies and come up with different estimates of $\theta$. If they all use different independent datasets (e.g.~different ice cores), then assuming their methods are good we expect them to come back with results $y_i$ clustered around $\theta$. If the methods are good, it would be fairly unlikely for the true $\theta$ to lie above \emph{all} of the $y_i$'s.

In the second case, probably what teams will do is use an answer from the first case, combined somehow with some regional weather variation models. They will each come back with a $y_i$ representing a long-run average and a fairly large $\sigma_i$, because on any given day the temperature can be quite different. If January 1, 10,000 BP happened to be a very hot day in what is now Berkeley, CA, then it is quite possible for the true $\theta$ to lie above \emph{all} of the $y_i$'s.

In the first case, it is possible for the $y_i$'s to be roughly conditionally independent given $\theta$. In the second case, the $y_i$'s will not be conditionally independent given $\theta$. Maybe they will be conditionally independent given a long-term average, but conditioning on the $\theta$ itself does not make them independent---the $y_i$'s will vary above and below $\theta$ together.

How does this inform the way we do aggregation? It may be helpful to take up a Bayesian perspective to understand this difference.

\begin{enumerate}
\item
  In the case of experiment aggregation, we have $n$ different experiments which produce data $d_1,\ldots,d_n$. These data come from likelihoods $p(D_1|\theta),\ldots,p(D_n|\theta)$. Crucially, we think the $D_i$ are independent given $\theta$. The experimenters themselves think the $d_i$ come from $p_1(D_1|\theta),\ldots,p_n(D_n|\theta)$, because they have their own understandings of the likelihood of an observation given the value of $\theta$, due to, for example, distributional approximations or different beliefs about their instruments. Now, suppose all of the estimates we receive from the different experimenters are reflected as Bayesian posterior probabilities for $\theta$, written as distributions $p_1(\theta|D_1=d_1),\ldots,p_n(\theta|D_n=d_n)$.\footnote{The posteriors $p_i(\theta|D_i=d_i)$ will depend not only on the different likelihoods $p_i(D_i|\theta)$, but also on priors $p_i(\theta)$ which may not be the same for different experimenters.} The key is that each experiment created different data, and we want to combine it to obtain something like $p(\theta|D_1=d_1,\ldots,D_n=d_n)$ as our ``aggregated'' estimate for $\theta$. We will expect that, as $n$ increases, our distribution $p(\theta|D_1=d_1,\ldots,D_n=d_n)$ will get narrower.\footnote{This narrowing may only be up to a point. For example, we can consider a situation where we are actually interested in $\theta'=\theta+\varepsilon$, where, say, $\varepsilon\sim\mathcal{N}(0,1)$, but $D_i\perp\!\!\!\!\perp\varepsilon$. In this case, we will always have an irreducible error in our posterior for $\theta'$. As the variance of $\varepsilon$ increases we will actually start to be more in the second ``opinion pooling'' setting, where the different estimates were made with the same information.}
\item
  In the case of opinion pooling or aggregating forecasts, we have $n$ different people who each observe the same state of the world $D=d$. Then, each of them comes up with their own posterior distributions (based on their own priors and likelihood functions), and they report them to us. So we get $p_1(\theta|D=d),\ldots,p_n(\theta|D=d)$. Our task is to somehow combine these into one $p(\theta|D=d)$. Because each forecaster has access to roughly the same data $D=d$, we do not expect our distribution $p(\theta|D=d)$ to get narrower as $n$ increases (though it may become more well-calibrated).
\end{enumerate}

It is extremely important to distinguish which case we are in, as it will lead to different statistical methods being appropriate.\footnote{It may not always be easy to determine which setting we are in. Consider the case of Antarctic ice mass loss, which will be discussed later. If we are estimating how much ice Antarctica lost over the last 20 years, that seems like it lies more in case (1), due to the large amount of independent data sources available documenting this phenomenon over a long time period at a high temporal resolution. How about the ice mass loss on one particular day in the last 20 years? That seems more like (2)---the temporal resolution of our data won't be good enough to do much better than a monthly average with some correction for the day's weather and a wide uncertainty. But if we consider the mass loss on a weekly, monthly, or yearly scale, these must lie somewhere in between the two cases.} In particular, methods for (2) will typically lead to far wider uncertainties than those for (1). For example, suppose $y_1=\ldots=y_n=0$, and $\sigma_1=\ldots=\sigma_n=1$. Under (1) a reasonable estimate for $\theta$ might be $0$ with standard deviation $1/\sqrt{n}$. Under (2) a reasonable estimate for $\theta$ might be $0$ with standard deviation 1.

\subsection{Climate predictions}

Climate modeling and prediction is becoming increasingly important as the threat of climate change looms. Making predictions involves simulating the global climate for decades into the future. Unfortunately, it is difficult:
\begin{itemize}
  \item Some parts of the complicated climate system are not well-understood, and some parameters are poorly-constrained.
  \item Simulations cannot be made at infinite granularity; approximations and simplifications must be made to make it computationally efficient.
  \item We do not have complete knowledge of the initial state of the system (i.e.~the Earth).
  \item Some predictions involve feedbacks loop with human activity, which itself is hard to model.
\end{itemize}
Nonetheless, climate predictions are made. Many predictions are compiled in the reports of the Intergovernmental Panel on Climate Change (IPCC), with Working Group 1 \citeyearpar{ipcc2021physical}focusing on the physical science (other groups focus on consequences and mitigation).

One common method in the IPCC reports is to deal with these uncertainties by using an ensemble of models from different teams. These different models use different methods, assumptions and initial parameters, and therefore yield an ensemble of different forecasts. Then, according to \citet[pg. 96]{winsberg2018philosophy},\footnote{Here is Winsberg's discussion: \begin{quote}
  The most common method of estimating the degree of structural uncertainties in the predictions of climate models is a set of sampling methods called ``multi-model ensemble methods.'' [\ldots] By looking at the average prediction of the set of models and calculating their standard deviation, one can produce a probability distribution for every value that the models calculate.
  
  Most multi-model ensemble methods and perturbed-physics ensemble methods have in common that they attempt to objectively (or perhaps more accurately: mechanically) calculate the degree of uncertainty from each source, respectively, by trying to systematically sample the range of possible results. If 80 percent of the results from a space of models and parameter values lie in some range, then the probability of the true result lying in that range is said to be 80 percent. Of course, they are objective only in the sense that they are independent of the degrees of belief of any particular expert, and that they are calculated mechanically.
  \hfill -- \citet[pg. 96]{winsberg2018philosophy}
\end{quote}} the typical approach is simply to take the quantiles of the ensemble's point predictions (or a distributional fit to them) as the resulting interval. For example, for a 90\% interval, they might take the 5\% and 95\% quantiles of the point predictions. As noted by \citet[Section 2.5.2, pg. 63]{harris2021conceptualizing}, the IPCC authors do not necessarily give such probabilities, and may subjectively reduce the assigned probability.\footnote{I avoid defining what the ``probability'' in the IPCC WGI reports actually means, or the statistical nature of the given intervals. As discussed by \citet[Section 1.4]{harris2021conceptualizing}, it is not clear how to interpret the IPCC's probability statements.}

%TODO: study harris2021conceptualizing and discuss the philosophical issues in using model ensembles. Pay attention to chapter 5





\subsection{Which results do we aggregate?}

At times, we may feel it is appropriate to not include a particular result $y_i$ in our aggregation. We can put the justifications for this into three categories: based on \textit{a priori} \textbf{domain knowledge}, \textit{a posteriori} \textbf{statistics}, or \textbf{method incompatibility}.

\begin{itemize}
  \item \textbf{Domain knowledge:} Perhaps $y_i$ doesn't measure precisely what we want to measure (e.g.~in medicine, $y_i$ might be the calculated treatment effect of a medical intervention which is too different from what we are studying). Or, maybe we judge \textit{a priori} that the experiment producing $y_i$ is seriously flawed and cannot be relied on to provide a good estimate of $\theta$.
  \item \textbf{Statistics:} We may see that a result $y_i$ lies far from most of the other results we are combining. $y_i$ is an outlier. There are now two reasons we might wish to exclude $y_i$ from our aggregation. (a) Based on the fact that it lies so far from the main cluster of results, we judge \textit{a posteriori} that there must be something seriously wrong with how $y_i$ was produced, and therefore excluding $y_i$ will give us a more accurate result. (b) Maybe there is nothing wrong with how $y_i$ was produced, but we know our aggregation method is sensitive to outliers, and $y_i$ will have an undue influence on the results if we include it. Another statistical reason for exclusion could be that $y_i$ has such a high error $\sigma_i$ that it won't have much of an effect on the aggregated result, so we exclude it.
  \item \textbf{Method incompatibility:} We may know, for example, that $y_1$ and $y_2$ are not independent. If our method requires independent results, we can choose to exclude one of $y_1$ or $y_2$. Or, in the case of a method that requires $\sigma_i$'s, we will have to exclude any observations $y_i$ that don't have an associated $\sigma_i$.
\end{itemize}

\citet{taylor1982numerical} points to several cases where, in aggregations of experiments measuring fundamental constants, values $y_i$ were judged unreliable and excluded based on \textit{a posteriori} discrepancy with the bulk of the data, but later evidence showed them to be closer to the truth. However, he also points to a case where excluding a value was later shown to have improved the resulting aggregation. In some cases, excluding a discrepant value improves the result, and in others it worsens it, so a general rule is hard to come by.

The Particle Data Group, in its aggregations of data on particle properties, has several reasons for excluding measurements. Here is an abbreviated list:

\begin{quote}
  \begin{enumerate}
  \item It is superseded by or included in later results.
  \item No error is given.
  \item It involves assumptions we question.
  \item It has a poor signal-to-noise ratio, low statistical significance, or is otherwise of poorer quality than other data available.
  \item It is clearly inconsistent with other results that appear to be more reliable.
  \item It is not independent of other results.
  \item It is quoted from a preprint or a conference report.
  \end{enumerate} \hfill -- Abbreviated from \citet[Section 5.1]{navas2024review}
\end{quote}

Reason (3) given by PDG lies in the first \textbf{domain knowledge} category we give, reasons (4, 5) belong to \textbf{statistics}, and reasons (1, 2, 6) belong to \textbf{method incompatibility}. In physics, the fourth reason, that the result has a much higher error than others, is very common. Some target quantities have been the subject of measurement for decades, with uncertainties becoming much smaller over time as more precise instruments and techniques are developed. Reason (6), lack of independence, is crucial, as we will see later that methods for combining experiments typically require the assumption that the results are independent.

The challenge of data selection is one we must pay special attention to. As \citet{taylor1982numerical} argues, ``the actual algorithm one uses to carry out an adjustment is not nearly as critical as the actual items of input data one selects for inclusion in the adjustment.'' As in any work of applied statistics, an aggregation of results thus requires close collaboration with domain experts and careful study of the underlying data sources.

In this paper, we will largely ignore the problem of such domain-specific data selection (which is largely intractable to generalization) until we come to the case studies.

\section{Assumptions and Inference}\label{sec:methods}

There are many methods that can be used to aggregate estimates. As we will see, most of them assume a statistical model which, given $\theta$, generates the data. For each approach, we will discuss the assumptions and associated parameter inference methods. The assumptions will be discussed empirically later, as will the performance of the different methods on simulated data.

\subsection{Independent experiments}

\subsubsection{Random Effects}\label{random-effects}

The Random Effects model and method originated in the medical meta-analysis literature \citep{dersimonian1986meta}. It is now common in meta-analyses in medicine and the social sciences, but uncommon in the physical sciences.

\paragraph{Assumptions}\label{assumptions}

The Random Effects model places some distributional assumptions upon our setting:

\begin{enumerate}
\item
  $y_i|\theta_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\theta_i,\sigma_i^2)$
\item
  $\theta_i\overset{\mathrm{iid}}{\sim}\mathcal{N}(\theta,\tau^2)$
\end{enumerate}

$\tau^2$ is a new parameter introduced to model the spread of the systematically-biased experiment values $\theta_1,\ldots,\theta_n$ around the true value $\theta$.

From these assumptions, we get $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\theta,\sigma_i^2+\tau^2)$. Effectively, the systematic uncertainty and noise are additive.

These assumptions could make sense if we think that all of the studies suffer from random unaccounted-for systematic errors with the same standard deviation. Assumption 2 effectively says that when you go out to measure $\theta$, you will actually be measuring a $\theta_i$ which is $\theta$ plus a realization of a random Normal; and this random Normal has the same standard deviation as those of all the other researchers, and does not have any relation with the level of noise $\sigma_i$. The assumptions will not hold if the size of systematic errors is related to the amount of noise, or if the overall distribution of systematic errors is not Normal (maybe there are outliers, or there is a sharper peak of experimenters with very little systematic error).

\paragraph{Inference}\label{inference}

Inference on $\theta$ can be done in a number of ways under this model. The original DerSimonyan and Laird procedure (DL) first estimates $\tau^2$ using the method of moments, then uses this as a plug-in value to estimate $\theta$ and a corresponding confidence interval Under a Normal approximation \citep{dersimonian1986meta}. Variants exist, such as using maximum likelihood for $\tau^2$ \citep{dersimonian1986meta,jackson2010does}.

A later procedure, Hartung-Knapp-Sidik-Jonkman (HKSJ), modified the uncertainty estimation in DL to better account for the fact that our estimate of $\tau^2$ is uncertain \citep{hartung1999alternative,sidik2002simple}. HKSJ has been shown to generally achieve target coverage far more consistently than DL \citep{inthout2014hartung} and to generally give wider confidence intervals \citep{wiksten2016hartung}. In this article we will use the HKSJ procedure as we also found it to generally perform better than DL in our simulations.

Other works instead perform Bayesian inference by placing informative or uninformative priors on $\tau$ and $\theta$ \citep{sutton2001bayesian}.

\paragraph{Extensions}\label{extensions}

The Random Effects model can and has been extended to the case where there are correlations between experiments in $\theta_i$'s or $y_i|\theta_i$'s.

The model can also be extended to include information from study-level variables (e.g.~year of publication, method used) in the distribution of $\theta_i$ to explain more of the spread in study-level effects.

\subsubsection{Fixed Effect}

For comparison with RE we also present the Fixed Effect (FE) model.

\paragraph{Assumptions}\label{assumptions-1}

\begin{enumerate}
\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\theta,\sigma_i^2)$
\end{enumerate}

In this model, we suppose that the standard deviations $\sigma_i$ given to us by the researchers truly represent all of the error in the problem---both systematic and noise. In general, in a problem where there really is unaccounted-for systematic error, this will lead to an underestimation of the amount of error (i.e.~a confidence interval that is too narrow).

\paragraph{Inference}\label{inference-1}

Inference under FE is much simpler than under RE.

\begin{enumerate}
\item
  Calculate the weighted mean $\bar{y}_w=\frac{\sum_{i=1}^n w_iy_i}{\sum_{i=1}^nw_i}$, where $w_i=1/\sigma_i^2$. This will be our point estimate, the center of the interval.
\item
  The standard deviation of $\bar{y}_w$ is $\sigma_w=\frac{1}{\sqrt{\sum_{i=1}^nw_i}}=\sqrt{\sum_{i=1}^n1/\sigma_i^2}$. Therefore we can form a $1-\alpha$ confidence interval for $\theta$ as \[\bar{y}_w\pm z_{1-{\alpha/2}}\cdot \hat c\sigma_w,\] where $z_{1-\alpha/2}$ is the $z$-statistic at the desired level (e.g.~for a $1\sigma$ or $68.27\%$ interval, we use $z_{1-\alpha/2}=1$; for a $95\%$ interval, we have $\alpha=0.05$ and $z_{1-\alpha/2}=1.96$)
\end{enumerate}

\subsubsection{Birge Ratio}

The Birge ratio originated as a way to measure the inconsistency of different estimates and their uncertainties (a quantitative version of ``these confidence intervals don't overlap''). The Birge Ratio method (BR) is effectively what is now used by CODATA (under the name ``Least-Squares Adjustment'' \citep{tiesinga2021codata}) when they combine estimates of fundamental constants from multiple experiments. (Historically this has not always been the case, as in the 2002 adjustment's determination of $G$ \citep{mohr2005codata}). It is also what is used by the Particle Data Group in its \emph{Review of Particle Physics} when aggregating estimates of the properties of fundamental particles \citep[Section 5.2.2]{navas2024review}.

\paragraph{Assumptions}\label{assumptions-2}

\begin{enumerate}
\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\theta,c^2\sigma_i^2)$
\end{enumerate}

We don't know $c$ a priori. The idea is that the uncertainties reported in the different experiments are too small---as they only represent noise, and not systematic uncertainty. So we can just expand the uncertainties by a multiplicative scaling factor.

The difference from RE is that the uncertainties are underestimated multiplicatively, instead of additively. So, for example, suppose experimenters 1 and 2 reported $\sigma_1=3$ and $\sigma_2=6$. Under BR, we could have that the true sigmas of experimenters 1 and 2 are $9$ and $18$ instead---but under the model experimeter 2 will always have double the uncertainty as experimenter 1. Under RE, we could have the true sigmas be $9$ and $12$ instead---still a difference of 3 as reported, but the relative scaling has changed.

This assumption could make sense if we feel that the true error in experiments is proportional to the reported error. If the reported error is statistical noise, then this also means that the noise standard deviations are proportional to the standard deviations of the unknown systematic errors. This can be reasonable, as a study which seeks to reduce statistical noise to a certain level will also try to correspondingly reduce systematic errors (otherwise, why reduce the noise?). But the assumption of \emph{exact} proportionality is quite strong.

\paragraph{Inference}\label{inference-2}

To do inference, we estimate $c$ and then directly estimate $\theta$ by plugging in $\hat c$. Note that in practice, when estimating $c^2$, it is often truncated to be at least $1$. One procedure to do this can be stated briefly as:

\begin{enumerate}
\item
  Calculate the weighted mean $\bar{y}_w=\frac{\sum_{i=1}^n w_iy_i}{\sum_{i=1}^nw_i}$, where $w_i=1/\sigma_i^2$. This will be our point estimate, the center of the interval. Note that if $c=1$, the standard deviation of $\bar{y}_w$ is $\sigma_w=\frac{1}{\sqrt{\sum_{i=1}^nw_i}}=\sqrt{\sum_{i=1}^n1/\sigma_i^2}$.
\item
  Calculate $\chi^2=\sum_{i=1}^n w_i(\bar{y}_w-x_i)^2$. If $c=1$, this will follow a Chi-squared distribution with degrees of freedom $n-1$, and will correspondingly have expectation $n-1$. Take $\hat c=\max(\sqrt{\chi^2/(n-1)},1)$, thus creating an estimate of $c$ truncated to be $\geq 1$.
\item
  Now we can plug this in as a true value of $c$, and take the standard deviation of $\bar{y}_w$ as $\hat c\sigma_w=\sqrt{\sum_{i=1}^n1/\hat c\sigma_i^2}=\frac{\hat c}{\sqrt{\sum_{i=1}^nw_i}}$.
\item
  Finally, form the $1-\alpha$ confidence interval for $\theta$ as \[\bar{y}_w\pm z_{1-{\alpha/2}}\cdot \hat c\sigma_w,\] where $z_{1-\alpha/2}$ is the $z$-statistic at the desired level (e.g.~for a $1\sigma$ or $68.27\%$ interval, we use $z_{1-\alpha/2}=1$; for a $95\%$ interval, we have $\alpha=0.05$ and $z_{1-\alpha/2}=1.96$)
\end{enumerate}

This procedure is almost exactly what is used by the Particle Data Group; they use two additional heuristics \citep[Section 5.2.2]{navas2024review}:

\begin{itemize}

\item
  If the estimate $\hat c$ is ``very large,'' indicating large discrepancies in the experimental results not accounted for by noise, the average and interval may not be reported.
\item
  If the errors $\sigma_i$ vary widely, only the experiments with smaller errors are used to compute $\chi^2$. Specifically, only the experiments with $\sigma_i\leq 3\sigma_w\sqrt{n}$. This cutoff is ``arbitrarily chosen'' \citep[Section 5.2.2]{navas2024review}.
\end{itemize}

This procedure is also close to what is used by CODATA, except their $\hat c$ is chosen heuristically as that which makes the normalized absolute residuals of the $y_i$'s from the weighted mean $\bar y_w$ less than two \citep{tiesinga2021codata}. Note that their work is considerably more complex than the scenarios we consider, because they simulatenously compute large sets of constants which depend on each other.

As with the Random Effects model, we can also do inference by placing informative or uninformative priors on $c^2$ and $\theta$ \citep{bodnar2014adjustment}.

\paragraph{Extensions}\label{extensions-1}

Just like the random effects model, the Birge ratio method can be extended to the case where there are correlations between experiments. The CODATA analyses often have correlated experiments, so they use this extension.

\paragraph{Discussion}\label{discussion}

The Birge Ratio method offers a simple model and relatively straightforward inference. \citet{taylor1982numerical} refers to it as ``objective.'' But \citet{taylor1982numerical} also points out an interesting consequence that we have ``the a priori uncertainty of a highly consistent input datum\ldots with possibly a well defined uncertainity increased by the same factor as a highly discrepant datum\ldots with perhaps a poorly defined uncertainty.'' What Taylor is pointing to is the unintuitive result that we will scale the uncertainties of all points by the same factor. If we have a distant outlier $y_0$ with $\sigma_0=1$ and a point $y_1$ in about the middle of the $y_i$'s which also has $\sigma_1=1$, it seems intuitive that we would expand the uncertainity of the distant outlier by a larger factor, because it is quite inconsistent with the rest of the data. But the Birge Ratio method will effectively lead to both $y_0$ and $y_1$ having an ``adjusted'' uncertainity of $\hat c$ when used to compute the final interval.

\subsubsection{Sign Test}\label{sign-test}

We can call this method the ``Sign Test'' (ST): it is in fact an inversion (to get a confidence interval) of a test for the median of a distribution. But for clarity we will present only as a method to get confidence intervals.

\paragraph{Assumptions}\label{assumptions-3}

Let's make the following assumptions:

\begin{enumerate}
\item $P(y_i<\theta)=0.5$
\item The event $y_i<\theta$ is independent for each $i$.
\end{enumerate}

In words, Assumption 1 says that each experiment has a one-half chance of being an underestimate of the truth (and, correspondingly, a 1/2 chance of being an overestimate). We don't get much from this assumption alone, because we could have all the experiments be correlated. So we need the additional Assumption 2 that the experiments are independently overestimates or underestimates.

The $y_i$'s being independent for each $i$ is sufficient but not necessary to satisfy Assumption 2. A setting where Assumption 2 holds and the $y_i$'s are not independent would surely be very contrived, so for practical purposes we can consider Assumption 2 not satisfied when the $y_i$'s are not independent.

Let $K$ be the number of experiments which yield an underestimate. From Assumptions 1 and 2, we get:
\begin{equation*}K\sim\mathrm{Binomial}(n,0.5)\end{equation*}

\paragraph{Inference}\label{inference-3}

For simplicity, let's suppose $y_i\neq y_j$ for all $i\neq j$, and that $y_i\neq \theta$ for all $i$. (This will happen with probability 1 if the $y_i$'s are continuously distributed)

Let's consider the order statistics $y_{(1)}<\ldots< y_{(n)}$ (simply the sorted $y_1,\ldots,y_n$). For simplicity of notation, let $y_{(n+1)}=\infty$.

If $K=k$, we know that $y_{(k)}$ is an underestimate and therefore $y_{(k)}< \theta$. We also know that $y_{(k+1)}$ is an overestimate, and therefore $\theta < y_{(k+1)}$. In fact, we can say:

\begin{itemize}
\item
  Whenever $K\leq k$, we know $y_{(k+1)}$ is an overestimate, and therefore $\theta<y_{(k+1)}$.
\item
  Whenever $\theta<y_{(k+1)}$, $y_{(k+1)}$ is an overestimate, and therefore $K\leq k$.
\end{itemize}

In short, we have
\begin{equation*}K\leq k\leftrightarrow \theta< y_{(k+1)},\end{equation*}
from which we directly get
\begin{equation*}P(K\leq k)=P(\theta<y_{(k+1)}).\end{equation*}
We are framing this as a frequentist problem where $\theta$ is unknown, but fixed (not a random variable). Thus the uncertainty in the RHS is over $y_{(k+1)}$. $P(K\leq k)$ can be directly calculated from the Binomial CDF.

How can we turn these nice facts into a $1-\alpha$ confidence interval? Say we want a symmetric interval. For our lower bound $l$, pick the smallest $j$ such that $P(K\leq j)\leq \alpha/2$, and let the lower bound be $l=y_{(j+1)}$. We will get that:
\begin{equation*}P(\theta<l)=P(\theta<y_{(j+1)})=P(K\leq j)\leq \alpha/2 .\end{equation*}
For our upper bound $u$, pick the largest $j$ such that $P(K\geq j)\leq \alpha/2$, and let the upper bound be $u=y_{(j)}$. We
will get that:
\begin{equation*}P(\theta>u)=P(\theta>y_{(j)})=P(K\geq j)\leq \alpha/2\end{equation*}
The interval $[l,u]$ will have level $1-\alpha$.

Note that this method doesn't require knowledge of the $\sigma_i^2$'s, so them being mis-specified won't affect results.

See the code belowfor a minimal Python implementation of this inference procedure.

\begin{listing}[h]
\caption{Minimal Python code for the Sign Test's inference. The main thing to be careful with in implementation is correct indexing.}
\smaller
\begin{minted}{python}

import numpy as np
from scipy.stats import binom
def signtest(values, p=0.5, target=0.15865):
n = len(values)
values = np.sort(values)

cdf = binom.cdf(np.arange(n+1), n, p)

lower_idx = np.argmax(cdf > target)-1
upper_idx = np.argmax(cdf >= 1-target)

return values[lower_idx], values[upper_idx]
\end{minted}
\end{listing}


\paragraph{Relaxing Assumption 1}\label{relaxing-assumption-1}

Saying that the probability of an overestimate is exactly 0.5 could be a strong assumption---though it is still much weaker than the assumptions of the Random Effects or Birge ratio methods. Here are some ways to relax this:

\begin{itemize}
\item
  Assume instead ``the probability of an overestimate is between $p_1$ and $p_2$,'' where $p_1<p_2$. Then, we can compute $l$ under $p_2$ and $u$ under $p_1$ (a larger $p$ shifts the interval downwards, because it implies more estimates are too high).
\item
  Assume instead ``the probability $p$ of an overestimate follows a $\mathrm{Beta}(\alpha,\beta)$ distribution.'' Then $K\sim\mathrm{BetaBinomial}(n,\alpha,\beta)$, and we can do everything else the same.
\end{itemize}

Either method will yield a wider interval which takes into account our less-confident assumptions about $p$.

\paragraph{Relaxing Assumption 2}\label{relaxing-assumption-2}

Saying that the events $y_i>\theta$ are independent could also be a strong assumption. For example, there may be a grouping effect among studies. To relax this, we can model $K$ as another known or derived probability distribution (e.g., again, a Beta-Binomial) or one estimated from simulations under a custom data-generating process. However, it does not seem easy to reason about what the relevant conditional probabilities might be, even for two estimates (this would be the probability $P(y_i>\theta|y_j>\theta),\ i\neq j$), without placing additional distributional assumptions on our setting.

\paragraph{Analysis}\label{analysis}

The assumptions of the Sign Test are far weaker than the assumptions of the previous methods. In fact, the assumptions of RE and BR imply Assumptions 1 and 2. So a method using only Assumptions 1 and 2 should work well under data from either model, but not vice-versa.

Our assumptions and inference also do not at all involve the $\sigma_i$'s, making our method completely insensitive to their mis-specification.

Unlike most inferential methods for RE and BR, our inference does not involve a prior or any distributional approximations. Under Assumptions 1 and 2, inference will be exact for any value of $n$, and coverage which is at least at the target level is guaranteed.

One caveat is that the Binomial distribution is discrete, so for finite $n$ the tail probabilities only take on a set of discrete values. So, without playing tricks like randomly shrinking or collapsing the interval, we cannot achieve exactly 0.6827 ($1\sigma$) coverage, but will always achieve coverage $\geq 0.6827$. See \ref{table-n} for examples of values of $n$ and the smallest coverage level $\geq0.6827$ which can be achieved exactly---usually, it is not too much higher.

\subsubsection{Signed-Rank Test}\label{signed-rank-test}

This approach is due to \citet{wilcoxon1945individual}, and we can refer to it as SRT. Our discussion will follow \citet[Section 5.7]{conover1999practical}.

\paragraph{Assumptions}\label{assumptions-4}

The Signed-Rank Test will strengthen the assumptions of ST:

\begin{enumerate}
\item
  The $y_i$'s are symmetrically distributed about $\theta$.
\item
  The $y_i$'s are independent.
\end{enumerate}

Effectively, ST's Assumption 1 has been strengthened to a symmetry assumption, and Assumption 2 has been strengthened to independence of the $y_i$'s themselves, and not just the binary over/under-estimating event. In practice, these assumptions may not be much stronger than those of ST:

\begin{itemize}

\item
  If we can argue convincingly that in a real-world problem $P(y_i<\theta)=0.5$, we should be able to argue that $y_i$ is symmetric about $\theta$ without much more effort (``the sizes of the errors look the same if you're over or under-estimating'').
\item
  As mentioned above, a scenario where the events $y_i<\theta$ are independent but the $y_i$'s themselves are not would seem fairly contrived.
\end{itemize}

\paragraph{Inference}\label{inference-4}

Inference under these assumptions is far less trivial than for ST. We can take the $\alpha/2$ and $1-\alpha/2$ quantiles $L$, $U$ of the distribution of $\sum_{i=1}^n iB_i$, where the $B_i$'s are independent and each $-1$ or $1$ with equal probability. Then, if we compute the $n(n+1)/2$ averages of the form $(y_i+y_j)/2$, $i\leq j$ and order them in increasing order, the $L$th and $U$th averages will form our level $\geq 1-\alpha$ confidence interval.

\subsubsection{t-Test}\label{t-test}

Inspired by the Sign Test and Signed-Rank Test, which ignore the $\sigma_i$'s, why not try just a standard $t$-test confidence interval over the $y_i$'s?

\paragraph{Assumptions}\label{assumptions-5}

\begin{enumerate}
\item
  $y_i\overset{\mathrm{iid}}{\sim}\mathcal{N}(\theta,\tau^2)$.
\end{enumerate}

Where $\tau^2$ is unknown. Of course, this model will not be true, because the $y_i$'s should have different variances. But it might still work in practice, and it is a useful point of comparison against the Random Effects and Birge Ratio methods.

\paragraph{Inference}\label{inference-5}

We can use the standard confidence interval for the mean of a Normal distribution:
\%TODO

\subsection{Forecast combining}

\subsubsection{Linear pool}

Let's now leave the setting where we have independent experiments measuring a quantity $\theta$, and enter the setting of aggregating forecasts or opinions of $\theta$. Suppose we have estimates $f_i$ of the \emph{distribution} of $\theta$. For example, we can have $n$ pairs $(y_i,\sigma_i)$, and suppose that $f_i$ is the probability density function of $\mathcal{N}(y_i,\sigma_i^2)$.

In this setting, \cite{stone1961opinion} introduced the linear opinion pool.\footnote{As noted by \cite{koepke2017consensus}, \cite{bacharach1979normal} attributes the linear opinion pool all the way back to Laplace's 1814 \emph{Essai Philosophique sur les Probabilités} \citep{laplace1951essay}. It is unclear which part of Laplace's long text is referred to, but it might be chapter XII, \emph{Concerning The Selections and the Decisions of Assemblies}. In this section, Laplace discusses a group of people choosing a ``proposition'' among a set of disjoint propositions (we can think of the value of a discrete variable). He proposes a voting method:
  \begin{quote}Let us give to each voter an urn which contains an infinite number of balls, and let us suppose that he distributes them upon the divers propositions according to the respective probabilities which he attributes to them.

  \hfill -- \citet[pg. 130]{laplace1951essay}\end{quote}

  This voting method, if normalized, will result in an averaged probability in each bin, and can perhaps be interpreted as equivalent to a linear opinion pool for a discrete variable.} Given probability density functions $f_1(\theta),\ldots,f_n(\theta)$ and weights $w_1,\ldots,w_n\geq 0$ such that $\sum_{i=1}^n w_i=1$, we can pool the probability distributions thus:

\[\hat f(\theta)=\sum_{i=1}^n w_if_i(\theta)\]

A more general and perhaps more computationally useful definition is via the cumulative distribution funtions $F_1(\theta),\ldots,F_n(\theta)$:

\[\hat F(\theta)=\sum_{i=1}^n w_iF_i(\theta)\]

In practice we can set all weights to $1/n$ if we do not wish to favor any of the underlying estimates $f_i$.

\paragraph{Inference}\label{inference-6}

To turn the pooled distribution $\hat f$ or $\hat F$ into an interval for $\theta$, we can just take the appropriate quantiles of the distribution (e.g., for a symmetric 95\% interval, take $0.025$ and $0.975$ quantiles of $\hat f $ or $\hat F$.). One simple way to do this is to numerically evaluate $\hat F(\theta)$ at a grid of values for $\theta$. \cite{koepke2017consensus} suggests instead using a Monte Carlo method of, $T$ times, picking a distribution $f_i$ from $f_1,\ldots,f_n$ according to the weights $w_1,\ldots,w_n$, and then sampling a value from $f_i$. Collecting all these $T$ values, their quantiles will be approximations of quantiles of $\hat f$.

\subsection{Other}

\subsubsection{Birge ratio for forecasts}\label{birge-ratio-for-forecasts}

The settings of experiment aggregation and forecasting are two extremes, but certainly there are settings somewhere in between (e.g., maybe forecasters use different data sources, which should sometimes narrow the resulting estimate when combining forecasts, or sometimes widen it if the forecasts are widely dispersed).

\paragraph{Assumptions}\label{assumptions-6}

Because we think of $\theta$ as having some irreducible uncertainty, we will make explicit the distinction between $\theta$, the eventually-realized value, and $\mu$, the mean of $\theta$'s distribution.

\begin{enumerate}
\item
  $\theta\sim \mathcal{N}(\mu,\lambda^2)$.
\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\mu,c^2\sigma_i^2)$.
\item
  The given $\sigma_i^2$ is an attempt to estimate $c^2\sigma_i^2+\lambda^2$, and therefore $(1-c^2)\sigma_i^2$ is an estimate of $\lambda^2$.
\end{enumerate}

This is similar to the Birge Ratio defined above, but here the $y_i$'s are not directly measuring $\theta$, only its mean $\mu$.

The last assumption is about the nature of the forecasts: each forecaster, when estimating $\theta$, incorporates into $\sigma_i$ both their uncertainty in $\mu$ and the irreducible uncertainty $\lambda$.

\paragraph{Inference}\label{inference-7}

Let's use a frequentist inference procedure basen that for the Birge Ratio.

\begin{enumerate}
\item
  Calculate the weighted mean $\hat\mu=\frac{\sum_{i=1}^n w_iy_i}{\sum_{i=1}^nw_i}$, where $w_i=1/\sigma_i^2$. This will be our point estimate, the center of the interval. Note that if $c=1$, the standard deviation of $\hat\mu$ is $\sigma_w=\frac{1}{\sqrt{\sum_{i=1}^nw_i}}=\sqrt{\sum_{i=1}^n1/\sigma_i^2}$.
\item
  Calculate $\chi^2=\sum_{i=1}^n w_i(\bar{y}_w-x_i)^2$. If $c=1$, this will follow a Chi-squared distribution with degrees of freedom $n-1$, and will correspondingly have expectation $n-1$. Take $\hat c=\sqrt{\chi^2/(n-1)}$. Note the lack of truncation compared to the Birge Ratio method.
\item
  Now we can plug this in as a true value of $c$, and take the standard deviation of $\hat\mu$ as $\hat c\sigma_w=\sqrt{\sum_{i=1}^n1/\hat c\sigma_i^2}=\frac{\hat c}{\sqrt{\sum_{i=1}^nw_i}}$
\item
  Now, unlike in the Birge Ratio, we still need to estimate the irreducible uncertainty $\lambda$ of $\theta$.

  \begin{itemize}
  
  \item
    If $\hat c < 1$, it looks like the estimates are underdispersed, which leads us to suppose that each $\sigma_i$ includes an estimate of the irreducible uncertainity $\lambda$. Take \[\hat \lambda^2=(1-\hat c^2)\frac{1}{n}\sum_{i=1}^n \sigma_i^2.\] Alternatively, instead of taking a mean, we can take a median.
  \item
    If $\hat c \geq 1$, there may well be no irreducible uncertainty (or at least, it is small in comparison to the systematic differences in estimating $\mu$), so we take $\hat\lambda^2=0$.
  \end{itemize}
\item
  Now the total uncertainity in our estimate is $\sigma_\theta=\sqrt{\hat c^2\sigma_w^2 + \hat\lambda^2}$
\item
  Finally, form the $1-\alpha$ confidence interval for $\theta$ as \[\bar{y}_w\pm z_{1-{\alpha/2}}\cdot \hat \sigma_\theta,\] where $z_{1-\alpha/2}$ is the $z$-statistic at the desired level (e.g.~for a $1\sigma$ or $68.27\%$ interval, we use $z_{1-\alpha/2}=1$; for a $95\%$ interval, we have $\alpha=0.05$ and $z_{1-\alpha/2}=1.96$)
\end{enumerate}

When we obtain $\hat c\geq 1$, this procedure reduces to the inference procedure given for the Birge Ratio; and when $\hat c<1$, we take the same result from the Birge Ratio but add an extra uncertainity component by estimating $\lambda^2$. Note that, to design and justify a more principled procedure for estimating $\hat \lambda^2$ (e.g.~an MLE), we would need to specify how the $(y_i,\sigma_i)$ data is related to $\lambda$ more precisely than in Assumption 3 above.

As with the Birge Ratio, we can see how we could do inference in a Bayesian way by placing priors on $\mu$, $\lambda$, and $c$, and creating a generative model to link $\sigma_i$ to $\lambda$. Here is what a Bayesian model might look like:

\begin{itemize}

\item
  $\mu \sim \mathcal{N}(?,?)$
\item
  $\lambda\sim \mathrm{HalfNormal}(?)$
\item
  $c^2\sim\mathrm{Uniform}([0,1])$
\item
  $\gamma\sim\mathrm{Exp}(1)+1$
\item
  $\sigma_i^2\overset{\mathrm{iid}}{\sim}f(\lambda/(1-c^2))$
\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\mu,\gamma c^2\sigma_i^2)$
\end{itemize}

We have split up the $c$ we had previously (which did not allow for \emph{both} a multiplicative error underestimation and an accounting of the noise of $\theta$) into $\gamma$ and $c$. As we can see, we now have some difficulty, as the only way to identify $\lambda$ is through the $\sigma_i^2$'s, which are in principle estimates of $\lambda/(1-c^2)$. It is not obvious what would be an appropriate model $\sigma_i^2\overset{\mathrm{iid}}{\sim}\mathrm{f(\lambda/(1-c^2))}$.

\paragraph{Discussion}\label{discussion-1}

The frequentist procedure described is a simple extension of the Birge Ratio, and, through the estimate $\hat c$, nicely interpolates between meta-analysis and forecasting---the inference procedure offered, when $\hat c\geq1$, yields the Birge Ratio result, and when $\hat c<1$, automatically takes into account the irreducible uncertainity in $\theta$. Further, the method empirically estimates how much of the forecasts' uncertainties $\sigma_i$ represent irreducible uncertainity in $\theta$ or simply uncertainity in $\mu$, without requiring extra data from the forecasters.

The method does have some serious limitations, however:
1. The estimate of $\hat c$ is very unstable. In the Birge Ratio method, the reason that it $\hat c$ is truncated to be $\geq1$ is that, even in datasets where $c\geq 1$, we may often estimate $c<1$. As with the Birge Ratio, the uncertainity in $\hat c$ could be taken into account via a Bayesian approach.
2. Real forecasts do not come from the distribution $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\mu,c^2\sigma_i^2)$. In practice, some forecasters will be poorly calibrated and yield outlier $y_i$'s that are far from $\theta$ and have low $\sigma_i$'s. In the case of such outliers, we will likely have $\hat c>1$.
3. We assume that each forecast has the same ratio $(1-c^2)$ of irreducible to total uncertainty (this is very unrealistic).
4. Forecasts are often asymmetric.

In general, the problem of aggregating something which could be either a set of forecasts or a set of independent experiments, or something in between, given only $(y_i,\sigma_i)$ pairs, seems infeasible without making unrealistic assumptions.

\section{Evaluating the validity of assumptions}\label{evaluating-the-validity-of-assumptions}

We know that none of the sets of assumptions presented here will hold in the real world. But some assumptions will yield better inference (narrower intervals or correct coverage) than others. We therefore set out on the challenging task of empirically assessing which assumptions closely model real-world collections of experiments and which yield better inference. This will inform both the sensitivity analysis in our later simulation study and our practical recommendations. In particular, we will examine the following assumptions:

\begin{itemize}

\item
  Normality of $y_i$ about $\theta$
\item
  Symmetry of $y_i$ about $\theta$
\item
  Additive, multiplicative, or no systematic error (RE, Birge, and FE).
\item
  $P(y_i>\theta)=0.5$
\item
  Independence of $y_i$'s
\end{itemize}

To examine independence, we will look at the autocorrelation of the time series of estimates and examine any author effects (if there are any dependencies, certainly they will appear when one author has made multiple estimates).

The key difficulty with such an empirical analysis is that there are few quantities where we have both a ground truth and enough experiments estimating it to evaluate the above assumptions---if we had a ground truth, there probably wouldn't be so many experiments! Fortunately, in physics, some quantities have been estimated for centuries, with errors decreasing by orders of magnitude over time. We will analyze a couple cases of this, then move to a large-scale analysis of experiments in data from the Particle Data Group---where there are many experiments, but no ground truth.

In this section, we are not interested in directly evaluating the performance of the methods presented in Section \ref{sec:methods}. Only the validity of their assumptions. After this section, and the simulation study, we will return such of these datasets which have ground truths and conduct a direct performance evaluation. This way, we will have insights from the simulation study to frame our discussion.

\subsection{Historical studies with modern ground truth}\label{historical-studies}

Some physical quantities have the desirable quality that many experiments have been done in the past to measure them, and there is now a modern value which is either exact or known much more exactly than before. This makes them interesting case studies to evaluate the different assumptions presented in a setting where the ground-truth $\theta$ is known. We will use three sets of datasets, including measurements of:
\begin{itemize}
  \item Longstanding physical constants [17th century - 20th century].
  \item Atomic mass ratios [mid-19th century - early 20th century].
  \item Properties of fundamental particles [mid-20th century].
\end{itemize}
Thus we hope to cover a range of subjects and time periods from the modern history of the physical sciences. Unfortunately, we offer no datasets from 21st-century science, as those quantities which have been the subject of multiple studies tend to be those for which we do not yet have something approaching a ground-truth. Thus the insights we gain from this little investigation may not be so applicable to, for example, aggregating studies that reported Bayesian credible intervals---a more recent development.

We think this collection of datasets is an important advance. As we will see in a later section, it enables real-world stress tests of methods for meta-analysis in physics without resorting to a simulation with friendly assumptions.

\subsubsection{Long-term datasets}

\begin{figure}
  \includegraphics[width=\textwidth]{figs/historical.pdf}
  \caption{The three long-term historical datasets visualized, with the deviation of the result from the true value on the $x$-axis against the year on the $y$-axis. The symmetric logarithmic scale is used for the $x$-axis so that small deviations are still visible. In each dataset, we see the results converging towards the true value over time as experimental or observational techniques improve. We also see a remarkable symmetry in that the results converge towards the true value from both sides---we do not see, for example, results starting low then increasing towards the truth.}
  \label{fig:historical}
\end{figure}

We have obtained three datasets than span centuries of science: measurements of the speed of light, of the density of the Earth, and of the Astronomical Unit. Each has seen a reduction in error of orders of magnitude over time.

We don't expect any of these datasets to be complete or without errors. We collected them largely from review papers which list many experiments, and encountered several difficulties, including:
\begin{itemize}
  \item Some experiments are duplicated between reviews, with different values given. We typically pulled the data from the review which offered the most significant digits, or from that which offered an uncertainity if only one of them did.
  \item The date given for a result has no clear definition---it may be either the date of the result's publication, or the date when the experimental data was collected.
  \item Some experimentalists report a sequence of similar results in consecutive years---in this case we typically pick one of the results arbitrarily.
  \item Some reviews round data.
  \item Some reviews give data in nonstandard units, or narratively in relative units. We have tried to calculate the result appropriately.
\end{itemize}
Due to these issues, we will avoid drawing strong conclusions from the datasets.

\paragraph{The speed of light}

Since the 1980's, the speed of light, $c$, is defined as an exact constant in meters per second---the measurements of it became so precise that bulk of the remaining uncertainity was in the imprecision in the definition of the meter, thus the meter itself was redefined in terms of $c$.

The speed of light has been the subject of hundreds of experiments since the 1600's. We collated a dataset of 157 such experiments, spanning from 1676 to 1978. We relied on three reports which collected many experiments to create our dataset \citep{birge1934velocity,froome1971velocity,raynaud2013determining}, and added two of the final determinations \citep{evenson1972speed,blaney1974measurement}.

For the ground truth, we will take $\theta=c=299792458\mathrm{m}/\mathrm{s}$.

\paragraph{The density of the Earth}

The density of the Earth, $\rho$, is now generally agreed to be about 5.513. Though there is uncertainity in further digits, it is sufficiently precise when compared with historical estimates to offer another good case study.

We have collated a dataset of 37 experiments measuring $\rho$, spanning from 1687 to 1942, from three sources \citep{burgess1902value,sagitov1970current,hughes2006mean}.

\paragraph{The Astronomical Unit}

The Astronomical Unit is, roughly, the mean distance between the Earth and the Sun. The definition and thus the ``correct'' or ``target'' value has varied over time, but this is negligible compared to the errors in the observations. It is now fixed at $149597870700$ meters.

We have collected 27 estimates of this distance from \citet{atkins1964determination} and \citet{standish2004astronomical}.

\subsubsection{Chemical datasets}

\begin{figure}
  \includegraphics[width=\textwidth]{figs/chemical.pdf}
  \caption{The six chemical datasets obtained from \citet{clarke1910recalculation}. Results are arranged in order of their reported uncertainties. Shaded regions indicate the $1\sigma$ intervals indicated by modern values for their masses. We can see vast underestimations of error in the top four plots.}
  \label{fig:chemical}
\end{figure}

We obtain from \citet{clarke1910recalculation} several chemical datasets of interest, shown in Figure \ref{fig:chemical}. As described in the Section \ref{sec:history}, Clarke used least squares to combine estimates of various chemical properties. In doing so, he also helpfully provided tables of the values that went into his combinations. We will focus on his calculations of \textit{atomic mass ratios}, as those in particular have not had any drift in unit definitions which would make them incomporable to modern values without adjustments which we are not qualified to make.

Note that \citet{clarke1910recalculation} reports uncertainties in terms of ``probable error.'' We have converted these to $1\sigma$ uncertainties by dividing by $0.6745$---the appropriate conversion factor under a Normal assumption (also made by \citet{clarke1910recalculation}).

\subsubsection{Particle datasets}

\begin{figure}
  \includegraphics[width=\textwidth]{figs/particles.pdf}
  \caption{The six particle datasets obtained from \citet{roos1970review}. Results are arranged in order of publication, with more recent results lower. Shaded regions are the $1\sigma$ intervals indicated by \citet{navas2024review}. The datasets tend to show a narrowing of uncertainty and convergence to the truth over time.}
  \label{fig:particle}
\end{figure}

Although we don't have real ``ground truths'' for various properties of fundamental particles, the estimates offered by \citet{navas2024review} in many cases have uncertainties which, when compared to the experimental state-of-the-art in the mid-twentieth century, let them serve as an effective ground truth. We therefore compile some datasets from PDG's August 1970 edition \citet{roos1970review}. We chose this edition arbitrarily as a middle ground between older editions which are based on fewer available experimental results and newer editions which do not state older experimental results. We compile datasets according to these criteria:
\begin{itemize}
  \item Several experimental results available.
  \item Modern uncertainty much lower than the range of the results.
\end{itemize}
And apply the following processing steps:
\begin{itemize}
  \item Exclude any experiments not used in the average by \citet{roos1970review}
  \item For experiments with two-sided errors, take the average of the two and round up.
  \item Exclude any experiments which are used in the modern average by \citet{navas2024review}.
\end{itemize}
The datasets are shown in Figure \ref{fig:particle}.

\subsubsection{Checking assumptions}


\begin{table}
  \centering
  \caption{Summary of results of symmetry investigation on historical studies. For each study, $\theta$ is the ``ground truth.'' In some datasets, there is no real ground-truth, so the modern $1\sigma$ interval is taken as the range of plausible values for $\theta$. Where different plausible values for $\theta$ lead to different results this is indicated by an interval in the table. The columns are: $n$: the number of experiments in the dataset. $\#(>\theta)$: the number of experiments with a value larger than $\theta$. Prop. $>\theta$: the proportion of experiments with a value larger than $\theta$. Prop. $p$-value: the $p$-value from the hypothesis test with $H_0$: $P(y_i>\theta)=0.5$, $H_1$: $P(y_i>\theta)\neq 0.5$ (supposing that $y_i>\theta$, $y_j>\theta$ are independent events for $i\neq j$). Sign-rank $p$-value: the $p$-value from the sign-rank test, intended to test the symmetry of the distribution of $y_i$'s about $\theta$.}
  \label{tab:hist-sym}
  \smaller
  \begin{tabular}{l|rcccc}
    Dataset & $n$ & $\#>\theta$ & Prop. $>\theta$ & Prop. $p$-value & Sign-rank $p$-value \\\hline
    Density of Earth & 36 & 16 & 0.44 & 0.62 & 0.27 \\
    Speed of light & 157 & 80 & 0.51 & 0.87 & 0.78 \\
    Astronomical Unit & 27 & 11 & 0.41 & 0.44 & 0.16 \\
    Oxygen to Hydrogen mass ratio & 15 & [10, 11] & [0.67, 0.73] & [0.12, 0.3] & [0.0034, 0.064] \\
    Silver to Chlorine mass ratio & 23 & [2, 7] & [0.087, 0.3] & [6.6e-05, 0.093] & [4.5e-06, 0.0043] \\
    Silver to Iodine mass ratio & 13 & [3, 4] & [0.23, 0.31] & [0.092, 0.27] & [0.033, 0.04] \\
    Silver to Bromine mass ratio & 24 & [12, 19] & [0.5, 0.79] & [0.0066, 1] & [0.002, 0.81] \\
    Nitrogen mass (O=16) & 4 & [1, 3] & [0.25, 0.75] & [0.62, 1] & [0.38, 1] \\
    Carbon mass (O=16) & 4 & 0 & 0 & 0.12 & 0.12 \\
    $\Lambda$ mean lifetime $\mathrm{[10^{-10}s]}$ & 18 & 6 & 0.33 & 0.24 & [0.081, 0.18] \\
    $\Sigma^+$ mean lifetime $\mathrm{[10^{-10}s]}$ & 13 & [6, 8] & [0.46, 0.62] & [0.58, 1] & [0.68, 0.89] \\
    $m_{\pi^\pm}-m_{\pi^0}$ $\mathrm{[MeV]}$ & 7 & 4 & 0.57 & 1 & 1 \\
    $K^\pm$ mean lifetime $\mathrm{[10^{-8}s]}$ & 9 & 5 & 0.56 & 1 & [0.5, 0.57] \\
    $\pi^\pm$ mean lifetime $\mathrm{[10^{-9}s]}$ & 7 & 1 & 0.14 & 0.12 & 0.3 \\
    $\eta$ mass $\mathrm{[MeV]}$ & 8 & 7 & 0.88 & 0.07 & 0.11 \\
  \end{tabular}
\end{table}

\paragraph{Symmetry assumptions} The simplest assumptions to evaluate are $P(y_i>\theta)=0.5$ and the symmetry of $y_i$ about $\theta$. Results relating to these are shown in Table \ref{tab:hist-sym}. We find remarkable agreement with the first assumption in our datasets. Surprisingly, for the speed of light, 77 experiments produced underestimates, while 80 experiments produced overestimates. We conducted hypothesis tests with $H_0: P(y_i>\theta)=1/2$, $H_1: P(y_i>\theta)\neq 1/2$ for each dataset, and find $p<0.05$ in 2/13 datasets. To test the stronger assumption that the $y_i$'s are symmetrically distributed about $\theta$, we can use the one-sample sign-rank test. Results for this test are more mixed, with $p<0.05$ in 4/13 datasets.

We have no reason to expect the distribution of the $y_i$'s to be Normal within each set of experiments. But this does not mean that $y_i$ is not normal about $\theta$---we could consider a case where the $y_i$ are Normal about $\theta$, but have $\sigma_i$'s of different magnitudes. This would indeed produce a more-peaked-than-Normal distribution of $y_i$'s like the ones we see in Figure \ref{fig:historical}. It is in principle not be possible to use this data to disprove that $y_i\sim\mathcal{N}(\theta,?)$, because we can always choose an appropriate variance ``$?$'' to make the data consistent with this model. However, we can see by eye that $y_i\sim\mathcal{N}(\theta,\sigma_i^2)$ cannot always be the case---especially in the chemical datasets, we see that the $\sigma_i$'s are far too low for this to be likely. We will now investigate the different models for such underestimation.

\paragraph{Models for systematic error} 

\begin{table}
\centering
\caption{Summary of results of systematic error model investigation on historical studies. We compare the log-likelihood of the Birge Ratio, Random Effects, and Fixed Effect models at the maximum likelihood estimator. But, because we have a ground truth $\theta$ (or a best choice from a relatively narrow interval), we do not estimate $\hat\theta_{\mathrm{MLE}}$, and only estimate $\hat{c}_{\mathrm{MLE}}$ for the Birge Ratio and $\hat{\tau}_{\mathrm{MLE}}$ for Random Effects under the known $\theta$. Since each of these requires the $\sigma_i$ information, we exclude experiments with no $\sigma_i$ information, leading to reductions in $n$ in the historical datasets compared to Table \ref{tab:hist-sym} and the exclusion of the AU dataset. The Fixed Effect model is uniformly dominated by the Birge Ratio and Random Effects because it is a submodel of both. The Birge Ratio model achieves much higher log-likelihoods in the historical datasets (which causes the total to be in favor of Birge), while Random Effects achieves higher log-likelihoods in the chemical and particle datasets. Values of ``$\approx-\infty$'' indicate a likelihood so low as to be numerically intractable.}
\label{tab:hist-syst}
\smaller
\begin{tabular}{lrrrr}
  \hline Dataset & $n$ & Birge $\hat\ell$ & RE $\hat\ell$ & FE $\hat\ell$ \\\hline
  Density of Earth & 18 & 21.9 & 16.9 & -56.6 \\
  Speed of light & 99 & -493 & -611 & -611 \\\hline
  Oxygen to Hydrogen mass ratio & 15 & 16.4 & 25.4 & $\approx-\infty$ \\
  Silver to Chlorine mass ratio & 23 & 51.4 & 43.9 & -452 \\
  Silver to Iodine mass ratio & 13 & 14.7 & 16.8 & $\approx-\infty$ \\
  Silver to Bromine mass ratio & 24 & 73.8 & 85.3 & 34.9 \\
  Nitrogen mass (O=16) & 4 & 21 & 19.7 & 19.7 \\
  Carbon mass (O=16) & 4 & 12.1 & 12.7 & -284 \\\hline
  $\Lambda$ mean lifetime $\mathrm{[10^{-10}s]}$ & 18 & 2.7 & 6.58 & -5.08 \\
  $\Sigma^+$ mean lifetime $\mathrm{[10^{-10}s]}$ & 13 & 15.8 & 15.8 & 15.8 \\
  $m_{\pi^\pm}-m_{\pi^0}$ $\mathrm{[MeV]}$ & 7 & 13.1 & 14.7 & 13 \\
  $K^\pm$ mean lifetime $\mathrm{[10^{-8}s]}$ & 9 & 9.45 & 9.51 & 9.45 \\
  $\pi^\pm$ mean lifetime $\mathrm{[10^{-9}s]}$ & 7 & -3.1 & -3.93 & -3.93 \\
  $\eta$ mass $\mathrm{[MeV]}$ & 8 & -18.5 & -19.8 & -20.5 \\\hline
  Total (historical) & 117 & -472 & -594 & -668 \\
  Total (chemistry) & 83 & 189 & 204 & $\approx-\infty$ \\
  Total (particle) & 62 & 19.5 & 22.8 & 8.85 \\\hline
  Total & 262 & -263 & -367 & $\approx-\infty$ \\
  \hline
  \end{tabular}
\end{table}

Table \ref{tab:hist-sym} shows results for a comparison of the Birge Ratio, Random Effects, and Fixed Effect models.

\subsection{Fundamental particles}\label{fundamental-particles}

The Particle Data Group has published estimates of properties of fundamental particles since 1957 (see above for description of their measurement aggregation method). For example, the \emph{top quark} is a fundamental particle, and physicists can measure its mass (a property of the particle). As of the 2025 update, there are 1269 particle properties for which PDG provides an estimate (or ``average'') using their aggregation method which have at least three underlying measurements. Together, these use 6239 underlying measurements. Because there are so many, we can use this dataset to try to get a handle on the distribution of estimates in particle physics. One difficulty will be that we would like to put the measurements on a common scale, but we have no certain ground truth information by which to standardize them (otherwise there would be no need to measure).

\subsubsection{Distribution of measurements}\label{distribution-of-measurements}

For a given property of a particle, say we have $n$ measurements $y_1,\ldots,y_n$ with associated given standard deviations $\sigma_1,\ldots,\sigma_n$. We'd like to examine the distributional assumptions in generating $y_i$. The strongest assumption we can make is the Fixed Effects assumption that $y_i\overset{ind}{\sim}\mathcal{N}(\theta,\sigma_i^2)$, but we don't know the ground truth $\theta$. Let's compute their precision-weighted mean $\bar{y}=\frac{1}{S}\sum_{i=1}^n\frac{y_i}{\sigma_i^2}$, where $S=\sum_{i=1}^n \frac{1}{\sigma_i^2}$. This is equivalent to the aggregated estimate provided by PDG. Now we can consider the residuals $y_i-\bar{y}$. If $y_i\overset{ind}{\sim}\mathcal{N}(\theta,\sigma_i^2)$, then:
\[y_i-\bar{y}\sim \mathcal{N}\left(0, \sigma_i^2(1-\frac{1}{\sigma_i^2S})^2+\frac{S-\frac{1}{\sigma_i^2}}{S^2}\right).\]
Ideally, we'd like to put all of these $y_i-\bar{y}$ residuals from all different particle properties on the same scale, as now their distribution depends on the $\sigma_i$'s within each particle property. But we can simply divide $y_i-\bar{y}$ by the square root of the variance term given above. This will yield standardized residuals which, supposing that $y_i\overset{ind}{\sim} \mathcal{N}(\mu,\sigma_i^2)$, will follow a standard Normal distribution $\mathcal{N}(0,1)$. Do they?

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figs/pdg_std_residuals.pdf}
\caption{Distribution of the residuals of particle property measurements standardized using the method described above. They do not quite follow a Standard Normal distribution, especially evident with extra mass in the tails.}
\label{fig:pdg-std-residuals}
\end{figure}

As shown in Figure \ref{fig:pdg-std-residuals}, although they are surprisingly close to following a Standard Normal distribution, they do not quite follow it. The distribution is wider than an $\mathcal{N}(0,1)$ distribution, even though if it were true that $y_i\overset{ind}{\sim} \mathcal{N}(\mu,\sigma_i^2)$, the result should be $\mathcal{N}(0,1)$. What can we conclude from this? Intuitively, there are systematics at play which make the $y_i$'s more dispersed about the mean than the given $\sigma_i$'s and a Normal assumption would suggest. More precisely, the assumption $y_i\overset{ind}{\sim} \mathcal{N}(\mu,\sigma_i^2)$ is exactly the Fixed Effects assumption, where there is no systematic error. Therefore, in this dataset of thousands of particle properties in particle physics, performing inference under Fixed Effects assumptions would be suspect.

The Fixed Effects assumptions are suspect because PDG data does indeed have unaccounted-for systematic errors. But how many datasets have these? Let's go back to the full, unstandardized dataset and examine what values of $\hat c$ and $\hat \tau$ are achieved by the Birge Ratio method and Random Effects in PDG data. Figure \ref{fig:pdg-birge-re} displays this result. Even if there is no unaccounted-for error, it is still possible for $\hat c$ or $\hat \tau$ to be larger than 1 and 0 by random chance. Thus we have added a control to Figure \ref{fig:pdg-birge-re} displaying histograms for $\hat c$ and $\hat \tau$ obtained by resampling the whole dataset according to $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(0, \sigma_i)$.\footnote{The calculations of $\hat c$ and $\hat \tau$ are invariant to a shift in the mean, so we don't need to know $\theta$ to perform this resampling experiment.} This creates a hypothetical PDG dataset with no unaccounted-for errors.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figs/pdg_birge_re.pdf}
\caption{Within the PDG dataset, these are the empirical distributions of (left) the Birge Ratio $\hat c$ and (right) the $\hat\tau$ systematic error term from Random Effects. For both, we see that most sets of measurements ($57\%$) have a Birge Ratio $\hat c=1$ (no systematic error), and most ($75\%$)have a $\hat\tau=0$, indicating no (or low) unaccounted-for errors. However, judging by the control, this is a less than we would expect if the PDG data had no unaccounted-for error. Likewise, in the case of measurements with $\hat c>1$ or $\hat\tau>0$, we see that the PDG tends towards larger values of $\hat c$ and $\hat\tau$ than the control.
}
\label{fig:pdg-birge-re}
\end{figure}


Again, we see some more mass in the rightwards part of the distribution than we would expect under the Fixed Effects model $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(0, \sigma_i)$. We also see, in the non-control, a slightly higher rate of $\hat c >1$ and $\hat \tau > 0$. This again points us to the idea that the PDG data has some unaccounted-for errors, and thus the Fixed Effects model ought not to be used.

Thankfully, PDG does not use Fixed Effects to do inference. Instead, they use the Birge Ratio method described above. In the next section, we will use the PDG dataset to quantitatively compare different models.

\subsubsection{Model comparison}\label{model-comparison}

Let's now proceed to a direct model comparison for those methods which have well-specified generative models: these are Random Effects, Fixed Effects, and the Birge Ratio, which tell us exactly how the $y_i$'s are generated up to one or two unknown parameters.\footnote{Models like those for the Sign Test or Signed Rank Test would be harder to compare under this framework, as they do not specify a parametric distribution for the $y_i$'s.}

\textbf{Likelihoods}

In our dataset we can compute on each particle property, for Random Effects, the MLE of $\theta$ and $\tau$, for Fixed Effects, the MLE of $\theta$, and for the Birge Ratio, the MLE of $\theta$ and $c$. We can then compute the likelihood of the data at this MLE. We can also calculate standard metrics like the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC).\footnote{Recall that if $\ell$ is the log-likelihood, $k$ is the number of parameters, and $n$ is the number of samples, then $\mathrm{BIC}=k\ln(n)-2\ell$ and $\mathrm{AIC}=2k-2\ell$.}  Table \ref{tab:pdg-loglike} displays these results.

\begin{table}
  \centering
  \caption{MLE log-likelihood (totaled across all particle properties), as well as the BIC and AIC (averaged across particle properties) for the Birge Ratio, Random Effects, and Fixed Effects models on PDG data. The Birge Ratio model is used by PDG.}
  \label{tab:pdg-loglike}
  \begin{tabular}{lrrr}
    \hline
     & Total log-likelihood & Mean BIC & Mean AIC \\
    \hline
    Birge Ratio & -8087.33 & 16.32 & 17.38 \\
    Random Effects & -8022.13 & 16.21 & 17.27 \\
    Fixed Effects & -8905.87 & 16.20 & 16.73 \\
    \hline
    \end{tabular}
\end{table}

We should first note that the Birge Ratio and Random Effects models each have two unknown parameters---($\theta,c$) and ($\theta,\tau$) respectively---while the Fixed Effects model has one unknown parameter, $\theta$. So the Birge Ratio and Random Effects models can be compared directly using their log-likelihoods, without needing to account for a difference in the number of estimated parameters. That is, they are equally simple models. In Table \ref{tab:pdg-loglike}, we see that the Random Effects model achieves a larger log-likelihood than the Birge Ratio (and, correspondingly, lower BIC and AIC). This means that under these metrics, the Random Effects model is a better model for PDG data than the Birge Ratio---the model which is used by PDG to infer $\theta$. Before continuing further, let's examine whether the total log-likelihood favors Random Effects due to a few outlier particle properties. Figure \ref{fig:pdg-loglike} is a scatterplot of the Random Effects MLE log-likelihood against that for the Birge Ratio. It shows that particle properties more often favor Random Effects than they do the Ratio, and the Random Effects outperforming Birge Ratio in terms of the total log-likelihood is not solely driven by outliers.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/pdg_loglike.pdf}
  \caption{Log-likelihoods of the MLEs of the Random Effects and Birge Ratio models for each particle property in the PDG dataset described. Random Effects tends to be favored. The percentages do not add up to 100\% because many of the log-likelihoods are equal: this occurs in the case where $\hat\tau_{\mathrm{MLE}}=0$ and $\hat{c}_{\mathrm{MLE}}=1$.}
  \label{fig:pdg-loglike}
\end{figure}

Now we can turn to the curious fact in Table \ref{tab:pdg-loglike} that the Fixed Effects model outperforms both the Birge Ratio and Random Effects under BIC and AIC. Didn't we just establish in the previous section that the Fixed Effects model is not appropriate? Yes, but BIC and AIC, here calculated on a per-particle-property level and averaged, give an advantage to Fixed Effects because it has one fewer parameter. This yields an advantage of $2$ for AIC and an average advantage of $1.47$ for BIC. So, even if we can see based on descriptive statistics of the full PDG dataset that Fixed Effects does not fit the data well, it tends to yield lower BIC and AIC on a per-particle-property level. This indicates that, if we only have one small set of experiments that we wish to aggregate, it may be dangerous to select a model and method based only on the statistical properties of the small set. Perhaps the knowledge that ``there is nontrivial underestimation of errors in this set of experiments, and we will underestimate the final uncertainity if we don't account for this'' should alone be enough to disqualify using a model like Fixed Effects.

%TODO: cite the "two groups that should meet" paper's results on PDG data

\begin{itemize}

\item
  Bayes Factors
\item
  Posterior predictive checks (get posterior distributions for $\tau$ and $\hat{c}$ across the whole dataset. Then, generate data and see how closely it matches the truth).
\end{itemize}

\subsubsection{Inference comparison}\label{inference-comparison}

Let's now directly apply our different inference methods to this PDG dataset. Bearing in mind that PDG effectively uses the Birge Ratio, we will take a look at a few things:

\begin{itemize}

\item
  For Random Effects and the Birge Ratio, what is the distribution of $\tau$'s and $\hat c$'s?
\item
  How much wider/narrower are intervals from different methods?
\item
  How much overlap is there between intervals from different methods?
\end{itemize}

\subsubsection{Synthetic data evaluation}\label{synthetic-data-evaluation}

Let's perform an evaluation of different methods, using these standardized residuals to create synthetic data. In practice, for a fixed particle property, we don't expect the measurements to come from a distribution like that shown in \ref{pdg-std-residuals}. Some particle properties have measurements showing large systematic errors and severe underestimation of $\sigma_i$, while others do not.

\section{Simulation study}\label{simulation-study}

Let's do some simulations to benchmark the Sign Test (ST) against RE and BR. In every simulation, we will have these parameters:

\begin{itemize}
\item
  $n$: how many estimates we get.
\item
  $\theta=0$: the truth we want to contain in our interval
\item
  $\alpha/2$: our target tail probability.
\item
  $\sigma_i^2\sim\mathrm{Exp}(1)$: the standard deviations of the
  noise (observed)
\item
  $\tau$: a parameter we control to vary the level of systematic error
  or underestimation of $\sigma_i$'s. A larger $\tau$ indicates larger
  systematic errors or underestimation of $\sigma_i$'s.
\end{itemize}

All of the methods in this study are location- and scale-invariant, so we don't lose anything of interest by fixing $\theta=0$ and the scale of the $\sigma_i^2$'s. The different experiments will vary $n$, the distribution of the systematic errors, and their scale relative to the $\sigma_i^2$'s.

Because the Binomial distribution is discrete, for arbitrary choice of $(n,\alpha)$ the Sign Test we described will be conservative. Here are two ways to achieve exact or close coverage for arbitrary $\alpha$:

\begin{itemize}
\item
  Trivially, we could randomly shrink or collapse the interval to make it exact, but that would be silly.
\item
  We could make the interval not have symmetric tail probabilities. This would let us sometimes get closer to a $1-\alpha$ coverage, but the choice of which tail should be heaver in any given scenario is arbitrary.
\end{itemize}

So let's stick with the Sign Test as described. To provide a reasonable assessment of the its exactness and interval lengths, we will pick $\alpha$ so that the ST we propose can provide exact $1-\alpha$ coverage. Because $1\sigma$ intervals are commonly used in the sciences, we will pick the smallest $\alpha$ such that $1-\alpha\geq 0.6827$ and the ST can be exact.

For $n=10$, for example, this means targeting a coverage of $0.891$ (narrower symmetric intervals could achieve $0.656$---just below our stated minimum of $0.6827$).

For a choice of $\alpha$, we use the corresponding $z_{\alpha/2}$ to expand the intervals of the Random Effects and Birge methods. See Table \ref{tab:table-n} for our choices of $n$ and the corresponding target level, tail probability, and $z$-score.

\begin{table}[H]
\centering
\caption{Chosen level and corresponding $\alpha/2$ tail probability and $z$-score for different $n$. $n$ is evenly spaced on a logarithmic scale.}
\label{tab:table-n}
\begin{tabular}{rrrr}
\hline
$n$ & Level & $\alpha/2$ & $z_{\alpha/2}$ \\
\hline
3 & 0.750 & 0.125 & 1.150 \\
10 & 0.891 & 0.055 & 1.601 \\
31 & 0.719 & 0.141 & 1.078 \\
100 & 0.729 & 0.136 & 1.100 \\
316 & 0.715 & 0.143 & 1.069 \\
1000 & 0.703 & 0.148 & 1.044 \\
\hline
\end{tabular}  

\end{table}

\subsection{Simulation settings}\label{simulation-settings}

\textbf{Random Effects model}

\begin{itemize}
\item
  $\theta_i\overset{\mathrm{iid}}{\sim}\mathcal{N}(0,\tau^2)$: the systematic errors
\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\theta_i,\sigma_i^2)$: the estimates (observed)
\end{itemize}

\textbf{Birge}

\begin{itemize}

\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(0,\tau^2\sigma_i^2)$: the estimates (observed)
\end{itemize}

\textbf{Random Effects with outliers}

\begin{itemize}
\item
  $\theta_i\overset{\mathrm{iid}}{\sim}\tau\cdot \mathrm{Cauchy}(0,1)$: the systematic errors
\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\theta_i,\sigma_i^2)$: the estimates (observed)
\end{itemize}

This is effectively Random Effects, just with a different distribution for $\theta_i$ that makes outliers more likely.

\textbf{Offset}

\begin{itemize}

\item
  $y_i\overset{\mathrm{iid}}{\sim}\mathcal{N}(\tau,\sigma_i^2)$: the estimates (observed)
\end{itemize}

In this setting, all estimates are systematically offset by $\tau$ from the truth. As discussed in the first section, in principle there is no way to obtain the truth without knowing something about $\tau$, so all of the methods should fail. But we will see which methods fail faster.

\textbf{Correlated Random Effects}

\begin{itemize}
\item
  $\boldsymbol{\theta}\sim \mathcal{N}_n(\mathbf{0},\Sigma)$ where $\Sigma=\tau^2(0.8\mathbf{I}_n+0.2)$
\item
  $y_i\overset{\mathrm{ind}}{\sim}\mathcal{N}(\theta_i,\sigma_i^2)$
\end{itemize}

This is like Random Effects, but where the systematic errors are correlated between experiments. It will violate the assumptions of all the methods we have presented. Note that due to the computational difficulty of sampling a high-dimensional multivariate Normal, we will not experiment with $n>100$ under this model.

\subsection{Simulation results}\label{simulation-results}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figs/performance_random_effects.pdf}
\includegraphics[width=\textwidth]{figs/performance_birge.pdf}
\includegraphics[width=\textwidth]{figs/performance_random_effects_outliers.pdf}
\label{fig:sim-results}
\caption{Simulation results under the three settings satisfying our assumptions. The darkest color is $n=1000$, and the lightest color is $n=3$.}
\end{figure}

We show results for the settings satisfying our assumptions (Random Effects, Birge, Random Effects with Outliers) in \ref{fig:sim-results}. Since the target coverage level is varied across simulations, we report the difference in achieved coverage and target coverage. Here are some observations.

\begin{itemize}
\item
  The Sign Test perfectly achieves target coverage in every single scenario. This is not surprising given its mathematical setup, and that every single scenario satisfies ST's assumptions.
\item
  Under the Random Effects model, RE yields better coverage with smaller intervals when the systematic errors are small, but does not achieve coverage when systematic errors are comparable to or larger than noise. This is surprising until we remember that inference for the Random Effects model is approximate, not exact. BR performs very poorly, espeially for large $n$.
\item
  Under the Birge model, note that it is slightly unrealistic to have $\tau<1$, because that implies that the $\sigma_i$'s given to us are actually \emph{overestimates} of the true standard deviation. Considering the case where $\tau\geq 1$, the BR method works well, as we would expect, though does it does not achieve target coverage when $n$ is small (as inference is approximate and not exact). BR also offers far narrower intervals than ST. RE works decently well; maintains at least target coverage, but offers larger intervals for large $n$ than Binomial and BR for roughly $\tau>3$. For context, in the gravitational constant data shown later \citep{tiesinga2021codata}, we estimate $\tau\approx 3.6$ under the Birge model.
\item
  Under Random Effects with Outliers, which satisfies the assumptions of neither RE nor BR, BR works poorly, while RE is fairly competitive with ST (though it yields intervals larger by orders of magnitude for large $n$ with $\tau>1$). This difficulty in this setting is outliers, especially at large $n$, which pull or greatly expand the RE and BR intervals. As expected, ST still achieves perfect coverage in this setting. In general, ST is robust to outliers as it only depends on the ranking of the experiments, not their relative magnitudes.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figs/performance_adversarial.pdf}
\includegraphics[width=\textwidth]{figs/performance_random_effects_corr.pdf}
\label{fig:sim-results2}
\caption{Simulation results under the two settings not satisfying our assumptions. The darkest color is $n=1000$, and the lightest color is $n=3$. Note that in the Correlated Random Effects setting, we don't do simulations for $n>100$ due to computational difficulties.}
\end{figure}



\ref{fig:sim-results2} contains results for the settings not satisfying our assumptions (Adversarial and Correlated Random Effects).

\begin{itemize}
\item
  As $\tau$ increases, the Sign Test maintains target coverage for longer than RE and BR for all values of $n$.
\item
  For small $n$, all methods are less sensitive to the these particular assumption violations.
\end{itemize}

\subsection{Simulation discussion}\label{simulation-discussion}

The main message from the simulations is that, by using nonparametric methods which ignore the $\sigma_i$'s, much robustness is gained, and (usually) little precision lost. The primary exception to this is when systematic errors are quite small and the scales of the noise vary widely. Intuitively, this is the case when the $\sigma_i$'s are most informative, and the most can be gained from using them.

But when systematic errors are large, or systematic errors are within the same order of magnitude of each other, it seems not much is gained by incorporating the $\sigma_i$ information.

This parallels results from nonparametric statistics that show nonparametric methods like the Sign Test and Signed-Rank Test don't sacrifice much efficiency over methods like a $t$-test which make stronger assumptions \citep{conover1999practical}.

\section{Results on real data}\label{results-on-real-data}

\subsection{CODATA: physical constants}\label{codata-physical-constants}

We ought to run a comparison on some real data. Let's see what happens with CODATA (Committee on Data of the International Science Council) estimates of physical constants. Because physical constants aren't known exactly, we don't have a ground truth, but we can at least see the behavior of the different methods with real experimental data.

\subsubsection{Gravitational constant}\label{gravitational-constant}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figs/G0.pdf}
\label{fig:G}
\caption{Results of different aggregation methods for estimates of the gravitational constant $G$ \cite[Table XXIX]{tiesinga2021codata}. Error bars are $1\sigma$ intervals using the $sigma_i$'s reported by each study.}
\end{figure}


We get data from 16 experiments estimating the gravitational constant $G$ from the 2018 CODATA adjustment \citep[Table XXIX]{tiesinga2021codata}. Results are shown in \ref{fig:G}. Note that for $n=16$, the nominal coverage level of the Binomial interval is $0.7900$. The nominal coverages of the other intervals under their respective models are $0.6827$.

We see that the CODATA interval is slightly wider than that given by our application of BR. CODATA uses BR too, but there are two differences:

\begin{itemize}
\item
  They take into account small correlations between some pairs of experiments.\footnote{(NIST-82, LANL-97), (HUST-05, HUST-09), and (HUST-09, HUST$_\text{T}$-18) are correlated \citep{tiesinga2021codata}.}
\item
  Their inference on $\tau$, effectively the expansion factor for the $\sigma_i$'s is heuristic, and they choose $\tau=3.9$ to ``decrease the residuals to below two'' \citep{tiesinga2021codata}.
\end{itemize}

The CODATA and BR intervals are pulled to the right by the small $\sigma_i$'s on many of the rightmost points. The RE interval is just about disjoint with the CODATA and BR intervals---this is because it models systematic errors additively, rather than multiplicatively. The Binomial interval is wider than any of the other intervals. Note how it is supported by the HUST-09 and UZur-06 points, and has 5 points outside it on its left, and 5 points outside on the right.

\subsubsection{Planck constant}\label{planck-constant}

\begin{figure}[H]
\centering
\includegraphics{figs/h0.pdf}
\label{fig:h}
\caption{Results of different aggregation methods for estimates of the Planck constant $h$ \cite[Table 26]{mohr2012codata}. Error bars are $1\sigma$ intervals using the $sigma_i$'s reported by each study.}
\end{figure}

We get data from 11 experiments estimating the Planck constant $h$ from the 2010 CODATA adjustment \cite[Table 26]{mohr2012codata}.\footnote{Since the 2019 SI unit redefinition, the SI units are defined, in part, based on the Planck constant, so there is no longer ``uncertainty'' in it in terms of SI units. But the dataset still offers an interesting case study.}

Results are shown in \ref{fig:h}. Note that for $n=11$, the nominal coverage level of the Binomial interval is $0.7734$. The nominal coverages of the other intervals under their respective models are $0.6827$.

We see again that the CODATA interval is wider than that given by our BR---this is because CODATA take into account some correlations between data points. In the Planck constant data, the BR interval is contained within RE, and RE is contained with the Binomial interval. We see that the Binomial interval is supported by points NMI-89 and IAC-11, with 3 points lying outside to either side.

\subsection{Targeting the same coverage level on real data}\label{targeting-the-same-coverage-level-on-real-data}

\begin{figure}[h]
\centering
\includegraphics{figs/G1.pdf}
\includegraphics{figs/h1.pdf}
\label{fig:Gh-wider}
\caption{Since the Sign Test can target only a discrete set of coverages, for the $G$ data it will (under the assumptions) achieve coverage 79.0\%, and 77.3\% for the $h$ data. For comparison, in this figure the intervals from CODATA, RE, and BR are expanded to target these coverage levels. The CODATA, RE, and BR intervals from \ref{fig:G} and \ref{fig:h} have been scaled them by the corresponding $z_{\alpha/2}$.}
\end{figure}

\section{Case study: Fundamental particles}\label{case-study-fundamental-particles}

\section{Case study: Ice sheet mass balance}\label{case-study-ice-sheet-mass-balance}

\section{Discussion}\label{discussion-2}

The Sign Test is strong because it:

\begin{itemize}
\item
  Makes weaker assumptions which are implied by many existing common assumptions.
\item
  Yields confidence intervals which exactly achieve their nominal coverage level for any value of $n$ and for any data-generating process which satisfies the relatively weak assumptions.
\item
  Is insensitive to problems in the $\sigma_i$ uncertainties provided for each underlying estimate.
\item
  Is robust to outliers, because it only depends on the ranking of the $y_i$'s, not their relative magnitudes or residuals.
\item
  Relies on fairly simple math, making its justification, implementation, and interpretation easy for non-statisticians.
\end{itemize}

It is weak in that it:

\begin{itemize}
\item
  Cannot exactly achieve an arbitrary target coverage level for finite $n$ due to the discreteness of the Binomial distribution. (But it will be conservative, and anyway for any $n$ one can pick from a set of coverage levels which can be achieved exactly).
\item
  Yields wider intervals than needed if we know more about the data-generating process---methods tailored to that DGP can achieve coverage with narrower intervals.
\item
  Does not take advantage of the information present in the $\sigma_i$'s.
\end{itemize}

\bibliography{references}

\end{document}