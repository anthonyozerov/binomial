{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# use tex\n",
    "plt.rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.read_csv('data/c.csv', comment='#')\n",
    "c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_df = pd.read_csv('data/rho.csv', comment='#')\n",
    "rho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_df = pd.read_csv('data/au.csv', comment='#')\n",
    "au_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'rho': rho_df,\n",
    "    'c': c_df,\n",
    "    'au': au_df,\n",
    "}\n",
    "truths = {\n",
    "    'rho': 5.513,\n",
    "    'c': 299792.458,\n",
    "    'au': 149597870700,\n",
    "}\n",
    "yscales = {\n",
    "    'rho': 'symlog',\n",
    "    'c': 'symlog',\n",
    "    'au': 'symlog',\n",
    "}\n",
    "linthresh = {\n",
    "    'rho': 0.01,\n",
    "    'c': 0.1,\n",
    "    'au': 1,\n",
    "}\n",
    "\n",
    "nice_names = {\n",
    "    'c': 'Speed of light',\n",
    "    'rho': 'Density of Earth',\n",
    "    'au': 'Astronomical Unit',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 5), sharey=True, gridspec_kw={'wspace': 0.05})\n",
    "xlabels = {\n",
    "    'rho': r'Difference from true value $[\\mathrm{g/cm^3}]$',\n",
    "    'c': r'Difference from true value $[\\mathrm{km/s}]$',\n",
    "    'au': r'Difference from true value $[\\mathrm{km}]$',\n",
    "}\n",
    "historical_keys = ['rho', 'c', 'au']\n",
    "for i, name in enumerate(historical_keys):\n",
    "    ax = axs[i]\n",
    "\n",
    "    values = datasets[name].value - truths[name]\n",
    "    if name == 'au':\n",
    "        values = values / 1000\n",
    "    dates = datasets[name].year\n",
    "\n",
    "    ax.plot(values, dates, '.', color='black')\n",
    "    ax.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "    # reverse y axis\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    if yscales[name] == 'symlog':\n",
    "        print(name)\n",
    "        ax.set_xscale('symlog', linthresh=linthresh[name])\n",
    "        # skip every other tick\n",
    "        n_ticklabels = len(ax.xaxis.get_ticklabels())\n",
    "        for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "            if n % 2 != 0 and label.get_text() != '$\\\\mathdefault{0}$':\n",
    "                label.set_visible(False)\n",
    "        ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)\n",
    "    ax.set_xlabel(xlabels[name])\n",
    "    ax.set_ylim(2000, 1650)\n",
    "    ax.set_title(nice_names[name])\n",
    "    # make top x limit and bottom x limit equal\n",
    "    xlim = max(abs(ax.get_xlim()[0]), abs(ax.get_xlim()[1]))\n",
    "    ax.set_xlim(-xlim, xlim)\n",
    "axs[0].set_ylabel('Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/historical.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get error on ratio calculations\n",
    "def clarke_ratio(A, a, B, b, C=100, c=0):\n",
    "    # if A:B is the ratio, we want x where A:B = C:x\n",
    "    value = (B / A) * C\n",
    "    uncertainty = np.sqrt((B*C*a/A)**2 + (C*b)**2 + (B*c)**2)/A\n",
    "    return value, uncertainty\n",
    "def clarke_quotient(A, a, B, b):\n",
    "    # get the ratio B:A and its error\n",
    "    value = B/A\n",
    "    uncertainity = np.sqrt((B*a/A)**2+b**2)/A\n",
    "    return value, uncertainity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_df = pd.read_csv('data/clarke/H-O-mass.csv', comment='#')\n",
    "ho_df['uncertainty'] = ho_df['proberr'] / 0.6745\n",
    "agcl_df = pd.read_csv('data/clarke/Ag-Cl-mass.csv', comment='#')\n",
    "agcl_df['uncertainty'] = agcl_df['proberr'] / 0.6745\n",
    "agi_df = pd.read_csv('data/clarke/Ag-I-mass.csv', comment='#')\n",
    "agi_df['uncertainty'] = agi_df['proberr'] / 0.6745\n",
    "agbr_df = pd.read_csv('data/clarke/Ag-Br-mass.csv', comment='#')\n",
    "agbr_df['uncertainty'] = agbr_df['proberr'] / 0.6745\n",
    "no_df = pd.read_csv('data/clarke/N-mass.csv', comment='#')\n",
    "no_df['uncertainty'] = no_df['proberr'] / 0.6745\n",
    "co_df = pd.read_csv('data/clarke/C-mass.csv', comment='#')\n",
    "co_df['uncertainty'] = co_df['proberr'] / 0.6745\n",
    "\n",
    "datasets['ho'] = ho_df\n",
    "datasets['agcl'] = agcl_df\n",
    "datasets['agi'] = agi_df\n",
    "datasets['agbr'] = agbr_df\n",
    "# datasets['no'] = no_df\n",
    "# datasets['co'] = co_df\n",
    "\n",
    "truths['agcl'] = clarke_ratio(107.8682, 0.0002, 35.453, 0.004, 100, 0)\n",
    "truths['agbr'] = clarke_ratio(107.8682, 0.0002, 79.904, 0.003, 100, 0)\n",
    "truths['agi'] = clarke_ratio(107.8682, 0.0002, 126.90447, 0.00003, 100, 0)\n",
    "truths['ho'] = clarke_quotient(1.0080, 0.0002, 15.9995, 0.0005)\n",
    "# truths['no'] = clarke_ratio(15.9995, 0.0005, 14.007, 0.001, 16, 0)\n",
    "# truths['co'] = clarke_ratio(15.9995, 0.0005, 12.011, 0.002, 16, 0)\n",
    "\n",
    "nice_names['ho'] = 'O:H mass ratio'\n",
    "nice_names['agcl'] = 'Ag:Cl mass ratio'\n",
    "nice_names['agi'] = 'Ag:I mass ratio'\n",
    "nice_names['agbr'] = 'Ag:Br mass ratio'\n",
    "# nice_names['no'] = 'N mass (O=16)'\n",
    "# nice_names['co'] = 'C mass (O=16)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(8, 3))\n",
    "from methods import birge, random_effects_hksj, binomial_method\n",
    "\n",
    "plot_est = False\n",
    "chemistry_keys = ['ho', 'agcl', 'agi', 'agbr']\n",
    "\n",
    "for i, name in enumerate(chemistry_keys):\n",
    "    ax = axs.flatten()[i]\n",
    "    # values = np.array(datasets[name].value - truths[name])\n",
    "    truth = truths[name][0]\n",
    "    err = truths[name][1]\n",
    "    ax.axvspan(truth-err, truth+err, color='black', alpha=0.3)\n",
    "\n",
    "    values = np.array(datasets[name].value)\n",
    "    errs = np.array(datasets[name].uncertainty)\n",
    "    # sort by decreasing error\n",
    "    sort_idx = np.argsort(errs)[::-1]\n",
    "    values = values[sort_idx]\n",
    "    errs = errs[sort_idx]\n",
    "    \n",
    "    ax.errorbar(values, np.arange(len(values)), xerr=errs, fmt='.', markersize=2, linewidth=1, color='black')\n",
    "    ax.invert_yaxis()\n",
    "    # xlim = max(abs(ax.get_xlim()[0]), abs(ax.get_xlim()[1]))\n",
    "    # ax.set_xlim(-xlim, xlim)\n",
    "    if plot_est:\n",
    "        interval_birge, _, _, _ = birge(values, errs, coverage=0.6827)\n",
    "        ax.axvline(interval_birge[0], color='red', linestyle='--', linewidth=1)\n",
    "        ax.axvline(interval_birge[1], color='red', linestyle='--', linewidth=1)\n",
    "        interval_re, _, _, _ = random_effects_hksj(values, errs, coverage=0.6827)\n",
    "        ax.axvline(interval_re[0], color='blue', linestyle='--', linewidth=1)\n",
    "        ax.axvline(interval_re[1], color='blue', linestyle='--', linewidth=1)\n",
    "        binomial_lower, _ = binomial_method(values, p=0.5, target=0.15865, which='lower')\n",
    "        ax.axvline(binomial_lower, color='green', linestyle='--', linewidth=1)\n",
    "        binomial_upper, _ = binomial_method(values, p=0.5, target=0.15865, which='upper')\n",
    "        ax.axvline(binomial_upper, color='green', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_title(nice_names[name])\n",
    "\n",
    "    # ax.axvline(truth, color='black', linestyle='--', linewidth=1)\n",
    "    # ax.axvline(truth-err, color='grey', linestyle='--', linewidth=1)\n",
    "    # ax.axvline(truth+err, color='grey', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # ax.set_xlabel(xlabels[name])\n",
    "    # remove y ticks\n",
    "    ax.set_yticks([])\n",
    "    ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "    xmin = ax.get_xlim()[0]\n",
    "    xmax = ax.get_xlim()[1]\n",
    "    ax.set_xlim(min(xmin, truth-err), max(xmax, truth+err))\n",
    "    # add top ticks\n",
    "    ax.tick_params(axis='x', top=True)\n",
    "    # make ticks point inwards\n",
    "    ax.tick_params(direction='in')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/chemical.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "particle_keys = sorted([f.split('.')[0] for f in os.listdir('data/pdg1970') if f.endswith('.csv')])\n",
    "print(particle_keys)\n",
    "units = {}\n",
    "for p in particle_keys:\n",
    "    path = f'data/pdg1970/{p}.csv'\n",
    "    datasets[p] = pd.read_csv(path, comment='#')\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        nice_names[p] = lines[0].strip('# ').strip('\\n')\n",
    "        units[p] = lines[1].strip('# ').strip('\\n')\n",
    "        value = float(lines[2].split(':')[1].strip())\n",
    "        sigma = float(lines[3].split(':')[1].strip())\n",
    "        truths[p] = (value, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_group_keys = {\n",
    "    'T': 'Lifetime',\n",
    "    'MM': 'Mag.~moment',\n",
    "    'M': 'Mass',\n",
    "    'M-': 'Mass',\n",
    "    'DEL': 'Decay param',\n",
    "    'D': 'Mass diff',\n",
    "    'W': 'Width',\n",
    "    'R': 'Branching ratio',\n",
    "    'L+E': 'Decay param',\n",
    "    'A': 'Decay param',\n",
    "    'F+-': 'Decay param',\n",
    "    'XI': 'Decay param'\n",
    "}\n",
    "from collections import defaultdict\n",
    "particle_group_map = {}\n",
    "remaining = set(particle_keys)\n",
    "for k, v in particle_group_keys.items():\n",
    "    for p in remaining:\n",
    "        if k in p[1:]:\n",
    "            particle_group_map[p] = v\n",
    "            remaining = remaining - set([p])\n",
    "assert len(remaining) == 0, print(remaining)\n",
    "nice_names.update({v:v for v in particle_group_keys.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest\n",
    "from methods import sign_rank_test\n",
    "\n",
    "names = list(datasets.keys())\n",
    "\n",
    "results = {}\n",
    "\n",
    "def format_number(x):\n",
    "    if x == -np.inf:\n",
    "        return r'$\\approx-\\infty$'\n",
    "    if x == np.inf:\n",
    "        return r'$\\approx\\infty$'\n",
    "    if x >= 1e2 or x <= -1e2:\n",
    "        return f'${str(int(x))}$'\n",
    "    return r'$\\num{{{0:.2g}}}$'.format(x)\n",
    "\n",
    "def fmt_result(result):\n",
    "    # result is a list\n",
    "    minmax = [min(result), max(result)]\n",
    "    result_fmt = [format_number(x) for x in minmax]\n",
    "    # get unique values (preserving order)\n",
    "    result_fmt = pd.unique(np.array(result_fmt))\n",
    "    if len(result_fmt) == 1:\n",
    "        return result_fmt[0]\n",
    "    else:\n",
    "        return f'[{\", \".join(result_fmt)}]'\n",
    "\n",
    "for n in names:\n",
    "    results[n] = {}\n",
    "    results[n]['count'] = [len(datasets[n])]\n",
    "\n",
    "    truth_vals = []\n",
    "    if hasattr(truths[n], '__iter__'):\n",
    "        truth_vals.append(truths[n][0]-truths[n][1])\n",
    "        truth_vals.append(truths[n][0]+truths[n][1])\n",
    "        truth_vals.append(truths[n][0])\n",
    "    else:\n",
    "        truth_vals.append(truths[n])\n",
    "    \n",
    "    results[n]['num_over'] = [np.sum(datasets[n].value > t) for t in truth_vals]\n",
    "    results[n]['prop_over'] = [np.sum(datasets[n].value > t) / len(datasets[n]) for t in truth_vals]\n",
    "\n",
    "    # binomial test\n",
    "    results[n]['binom_p_value'] = [binomtest(np.sum(datasets[n].value > t), len(datasets[n]), p=0.5, alternative='two-sided').pvalue for t in truth_vals]\n",
    "\n",
    "    results[n]['sign_rank_p_value'] = [sign_rank_test(datasets[n].value, t) for t in truth_vals]\n",
    "\n",
    "results_tex = {}\n",
    "for n, res in results.items():\n",
    "    results_tex[n] = {}\n",
    "    for k, val in res.items():\n",
    "        results_tex[n][k] = fmt_result(val)\n",
    "\n",
    "# put all the results in a pandas dataframe\n",
    "results_df = pd.DataFrame(results_tex).T\n",
    "results_df = results_df.drop(index=particle_keys)\n",
    "latex_names = [nice_names[k] for k in results_df.index]\n",
    "\n",
    "\n",
    "# use np to save latex table with & separator\n",
    "rows = results_df.values.tolist()\n",
    "rows = [list(row) for row in rows]\n",
    "rows = [[latex_names[i]] + rows[i] for i in range(len(rows))]\n",
    "txt = ' \\\\\\\\\\n'.join([' & '.join(row) for row in rows])\n",
    "print(txt)\n",
    "with open('tables/hist-sym.tex', 'w') as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(particle_keys))\n",
    "fig, axs = plt.subplots(6, 10, figsize=(8, 8), gridspec_kw={'wspace':0, 'hspace': 0.4})\n",
    "from methods import birge, random_effects_hksj, binomial_method\n",
    "assert len(particle_keys) <= len(axs.flatten())\n",
    "plot_est = False\n",
    "# particle_keys = ['lambda-lifetime', 'sigma+-lifetime', 'pion-mass-diff', 'charged-kaon-lifetime', 'charged-pion-lifetime', 'eta-mass']\n",
    "\n",
    "def pval_to_stars(pval):\n",
    "    if pval < 0.001:\n",
    "        return '***'\n",
    "    elif pval < 0.01:\n",
    "        return '**'\n",
    "    elif pval < 0.05:\n",
    "        return '*'\n",
    "\n",
    "for i, name in enumerate(particle_keys):\n",
    "    ax = axs.flatten()[i]\n",
    "    # values = np.array(datasets[name].value - truths[name])\n",
    "    truth = truths[name][0]\n",
    "    err = truths[name][1]\n",
    "    ax.axvspan(truth-err, truth+err, color='black', alpha=0.3)\n",
    "\n",
    "    values = np.array(datasets[name].value)\n",
    "    errs = np.array(datasets[name].uncertainty)\n",
    "    # sort by decreasing error\n",
    "    sort_idx = np.argsort(datasets[name].year)\n",
    "    values = values[sort_idx]\n",
    "    errs = errs[sort_idx]\n",
    "    \n",
    "    ax.errorbar(values, np.arange(len(values)), xerr=errs, fmt='.', markersize=2, linewidth=1, color='black')\n",
    "    ax.invert_yaxis()\n",
    "    # xlim = max(abs(ax.get_xlim()[0]), abs(ax.get_xlim()[1]))\n",
    "    # ax.set_xlim(-xlim, xlim)\n",
    "    if plot_est:\n",
    "        interval_birge, _, _, _ = birge(values, errs, coverage=0.6827)\n",
    "        ax.axvline(interval_birge[0], color='red', linestyle='--', linewidth=1)\n",
    "        ax.axvline(interval_birge[1], color='red', linestyle='--', linewidth=1)\n",
    "        interval_re, _, _, _ = random_effects_hksj(values, errs, coverage=0.6827)\n",
    "        ax.axvline(interval_re[0], color='blue', linestyle='--', linewidth=1)\n",
    "        ax.axvline(interval_re[1], color='blue', linestyle='--', linewidth=1)\n",
    "        binomial_lower, _ = binomial_method(values, p=0.5, target=0.15865, which='lower')\n",
    "        ax.axvline(binomial_lower, color='green', linestyle='--', linewidth=1)\n",
    "        binomial_upper, _ = binomial_method(values, p=0.5, target=0.15865, which='upper')\n",
    "        ax.axvline(binomial_upper, color='green', linestyle='--', linewidth=1)\n",
    "\n",
    "    # ax.set_title(f'{nice_names[name]} {units[name]}')\n",
    "    ax.set_title(r'\\texttt{'+name+'}', pad=2)\n",
    "\n",
    "    # ax.axvline(truth, color='black', linestyle='--', linewidth=1)\n",
    "    # ax.axvline(truth-err, color='grey', linestyle='--', linewidth=1)\n",
    "    # ax.axvline(truth+err, color='grey', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # ax.set_xlabel(xlabels[name])\n",
    "    # remove y ticks\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "    xmin = ax.get_xlim()[0]\n",
    "    xmax = ax.get_xlim()[1]\n",
    "    ax.set_xlim(min(xmin, truth-err), max(xmax, truth+err))\n",
    "    # add top ticks\n",
    "    ax.tick_params(axis='x', top=True)\n",
    "    # make ticks point inwards\n",
    "    ax.tick_params(direction='in')\n",
    "\n",
    "    pvals = [min(results[name]['binom_p_value']), min(results[name]['sign_rank_p_value'])]\n",
    "    for i, pval in enumerate(pvals):\n",
    "        if pval < 0.05:\n",
    "            stars = pval_to_stars(pval)\n",
    "            side = 'left' if i==0 else 'right'\n",
    "            x = 0.1 if i==0 else 0.91\n",
    "            ax.text(x, -0.02, stars, color='black', ha=side, va='top', transform=ax.transAxes)\n",
    "\n",
    "# remove unused axes\n",
    "for ax in axs.flatten()[len(particle_keys):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.savefig('figs/particles.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.stats import chi2\n",
    "# print(values)\n",
    "# res.fun\n",
    "chi2.ppf(0.05, df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import birge, random_effects_mle\n",
    "from scipy.stats import norm, chi2\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "results = {}\n",
    "def fmt_result(r, bold=False):\n",
    "    if bold:\n",
    "        start = r'$\\mathbf{'\n",
    "        end = r'}$'\n",
    "    else:\n",
    "        start = r'$'\n",
    "        end = r'$'\n",
    "    if r == -np.inf:\n",
    "        return r'$\\approx-\\infty$'\n",
    "    elif r < -100:\n",
    "        return start + str(int(r)) + end\n",
    "    else:\n",
    "        return start + r'{:.3g}'.format(r) + end\n",
    "category_map = {k: 'chemistry' for k in chemistry_keys}\n",
    "category_map.update({k: 'particle' for k in particle_keys})\n",
    "category_map.update({k: 'historical' for k in historical_keys})\n",
    "\n",
    "key_order = []\n",
    "\n",
    "exponent_grid = np.linspace(0,1,201)\n",
    "for n in tqdm(names):\n",
    "    values = np.array(datasets[n].value)\n",
    "    sigmas = np.array(datasets[n].uncertainty)\n",
    "    has_sigma = ~np.isnan(sigmas)\n",
    "    values = values[has_sigma]\n",
    "    sigmas = sigmas[has_sigma]\n",
    "    if len(values) < 2:\n",
    "        continue\n",
    "    \n",
    "    truth = truths[n][0] if hasattr(truths[n], '__iter__') else truths[n]\n",
    "    # scaler = truth\n",
    "    # values = values/scaler\n",
    "    # sigmas = sigmas/scaler\n",
    "    # truth = truth/scaler\n",
    "    \n",
    "\n",
    "    _, muhat_birge, _, chat = birge(values, sigmas, coverage=0.6827, truth=truth, mle=True)\n",
    "    assert np.isclose(truth, muhat_birge)\n",
    "    # brs.append(chat)\n",
    "    # mean_sigma = np.mean(sigmas)\n",
    "    # muhat_re, _, tau = random_effects_dl_base(values, sigmas)\n",
    "    # taus.append(np.mean(tau/sigmas))\n",
    "    _, muhat_re, _, tau = random_effects_mle(values, sigmas, coverage=0.6827, truth=truth)\n",
    "    assert np.isclose(truth, muhat_re)\n",
    "    # taus.append(np.mean(tau/sigmas))\n",
    "    # I2s.append(I2(values, sigmas))\n",
    "\n",
    "    # # generate values with same sigmas but no unaccounted for errors.\n",
    "    # # to be used as a control when analyzing the distribution of chat and tau\n",
    "    # values_control = np.random.normal(loc=0, scale=sigmas)\n",
    "    # _, _, _, chat_cont = birge(values_control, sigmas, coverage=0.6827)\n",
    "    # brs_cont.append(chat_cont)\n",
    "    # _, _, _, tau_cont = random_effects_mle(values_control, sigmas, coverage=0.6827)\n",
    "    # taus_cont.append(np.mean(tau_cont/sigmas))\n",
    "    # I2s_cont.append(I2(values_control, sigmas))\n",
    "\n",
    "    # # errscale_ps.append(errscale_test(values, sigmas))\n",
    "    # # errscale_ps_cont.append(errscale_test(values_control, sigmas))\n",
    "    if n in particle_keys:\n",
    "        key = particle_group_map[n]\n",
    "    else:\n",
    "        key = n\n",
    "    if key not in key_order:\n",
    "        key_order.append(key)\n",
    "    if key not in results:\n",
    "        results[key] = {}\n",
    "        results[key]['name'] = nice_names[key]\n",
    "        results[key]['ds_count'] = 0\n",
    "        results[key]['count'] = 0\n",
    "        for r in ['birge_loglike', 're_loglike', 'fe_loglike']:\n",
    "            results[key][r] = 0\n",
    "        results[key]['exponent_loglike'] = np.zeros_like(exponent_grid)\n",
    "        results[key]['exponent_loglike_iter'] = np.zeros_like(exponent_grid)\n",
    "    results[key]['ds_count'] += 1\n",
    "    results[key]['count'] += len(values)\n",
    "    # print(norm.pdf(values, loc=muhat_birge, scale=sigmas*chat))\n",
    "    results[key]['birge_loglike'] += np.sum(norm.logpdf(values, loc=muhat_birge, scale=sigmas*chat))\n",
    "    results[key]['re_loglike'] += np.sum(norm.logpdf(values, loc=muhat_re, scale=np.sqrt(sigmas**2+tau**2)))\n",
    "    results[key]['fe_loglike'] += np.sum(norm.logpdf(values, loc=muhat_birge, scale=sigmas))\n",
    "\n",
    "    # def to_minimize(params):\n",
    "    #     tau, exponent = params\n",
    "    #     return -np.sum(norm.logpdf(values, loc=truth, scale=np.sqrt(sigmas**2+(tau**2)*(sigmas**(2*exponent)))))\n",
    "    # res = minimize(to_minimize, x0=(0,0), bounds=[(0,max(sigmas)*100),(0,4)])\n",
    "    def to_minimize(params, exponent):\n",
    "        tau = params[0]\n",
    "        return -np.sum(norm.logpdf(values, loc=truth, scale=np.sqrt(sigmas**2+(tau**2)*(sigmas**(2*exponent)))))\n",
    "    for i,exponent in enumerate(exponent_grid):\n",
    "        res = minimize(to_minimize, x0=(0,), bounds=[(0,max(sigmas)*100)], args=(exponent,), method='Nelder-Mead')\n",
    "        if not res.success:\n",
    "            print(res)\n",
    "            raise(ValueError())\n",
    "        results[key]['exponent_loglike_iter'][i] += -res.fun\n",
    "\n",
    "    #     _, muhat_re, _, tau = random_effects_mle(values, sigmas, coverage=0.6827, truth=truth, exponent=exponent)\n",
    "    #     loglike = np.sum(norm.logpdf(values, loc=muhat_re, scale=np.sqrt(sigmas**2+(tau**2)*sigmas**exponent)))\n",
    "    #     results[key]['exponent_loglike_iter'][i] += loglike\n",
    "    tau_grid = np.concatenate((np.logspace(-4,4,1000)*max(sigmas), np.logspace(-4,4,1000)))\n",
    "    for i,exponent in enumerate(exponent_grid):\n",
    "        scale = np.sqrt(sigmas[:,np.newaxis]**2 + ((tau_grid)**2) * (sigmas[:,np.newaxis]**(2*exponent)))\n",
    "        loglike = norm.logpdf(values[:,np.newaxis], loc=muhat_re, scale=scale)\n",
    "        results[key]['exponent_loglike'][i] += np.max(np.sum(loglike, axis=0))\n",
    "\n",
    "\n",
    "\n",
    "    results[key]['category'] = category_map[n]\n",
    "\n",
    "for k,res in list(results.items()):\n",
    "    category = res['category']\n",
    "    if category not in results:\n",
    "        # dict with default value of zero\n",
    "        results[category] = defaultdict(float)\n",
    "    if 'total' not in results:\n",
    "        results['total'] = defaultdict(float)\n",
    "    for r in ['ds_count', 'count', 'birge_loglike', 're_loglike', 'fe_loglike', 'exponent_loglike', 'exponent_loglike_iter']:\n",
    "        results[category][r] += res[r]\n",
    "        results['total'][r] += res[r]\n",
    "\n",
    "for res in results.values():\n",
    "    res['ds_count'] = fmt_result(res['ds_count'])\n",
    "    res['count'] = fmt_result(res['count'])\n",
    "    if len(np.unique(res['exponent_loglike'])) == 1:\n",
    "        print('all same loglike')\n",
    "        res['best_exponent'] = 'N/A'\n",
    "    else:\n",
    "        best_exponent = exponent_grid[np.argmax(res['exponent_loglike'])]\n",
    "        res['best_exponent'] = fmt_result(best_exponent)\n",
    "        logratio = 2 * (np.max(res['exponent_loglike']) - res['exponent_loglike'])\n",
    "        within = logratio < chi2.ppf(0.95, df=1)\n",
    "        idx_l = np.argmax(within)\n",
    "        idx_u = -np.argmax(within[::-1]) - 1\n",
    "        low = exponent_grid[idx_l]\n",
    "        high = exponent_grid[idx_u]\n",
    "        res['exponent_interval'] = f'$[{np.round(low,2)}, {np.round(high,2)}]$'\n",
    "        \n",
    "        \n",
    "\n",
    "    loglike_keys = ['birge_loglike', 're_loglike', 'fe_loglike']\n",
    "    loglike_values = [res[k] for k in loglike_keys]\n",
    "    largest_idx = np.argmax(loglike_values)\n",
    "    if len(np.unique(loglike_values)) == 1:\n",
    "        largest_idx = np.nan\n",
    "    for i, r in enumerate(loglike_keys):\n",
    "        res[r] = fmt_result(res[r], bold=(i==largest_idx))\n",
    "\n",
    "results['total']['name'] = 'Total'\n",
    "results['historical']['name'] = 'Total (historical)'\n",
    "results['chemistry']['name'] = 'Total (chemistry)'\n",
    "results['particle']['name'] = 'Total (particle)'\n",
    "print(results)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "del results_df['category']\n",
    "del results_df['exponent_loglike']\n",
    "del results_df['exponent_loglike_iter']\n",
    "# results_df.index = [r['name'] for r in results.values()]\n",
    "# sort to be: chemistry, particle, total chemistry, total particle, total\n",
    "row_order = key_order + ['historical','chemistry', 'particle', 'total']\n",
    "results_df = results_df.loc[row_order]\n",
    "# print(results_df)\n",
    "\n",
    "# use np to save latex table with & separator\n",
    "rows = results_df.values.tolist()\n",
    "hline_idxs = [2, 2+len(chemistry_keys), -4, -4, -1]\n",
    "for i in hline_idxs:\n",
    "    rows[i][0] = r'\\hline ' + rows[i][0]\n",
    "print(rows)\n",
    "txt = ' \\\\\\\\\\n'.join([' & '.join(row) for row in rows])\n",
    "print(txt)\n",
    "with open('tables/hist-syst.tex', 'w') as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the empirical probability of the next observation being on the same side of the ground truth as the current one.\n",
    "n_pairs = np.zeros(1000, dtype=int)\n",
    "same_side = np.zeros((1000,3), dtype=int)\n",
    "for i in range(1000):\n",
    "    idxs = np.random.choice(len(particle_keys), size=40, replace=False)\n",
    "    for idx in idxs:\n",
    "        p = particle_keys[idx]\n",
    "        truth = truths[p][0]\n",
    "        truth_l = truths[p][0]-1*truths[p][1]\n",
    "        truth_u = truths[p][0]+1*truths[p][1]\n",
    "        df = datasets[p].sort_index()\n",
    "        sort_idx = np.argsort(datasets[p].year+np.random.rand(len(df))*0.1)\n",
    "        values = np.array(datasets[p]['value'])[sort_idx]\n",
    "        \n",
    "        overs = np.array([values>truth, values>truth_l, values>truth_u])\n",
    "        # over = values > truth\n",
    "        xors = np.array([np.bitwise_xor(over[:-1], over[1:]) for over in overs])\n",
    "        n_pairs[i] += len(xors[0])\n",
    "        same_sides = np.sum(~xors, axis=1)\n",
    "        # print(same_sides)\n",
    "        same_side[i,0] += same_sides[0]\n",
    "        same_side[i,1] += min(same_sides)\n",
    "        same_side[i,2] += max(same_sides)\n",
    "        # same_side[0] += np.sum(~xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(same_side)\n",
    "# print(n_pairs)\n",
    "from scipy.stats import binom\n",
    "p_mid = 1-binom.cdf(k=same_side[:,0], n=n_pairs, p=0.5)\n",
    "p_generous = 1-binom.cdf(k=same_side[:,1], n=n_pairs, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest\n",
    "intervals_mid = np.array([list(binomtest(k=same_side[i,0], n=n_pairs[i], p=0.5, alternative='two-sided').proportion_ci(confidence_level=0.90, method='exact')) for i in range(same_side.shape[0])])\n",
    "intervals_generous = np.array([list(binomtest(k=same_side[i,1], n=n_pairs[i], p=0.5, alternative='two-sided').proportion_ci(confidence_level=0.90, method='exact')) for i in range(same_side.shape[0])])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1,2, figsize=(8,3))\n",
    "\n",
    "axs[0].hist(p_mid, bins=np.linspace(0,0.1,21), histtype='step', color='grey', density=True, linewidth=2, label=r'$\\theta$ as truth')\n",
    "axs[0].hist(p_generous, bins=np.linspace(0,0.1,21), histtype='step', color='black', density=True, linewidth=2, label=r'$\\theta$, $\\theta-\\sigma$, or $\\theta+\\sigma$ as truth')\n",
    "axs[0].set_xlabel('$p$-value on $H_0$: $p=0.5$, $H_1$: $p>0.5$')\n",
    "axs[0].set_ylabel('Density')\n",
    "axs[0].set_xlim(0,0.1)\n",
    "axs[0].legend(frameon=False)\n",
    "\n",
    "# histbins = np.linspace(0.45, 0.8, 41)\n",
    "# axs[1].hist(intervals_mid[:,0], bins=histbins, histtype='step', color='grey', density=True, linewidth=2)\n",
    "# axs[1].hist(intervals_mid[:,1], bins=histbins, histtype='step', color='grey', density=True, linewidth=2)\n",
    "\n",
    "colors = ['grey', 'black']\n",
    "for i, intervals in enumerate([intervals_mid, intervals_generous]):\n",
    "\n",
    "    midpoints = np.median(intervals, axis=0)\n",
    "    uppers = np.quantile(intervals, 0.75, axis=0)\n",
    "    lowers = np.quantile(intervals, 0.25, axis=0)\n",
    "    middle = np.mean(midpoints)\n",
    "    xerr0 = [middle-uppers[0], lowers[1]-middle]\n",
    "    xerr1 = [middle-midpoints[0], midpoints[1]-middle]\n",
    "    xerr2 = [middle-lowers[0], uppers[1]-middle]\n",
    "\n",
    "    axs[1].errorbar([middle], [i], xerr=np.array([xerr0]).T, fmt='o', color=colors[i], linewidth=2, capsize=4, markersize=0)\n",
    "    axs[1].errorbar([middle], [i], xerr=np.array([xerr1]).T, fmt='o', color=colors[i], linewidth=2, capsize=4, markersize=0)\n",
    "    axs[1].errorbar([middle], [i], xerr=np.array([xerr2]).T, fmt='o', color=colors[i], linewidth=2, capsize=4, markersize=0)\n",
    "\n",
    "axs[1].axvline(0.5, color='red', linestyle='--')\n",
    "axs[1].set_ylim(-0.5, 1.5)\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "axs[1].set_xlabel(r'90\\% confidence intervals for $p$')\n",
    "axs[1].set_xlim(0.45, 0.7)\n",
    "plt.suptitle(r'Tests on $p=P(y_i,y_{i+1}\\,$on same side of truth)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/pdg_indep.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = truths['c']\n",
    "values = np.array(datasets['c']['value'])\n",
    "over = values > truth\n",
    "xor = np.bitwise_xor(over[:-1], over[1:])\n",
    "n_pairs = len(xor)\n",
    "same_side = np.sum(~xor)\n",
    "print(n_pairs, same_side, binom.cdf(k=same_side, n=n_pairs, p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "ns = [len(datasets[n]) for n in particle_keys]\n",
    "ks_mid = [np.sum(datasets[n]['value'] > truths[n][0]) for n in particle_keys]\n",
    "ks_generous = []\n",
    "for p in particle_keys:\n",
    "    n = len(datasets[p])\n",
    "    truth = truths[p][0]\n",
    "    sigma = truths[p][1]\n",
    "    contenders = [truth, truth-sigma, truth+sigma]\n",
    "    k_contenders = np.array([np.sum(datasets[p]['value'] > contender) for contender in contenders])\n",
    "    closest_to_half = np.argmin(np.abs(k_contenders/n - 0.5))\n",
    "    ks_generous.append(k_contenders[closest_to_half])\n",
    "rhos = []\n",
    "for ks in [ks_mid, ks_generous]:\n",
    "    with pm.Model() as model:\n",
    "        rho = pm.Uniform('rho', lower=0, upper=1)\n",
    "        alphabeta = (1/rho)-1\n",
    "        alpha = alphabeta/2\n",
    "        beta = alphabeta/2\n",
    "        k = pm.BetaBinomial('k', alpha=alpha, beta=beta, n=ns, observed=ks)\n",
    "        trace = pm.sample(20000, tune=1000)\n",
    "    rhos.append(trace.posterior['rho'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,2))\n",
    "bins = np.linspace(0, 1, 401)\n",
    "plt.hist(rhos[0], color='grey', density=True, bins=bins, histtype='step', linewidth=1, label=r'$\\theta$ as truth')\n",
    "plt.hist(rhos[1], color='black', density=True, bins=bins, histtype='step', linewidth=1, label=r'$\\theta$, $\\theta-\\sigma$, or $\\theta+\\sigma$ as truth')\n",
    "plt.xlim(0, 0.5)\n",
    "plt.xlabel(r'$\\rho$')\n",
    "plt.ylabel('Posterior density')\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig('figs/pdg_rho.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate cdfs for correlated binomial\n",
    "\n",
    "binom_corr_cdfs = []\n",
    "p = 0.55\n",
    "for n in range(2, 11):\n",
    "    binom_corr_pmf = np.zeros(n+1)\n",
    "    for i in range(100000):\n",
    "        same_side = np.random.choice([-1, 1], size=n-1, p=[1-p, p])\n",
    "        initial = np.random.choice([-1, 1])\n",
    "        seq = np.concatenate(([initial], np.cumprod(same_side)*initial))\n",
    "        seq = (seq + 1)//2\n",
    "        binom_corr_pmf[np.sum(seq)] += 1\n",
    "    binom_corr_pmf /= np.sum(binom_corr_pmf)\n",
    "    binom_corr_cdfs.append(np.cumsum(binom_corr_pmf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_corr_cdfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(binom_corr_cdfs[2])\n",
    "plt.plot(binom.cdf(np.arange(5), n=4, p=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with the particle dataset\n",
    "from methods import birge, random_effects_hksj, binomial_method, random_effects_dl, vniim, sign_rank, flip_interval, fixed_effect, linear_pool, birge_forecast, boot, random_effects_mle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from math import comb\n",
    "from itertools import combinations\n",
    "from scipy.stats import norm, betabinom\n",
    "\n",
    "# birge_covs = defaultdict(lambda: defaultdict(list))\n",
    "# re_covs = defaultdict(lambda: defaultdict(list))\n",
    "binom_target_covs = {}\n",
    "signrank_target_covs = {}\n",
    "target_covs = {}\n",
    "binom_corr_target_covs = {}\n",
    "# binom_covs = defaultdict(lambda: defaultdict(list))\n",
    "# method_covs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "n_combs = 400\n",
    "methods = [\n",
    "    'fe',\n",
    "    'birge',\n",
    "    'pdg',\n",
    "    'codata',\n",
    "    're_hksj',\n",
    "    're_dl',\n",
    "    're_mle',\n",
    "    # 'vniim',\n",
    "    'binom',\n",
    "    'signrank',\n",
    "    # 'boot-normal',\n",
    "    # 'boot-student',\n",
    "    'binom-corr',\n",
    "    # 'birge-forecast'\n",
    "    # 'flip',\n",
    "    'lp'\n",
    "]\n",
    "ns = np.arange(2, 11)\n",
    "method_covs = np.full((len(ns), len(methods), len(particle_keys), n_combs), np.nan)\n",
    "method_widths = np.full((len(ns), len(methods), len(particle_keys), n_combs), np.nan)\n",
    "# signrank_covs = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# target_cov = 1-2*binom_tail\n",
    "# print(target_cov)\n",
    "# target_cov = 0.75\n",
    "# tail_prob = (1-target_cov)/2\n",
    "\n",
    "def add_result(method_covs, n, method, name, interval, truth_min, truth_max):\n",
    "    method_covs[n][method][name].append((interval[0]<=truth_max) & (truth_min<=interval[1]))\n",
    "\n",
    "def shrink_interval(interval, z_actual, z_target):\n",
    "    width = interval[1]-interval[0]\n",
    "    middle = (interval[0]+interval[1])/2\n",
    "    new_width = width*z_target/z_actual\n",
    "    return (middle-new_width/2, middle+new_width/2)\n",
    "\n",
    "# ns = [6]\n",
    "for i, n in enumerate(ns):\n",
    "    base_target_cov = 0.6827\n",
    "    if n > 2:\n",
    "        _, binom_target_cov, _ = binomial_method(np.sort(np.arange(n)), coverage=base_target_cov)\n",
    "        binom_target_covs[n] = binom_target_cov\n",
    "        _, signrank_target_cov = sign_rank(np.sort(np.arange(n)), coverage=base_target_cov)\n",
    "        signrank_target_covs[n] = signrank_target_cov\n",
    "        # z_signrank = -norm.ppf((1-signrank_target_cov)/2)\n",
    "        # _, binom_corr_target_cov = binomial_method(np.sort(np.arange(n)), cdf=binom_corr_cdfs[i], coverage=base_target_cov)\n",
    "        # binom_corr_target_covs[n] = binom_corr_target_cov\n",
    "        beta_binom_cdf = betabinom.cdf(np.arange(n+1), n, a=4.5, b=4.5)\n",
    "        _, binom_corr_target_cov, _ = binomial_method(np.sort(np.arange(n)), cdf=beta_binom_cdf, coverage=0.65)\n",
    "        binom_corr_target_covs[n] = binom_corr_target_cov\n",
    "        \n",
    "        target_cov = base_target_cov\n",
    "        \n",
    "        z_binom = -norm.ppf((1-binom_target_cov)/2)\n",
    "        z_binom_corr = -norm.ppf((1-binom_corr_target_cov)/2)\n",
    "\n",
    "    else:\n",
    "        target_cov = base_target_cov\n",
    "    z_target_cov = -norm.ppf((1-target_cov)/2)\n",
    "    target_covs[n] = target_cov\n",
    "\n",
    "    for j, name in tqdm(enumerate(particle_keys)):\n",
    "        df = datasets[name]\n",
    "        if len(df)<n:\n",
    "            continue\n",
    "        value = np.array(df.value)\n",
    "        sigma = np.array(df.uncertainty)\n",
    "        truth_min = truths[name][0]-3*truths[name][1]\n",
    "        truth_max = truths[name][0]+3*truths[name][1]\n",
    "\n",
    "        # if the number of combinations of n results is small, we use all combinations,\n",
    "        # otherwise we randomly sample\n",
    "        if comb(len(value), n) <= n_combs:\n",
    "            subset_idxs = [np.array(c) for c in combinations(range(len(value)), n)]\n",
    "        else:\n",
    "            subset_idxs = np.array([np.random.choice(len(value), size=n, replace=False) for _ in range(n_combs)])\n",
    "        for k, subset_idx in enumerate(subset_idxs):\n",
    "            value_s = value[subset_idx]\n",
    "            sigma_s = sigma[subset_idx]\n",
    "            intervals = {}\n",
    "            intervals['birge'], wm, _, _ = birge(value_s, sigma_s, coverage=target_cov)\n",
    "            intervals['pdg'], wm, _, _ = birge(value_s, sigma_s, coverage=target_cov, pdg=True)\n",
    "            intervals['codata'], wm, _, _ = birge(value_s, sigma_s, coverage=target_cov, codata=True)\n",
    "            intervals['re_hksj'], muhat, _, _ = random_effects_hksj(value_s, sigma_s, coverage=target_cov)\n",
    "            intervals['re_dl'], muhat, _, _ = random_effects_dl(value_s, sigma_s, coverage=target_cov)\n",
    "            intervals['re_mle'], muhat, _, _ = random_effects_mle(value_s, sigma_s, coverage=target_cov)\n",
    "            # intervals['boot-normal'] = boot(value_s, sigma_s, coverage=target_cov, which='normal')\n",
    "            # intervals['boot-student'] = boot(value_s, sigma_s, coverage=target_cov, which='studentized')\n",
    "            # intervals['birge-forecast'], _, _, _, _ = birge_forecast(value_s, sigma_s, coverage=target_cov)\n",
    "            # intervals['vniim'], wm = vniim(value_s, sigma_s, coverage=target_cov)\n",
    "            if n > 2:\n",
    "                interval, _, interval_shrink = binomial_method(value_s, p=0.5, coverage=target_cov, shrink='cdf-interp')\n",
    "                intervals['binom'] = interval # shrink_interval(interval, z_binom, z_target_cov)\n",
    "                interval, _, interval_shrink = binomial_method(value_s, cdf=beta_binom_cdf, coverage=0.65, shrink='center')\n",
    "                intervals['binom-corr'] = interval # shrink_interval(interval, z_binom_corr, z_target_cov)\n",
    "                interval, _ = sign_rank(value_s, coverage=target_cov)\n",
    "                intervals['signrank'] = interval # shrink_interval(interval, z_signrank, z_target_cov)\n",
    "            # intervals['flip'], _ = flip_interval(value_s, coverage=target_cov, max_iter=100, boot=False)\n",
    "            intervals['fe'], _, _ = fixed_effect(value_s, sigma_s, coverage=target_cov)\n",
    "            intervals['lp'], _ = linear_pool(value_s, sigma_s, coverage=target_cov, gridn=1000)\n",
    "\n",
    "            for l, method in enumerate(methods):\n",
    "                if method not in intervals:\n",
    "                    continue\n",
    "                interval = intervals[method]\n",
    "                method_covs[i][l][j][k] = (interval[0]<=truth_max) & (truth_min<=interval[1])\n",
    "                method_widths[i][l][j][k] = interval[1]-interval[0]\n",
    "method_covs = np.nanmean(method_covs, axis=3)\n",
    "method_widths = np.nanmean(method_widths, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_covs.shape)\n",
    "n_datasets = np.sum(~np.isnan(method_covs[:,0,:]), axis=1)\n",
    "print(n_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'binom': 'orange',\n",
    "    'binom-corr': 'goldenrod',\n",
    "    'signrank': 'red',\n",
    "    'flip': 'yellow',\n",
    "    'fe': 'grey',\n",
    "    'lp': 'lightgrey',\n",
    "    're_hksj': 'green',\n",
    "    're_dl': 'lightgreen',\n",
    "    'vniim': 'lightblue',\n",
    "    'birge': 'blue',\n",
    "    'pdg': 'lightblue',\n",
    "    'codata': 'plum',\n",
    "    'boot-normal': 'brown',\n",
    "    'boot-student': 'tan'\n",
    "    # 'birge-forecast': 'pink'\n",
    "}\n",
    "\n",
    "method_widths_rel = method_widths/method_widths[:,0,np.newaxis,:]\n",
    "\n",
    "# bootstrap over datasets\n",
    "B = 400\n",
    "method_covs_boot = np.full((len(ns), len(methods), len(particle_keys), B), np.nan)\n",
    "for b in range(B):\n",
    "    dataset_idxs = np.random.choice(len(particle_keys), size=len(particle_keys), replace=True)\n",
    "    method_covs_boot[:,:,:,b] = method_covs[:,:,dataset_idxs]\n",
    "method_covs_mean = np.nanmean(method_covs, axis=2)\n",
    "method_covs_mean_rel = method_covs_mean/method_covs_mean[:,methods.index('signrank'),np.newaxis]\n",
    "method_covs_mean_boot = np.nanmean(method_covs_boot, axis=2)\n",
    "method_covs_mean_boot_rel = method_covs_mean_boot/method_covs_mean_boot[:,methods.index('signrank'),np.newaxis,:]\n",
    "\n",
    "uppers_rel = 2*method_covs_mean_rel - np.nanquantile(method_covs_mean_boot_rel, 0.1, axis=2)\n",
    "lowers_rel = 2*method_covs_mean_rel - np.nanquantile(method_covs_mean_boot_rel, 0.9, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_covs_mean_rel[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hspace = 0\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8,7), sharex=True, gridspec_kw={'hspace': 0})\n",
    "for i, method in enumerate(methods):\n",
    "    axs[0].plot(ns, np.nanmean(method_covs[:,i,:], axis=1), label=method, color=colors[method])\n",
    "    axs[1].plot(ns, np.nanmean(method_widths_rel[:,i,:], axis=1), label=method, color=colors[method])\n",
    "    axs[2].plot(ns, method_covs_mean_rel[:,i], label=method, color=colors[method])\n",
    "    axs[2].plot(ns, uppers_rel[:,i], color=colors[method], linestyle='--', linewidth=0.5)\n",
    "    axs[2].plot(ns, lowers_rel[:,i], color=colors[method], linestyle='--', linewidth=0.5)\n",
    "# axs[0].plot(signrank_target_covs.keys(), signrank_target_covs.values(), label='target (signrank)', color='grey', linestyle='--')\n",
    "# axs[0].plot(binom_target_covs.keys(), binom_target_covs.values(), label='target (binom)', color='grey', linestyle=':')\n",
    "# axs[0].plot(binom_corr_target_covs.keys(), binom_corr_target_covs.values(), label='target (binom-corr)', color='grey', linestyle='--')\n",
    "axs[0].plot(target_covs.keys(), target_covs.values(), label='target', color='black', linestyle='--', linewidth=2)\n",
    "axs[0].set_ylabel('Coverage')\n",
    "# axs[0].legend()\n",
    "axs[1].set_ylabel('Width relative to fixed effect')\n",
    "axs[2].legend(frameon=False)\n",
    "\n",
    "axs[2].set_xlabel('Number of results')\n",
    "axs[2].set_ylabel('Coverage relative to signrank')\n",
    "\n",
    "# point ticks inwards and add top and right ticks\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction='in', top=True, right=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8,3.5), sharex=True, gridspec_kw={'hspace': 0})\n",
    "linewidth = 1.5\n",
    "\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('re_dl'),:], axis=1), label='Random Effects (DL)', color='black', linewidth=linewidth)\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('re_hksj'),:], axis=1), label='Random Effects (HKSJ)', color='black', linewidth=linewidth, linestyle=':')\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('re_mle'),:], axis=1), label='Random Effects (MLE)', color='black', linewidth=linewidth, linestyle='--')\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('fe'),:], axis=1), label='Fixed Effect', color='grey', linewidth=linewidth)\n",
    "\n",
    "axs[0].set_ylabel('Coverage')\n",
    "axs[0].plot(target_covs.keys(), target_covs.values(), color='black', linestyle='-', linewidth=1)\n",
    "axs[0].text(9, list(target_covs.values())[-1]*0.98, r'$\\uparrow$ Target coverage', va='top', ha='center')\n",
    "axs[0].text(8, 0.56, r'Achieved coverage', va='bottom', ha='center')\n",
    "\n",
    "\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('re_dl'),:], axis=1), label='Random Effects (DL)', color='black', linewidth=linewidth)\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('re_hksj'),:], axis=1), label='Random Effects (HKSJ)', color='black', linewidth=linewidth, linestyle=':')\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('re_mle'),:], axis=1), label='Random Effects (MLE)', color='black', linewidth=linewidth, linestyle='--')\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('fe'),:], axis=1), label='Fixed Effect', color='grey', linewidth=linewidth)\n",
    "\n",
    "plt.xlabel('Number of results $n$')\n",
    "axs[1].set_ylabel(r'Average width\\\\relative to Fixed Effect')\n",
    "\n",
    "axs[0].legend(frameon=False, ncol=4, prop={'size': 8}, bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "              loc='lower left',mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.xlim(2, 10)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction='in', top=True, right=True)\n",
    "plt.savefig('figs/pdg1970_cov_re.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8,3.5), sharex=True, gridspec_kw={'hspace': 0})\n",
    "linewidth = 1.5\n",
    "\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('birge'),:], axis=1), label='Birge Ratio', color='black', linewidth=linewidth)\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('pdg'),:], axis=1), label='PDG', color='black', linewidth=linewidth, linestyle=':')\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('codata'),:], axis=1), label='CODATA', color='black', linewidth=linewidth, linestyle='--')\n",
    "axs[0].plot(ns, np.nanmean(method_covs[:,methods.index('fe'),:], axis=1), label='Fixed Effect', color='grey', linewidth=linewidth)\n",
    "\n",
    "axs[0].set_ylabel('Coverage')\n",
    "axs[0].plot(target_covs.keys(), target_covs.values(), color='black', linestyle='-', linewidth=1)\n",
    "axs[0].text(9, list(target_covs.values())[-1]*0.98, r'$\\uparrow$ Target coverage', va='top', ha='center')\n",
    "axs[0].text(8, 0.49, r'Achieved coverage', va='bottom', ha='center')\n",
    "axs[0].legend(frameon=False)\n",
    "\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('birge'),:], axis=1), label='Birge Ratio', color='black', linewidth=linewidth)\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('pdg'),:], axis=1), label='PDG', color='black', linewidth=linewidth, linestyle=':')\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('codata'),:], axis=1), label='CODATA', color='black', linewidth=linewidth, linestyle='--')\n",
    "axs[1].plot(ns, np.nanmean(method_widths_rel[:,methods.index('fe'),:], axis=1), label='Fixed Effect', color='grey', linewidth=linewidth)\n",
    "\n",
    "plt.xlabel('Number of results $n$')\n",
    "axs[1].set_ylabel(r'Average width\\\\relative to Fixed Effect')\n",
    "\n",
    "axs[0].legend(frameon=False, ncol=4, prop={'size': 8}, bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "              loc='lower left',mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.xlim(2, 10)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction='in', top=True, right=True)\n",
    "plt.savefig('figs/pdg1970_cov_birge.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,2.8))\n",
    "\n",
    "plt.plot(ns, np.nanmean(method_covs[:,methods.index('binom'),:], axis=1), label='ST', color='black', linewidth=3)\n",
    "plt.plot(ns, np.nanmean(method_covs[:,methods.index('binom-corr'),:], axis=1), label=r'ST$_{\\rho=0.1}$', color='dimgrey', linewidth=2)\n",
    "plt.plot(ns, np.nanmean(method_covs[:,methods.index('signrank'),:], axis=1), label=r'SRT', color='silver', linewidth=3)\n",
    "plt.plot(binom_target_covs.keys(), binom_target_covs.values(), color='black', linestyle='--', linewidth=1.5)\n",
    "plt.plot(binom_corr_target_covs.keys(), binom_corr_target_covs.values(), color='dimgrey', linestyle='--', linewidth=1.5)\n",
    "plt.plot(signrank_target_covs.keys(), signrank_target_covs.values(), color='silver', linestyle='--', linewidth=1.5)\n",
    "plt.xlabel('Number of results $n$')\n",
    "plt.ylabel('Coverage')\n",
    "# plt.plot(target_covs.keys(), target_covs.values(), label='target', color='black', linestyle='--', linewidth=2)\n",
    "print(binom_corr_target_covs[3])\n",
    "plt.xlim(3, 10)\n",
    "plt.legend(frameon=False)\n",
    "plt.tick_params(direction='in', top=True, right=True)\n",
    "plt.savefig('figs/pdg1970_cov_nonparam.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap over datasets\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "method_covs_boot_mean = np.nanmean(method_covs_ds_boot, axis=2)\n",
    "method_covs_mean = np.nanmean(method_covs, axis=(2, 3))\n",
    "uppers = 2*method_covs_mean - np.nanquantile(method_covs_boot_mean, 0.1, axis=2)\n",
    "lowers = 2*method_covs_mean - np.nanquantile(method_covs_boot_mean, 0.9, axis=2)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.plot(ns, np.nanmean(method_covs[:,i,:,:], axis=(1,2)), label=method, color=colors[method])\n",
    "    plt.plot(ns, uppers[:,i], color=colors[method], linestyle=':', linewidth=0.5)\n",
    "    plt.plot(ns, lowers[:,i], color=colors[method], linestyle=':', linewidth=0.5)\n",
    "plt.plot(ns, target_covs, color='black', linestyle='--', linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(datasets['agi'].value)\n",
    "sigma = np.array(datasets['agi'].uncertainty)\n",
    "\n",
    "import pymc as pm\n",
    "print(y)\n",
    "n = len(y)\n",
    "width = np.max(y) - np.min(y)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    theta = pm.Normal('theta', np.mean(y), np.mean(y))\n",
    "    # scaler_rate = pm.Exponential('scaler_rate', 0.2)\n",
    "    # scalers = pm.Exponential('scalers', scaler_rate, shape=n)+1\n",
    "    # y_pred = pm.Normal('y_pred', theta, sigma*scalers, observed=y)\n",
    "\n",
    "    # adder_rate = pm.Exponential('adder_rate', 10/width)\n",
    "    adders = pm.Exponential('adders', 5/width, shape=n)\n",
    "    y_pred = pm.Normal('y_pred', theta, np.sqrt(sigma**2+adders**2), observed=y)\n",
    "\n",
    "    trace = pm.sample(draws=1000, tune=1000, chains=4, target_accept=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    plt.hist(trace.posterior['scalers'].values[:,:,i].flatten()+1, bins=100, histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.posterior['scalers'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trace.posterior['theta'].values.flatten(), bins=100)\n",
    "qs = np.quantile(trace.posterior['theta'].values.flatten(), [0.16, 0.84])\n",
    "plt.axvline(qs[0], color='red', linestyle='--', linewidth=1)\n",
    "plt.axvline(qs[1], color='red', linestyle='--', linewidth=1)\n",
    "ymax = plt.ylim()[1]\n",
    "if 'scalers' in trace.posterior:\n",
    "    scaler_qs = np.quantile(trace.posterior['scalers'].values, [0.25, 0.5, 0.75], axis=(0,1))+1\n",
    "    print(scaler_qs.shape)\n",
    "    for i in range(3):\n",
    "        plt.errorbar(y, np.linspace(ymax/len(y), ymax, len(y)), xerr=sigma*scaler_qs[i], fmt='.', markersize=2, linewidth=1, color='black', capsize=2)\n",
    "if 'adders' in trace.posterior:\n",
    "    adder_qs = np.quantile(trace.posterior['adders'].values, [0.25, 0.5, 0.75], axis=(0,1))\n",
    "    print(adder_qs.shape)\n",
    "    for i in range(3):\n",
    "        plt.errorbar(y, np.linspace(ymax/len(y), ymax, len(y)), xerr=sigma+adder_qs[i], fmt='.', markersize=2, linewidth=1, color='black', capsize=2)\n",
    "# sigmas_adjust = sigma * np.median(trace.posterior['scalers'].values, axis=(0,1))\n",
    "\n",
    "plt.errorbar(y, np.linspace(ymax/len(y), ymax, len(y)), xerr=sigma, fmt='.', markersize=2, linewidth=2, color='black')\n",
    "\n",
    "# plt.ylim(0, ymax)\n",
    "# plt.xlim(qs[0], qs[1])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
