{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use tex\n",
    "plt.rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.read_csv(\"data/c.csv\", comment=\"#\")\n",
    "c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_df = pd.read_csv(\"data/rho.csv\", comment=\"#\")\n",
    "rho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_df = pd.read_csv(\"data/au.csv\", comment=\"#\")\n",
    "au_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avogadro_df = pd.read_csv(\"data/bailey/CsvData/Constants_Data/Avogadro_Number.csv\")\n",
    "# remove last row\n",
    "avogadro_df = avogadro_df.iloc[:-1]\n",
    "avogadro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finestructure_df = pd.read_csv(\n",
    "    \"data/bailey/CsvData/Constants_Data/Fine_Structure_Constant_Inverse.csv\"\n",
    ")\n",
    "finestructure_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rydberg_df = pd.read_csv(\"data/bailey/CsvData/Constants_Data/Rydberg_Constant.csv\")\n",
    "rydberg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_bailey_df = pd.read_csv(\"data/bailey/CsvData/Constants_Data/Speed_of_Light.csv\")\n",
    "c_bailey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"rho\": rho_df,\n",
    "    \"c\": c_df,\n",
    "    \"au\": au_df,\n",
    "    \"avogadro\": avogadro_df,\n",
    "    \"finestructure\": finestructure_df,\n",
    "    \"rydberg\": rydberg_df,\n",
    "    # 'c_bailey': c_bailey_df,\n",
    "}\n",
    "truths = {\n",
    "    \"rho\": 5.513,\n",
    "    \"c\": 299792.458,\n",
    "    \"au\": 149597870700,\n",
    "    \"avogadro\": 6.02214076e23,\n",
    "    \"finestructure\": [137.035999177, 0.000000021],\n",
    "    \"rydberg\": [10973731.568157, 0.000012],\n",
    "    \"c_bailey\": 299792.458,\n",
    "}\n",
    "yscales = {\n",
    "    \"rho\": \"symlog\",\n",
    "    \"c\": \"symlog\",\n",
    "    \"au\": \"symlog\",\n",
    "    \"avogadro\": \"symlog\",\n",
    "    \"finestructure\": \"symlog\",\n",
    "    \"rydberg\": \"symlog\",\n",
    "    # 'c_bailey': 'symlog',\n",
    "}\n",
    "linthresh = {\n",
    "    \"rho\": 0.01,\n",
    "    \"c\": 0.1,\n",
    "    \"au\": 1,\n",
    "    \"avogadro\": 1e17,\n",
    "    \"finestructure\": 1e-6,\n",
    "    \"rydberg\": 1e-4,\n",
    "    # 'c_bailey': 0.1,\n",
    "}\n",
    "\n",
    "nice_names = {\n",
    "    \"c\": \"Speed of light\",\n",
    "    \"rho\": \"Density of Earth\",\n",
    "    \"au\": \"Astronomical Unit\",\n",
    "    \"avogadro\": \"Avogadro Number\",\n",
    "    \"finestructure\": \"Inverse Fine Structure Constant\",\n",
    "    \"rydberg\": \"Rydberg Constant\",\n",
    "    # 'c_bailey': 'Speed of light (bailey)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(8, 8), sharey=True, gridspec_kw={\"wspace\": 0.05})\n",
    "xlabels = {\n",
    "    \"rho\": r\"Difference from true value $[\\mathrm{g/cm^3}]$\",\n",
    "    \"c\": r\"Difference from true value $[\\mathrm{km/s}]$\",\n",
    "    \"au\": r\"Difference from true value $[\\mathrm{km}]$\",\n",
    "    \"avogadro\": r\"Difference from true value $[\\mathrm{mol^{-1}}]$\",\n",
    "    \"finestructure\": r\"Difference from true value\",\n",
    "    \"rydberg\": r\"Difference from true value $[\\mathrm{m^{-1}}]$\",\n",
    "    # 'c_bailey': r'Difference from true value $[\\mathrm{km/s}]$',\n",
    "}\n",
    "historical_keys = [\n",
    "    \"rho\",\n",
    "    \"c\",\n",
    "    \"au\",\n",
    "    \"avogadro\",\n",
    "    \"finestructure\",\n",
    "    \"rydberg\",\n",
    "]  # , 'c_bailey']\n",
    "for i, name in enumerate(historical_keys):\n",
    "    ax = axs.flatten()[i]\n",
    "    if hasattr(truths[name], \"__iter__\"):\n",
    "        truth = truths[name][0]\n",
    "    else:\n",
    "        truth = truths[name]\n",
    "\n",
    "    values = datasets[name].value - truth\n",
    "    if name == \"au\":\n",
    "        values = values / 1000\n",
    "    dates = datasets[name].year\n",
    "\n",
    "    ax.plot(values, dates, \".\", color=\"black\")\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    # reverse y axis\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    if yscales[name] == \"symlog\":\n",
    "        print(name)\n",
    "        ax.set_xscale(\"symlog\", linthresh=linthresh[name])\n",
    "        # skip every other tick\n",
    "        n_ticklabels = len(ax.xaxis.get_ticklabels())\n",
    "        for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "            if n % 2 != 0 and label.get_text() != \"$\\\\mathdefault{0}$\":\n",
    "                label.set_visible(False)\n",
    "        ax.tick_params(axis=\"both\", which=\"both\", direction=\"in\", top=True, right=True)\n",
    "    ax.set_xlabel(xlabels[name])\n",
    "    ax.set_ylim(2015, 1650)\n",
    "    ax.set_title(nice_names[name])\n",
    "    # make top x limit and bottom x limit equal\n",
    "    xlim = max(abs(ax.get_xlim()[0]), abs(ax.get_xlim()[1]))\n",
    "    ax.set_xlim(-xlim, xlim)\n",
    "# axs[0].set_ylabel('Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/historical.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get error on ratio calculations\n",
    "def clarke_ratio(A, a, B, b, C=100, c=0):\n",
    "    # if A:B is the ratio, we want x where A:B = C:x\n",
    "    value = (B / A) * C\n",
    "    uncertainty = np.sqrt((B * C * a / A) ** 2 + (C * b) ** 2 + (B * c) ** 2) / A\n",
    "    return value, uncertainty\n",
    "\n",
    "\n",
    "def clarke_quotient(A, a, B, b):\n",
    "    # get the ratio B:A and its error\n",
    "    value = B / A\n",
    "    uncertainity = np.sqrt((B * a / A) ** 2 + b**2) / A\n",
    "    return value, uncertainity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_df = pd.read_csv(\"data/clarke/H-O-mass.csv\", comment=\"#\")\n",
    "ho_df[\"uncertainty\"] = ho_df[\"proberr\"] / 0.6745\n",
    "agcl_df = pd.read_csv(\"data/clarke/Ag-Cl-mass.csv\", comment=\"#\")\n",
    "agcl_df[\"uncertainty\"] = agcl_df[\"proberr\"] / 0.6745\n",
    "agi_df = pd.read_csv(\"data/clarke/Ag-I-mass.csv\", comment=\"#\")\n",
    "agi_df[\"uncertainty\"] = agi_df[\"proberr\"] / 0.6745\n",
    "agbr_df = pd.read_csv(\"data/clarke/Ag-Br-mass.csv\", comment=\"#\")\n",
    "agbr_df[\"uncertainty\"] = agbr_df[\"proberr\"] / 0.6745\n",
    "no_df = pd.read_csv(\"data/clarke/N-mass.csv\", comment=\"#\")\n",
    "no_df[\"uncertainty\"] = no_df[\"proberr\"] / 0.6745\n",
    "co_df = pd.read_csv(\"data/clarke/C-mass.csv\", comment=\"#\")\n",
    "co_df[\"uncertainty\"] = co_df[\"proberr\"] / 0.6745\n",
    "\n",
    "datasets[\"ho\"] = ho_df\n",
    "datasets[\"agcl\"] = agcl_df\n",
    "datasets[\"agi\"] = agi_df\n",
    "datasets[\"agbr\"] = agbr_df\n",
    "# datasets['no'] = no_df\n",
    "# datasets['co'] = co_df\n",
    "\n",
    "truths[\"agcl\"] = clarke_ratio(107.8682, 0.0002, 35.453, 0.004, 100, 0)\n",
    "truths[\"agbr\"] = clarke_ratio(107.8682, 0.0002, 79.904, 0.003, 100, 0)\n",
    "truths[\"agi\"] = clarke_ratio(107.8682, 0.0002, 126.90447, 0.00003, 100, 0)\n",
    "truths[\"ho\"] = clarke_quotient(1.0080, 0.0002, 15.9995, 0.0005)\n",
    "# truths['no'] = clarke_ratio(15.9995, 0.0005, 14.007, 0.001, 16, 0)\n",
    "# truths['co'] = clarke_ratio(15.9995, 0.0005, 12.011, 0.002, 16, 0)\n",
    "\n",
    "nice_names[\"ho\"] = \"O:H mass ratio\"\n",
    "nice_names[\"agcl\"] = \"Ag:Cl mass ratio\"\n",
    "nice_names[\"agi\"] = \"Ag:I mass ratio\"\n",
    "nice_names[\"agbr\"] = \"Ag:Br mass ratio\"\n",
    "# nice_names['no'] = 'N mass (O=16)'\n",
    "# nice_names['co'] = 'C mass (O=16)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(8, 3))\n",
    "from methods import birge, random_effects_hksj, binomial_method\n",
    "\n",
    "plot_est = False\n",
    "chemistry_keys = [\"ho\", \"agcl\", \"agi\", \"agbr\"]\n",
    "\n",
    "for i, name in enumerate(chemistry_keys):\n",
    "    ax = axs.flatten()[i]\n",
    "    # values = np.array(datasets[name].value - truths[name])\n",
    "    truth = truths[name][0]\n",
    "    err = truths[name][1]\n",
    "    ax.axvspan(truth - err, truth + err, color=\"black\", alpha=0.3)\n",
    "\n",
    "    values = np.array(datasets[name].value)\n",
    "    errs = np.array(datasets[name].uncertainty)\n",
    "    # sort by decreasing error\n",
    "    sort_idx = np.argsort(errs)[::-1]\n",
    "    values = values[sort_idx]\n",
    "    errs = errs[sort_idx]\n",
    "\n",
    "    ax.errorbar(\n",
    "        values,\n",
    "        np.arange(len(values)),\n",
    "        xerr=errs,\n",
    "        fmt=\".\",\n",
    "        markersize=2,\n",
    "        linewidth=1,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    ax.invert_yaxis()\n",
    "    # xlim = max(abs(ax.get_xlim()[0]), abs(ax.get_xlim()[1]))\n",
    "    # ax.set_xlim(-xlim, xlim)\n",
    "    if plot_est:\n",
    "        interval_birge, _, _, _ = birge(values, errs, coverage=0.6827)\n",
    "        ax.axvline(interval_birge[0], color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axvline(interval_birge[1], color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "        interval_re, _, _, _ = random_effects_hksj(values, errs, coverage=0.6827)\n",
    "        ax.axvline(interval_re[0], color=\"blue\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axvline(interval_re[1], color=\"blue\", linestyle=\"--\", linewidth=1)\n",
    "        binomial_lower, _ = binomial_method(\n",
    "            values, p=0.5, target=0.15865, which=\"lower\"\n",
    "        )\n",
    "        ax.axvline(binomial_lower, color=\"green\", linestyle=\"--\", linewidth=1)\n",
    "        binomial_upper, _ = binomial_method(\n",
    "            values, p=0.5, target=0.15865, which=\"upper\"\n",
    "        )\n",
    "        ax.axvline(binomial_upper, color=\"green\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    ax.set_title(nice_names[name])\n",
    "\n",
    "    # ax.axvline(truth, color='black', linestyle='--', linewidth=1)\n",
    "    # ax.axvline(truth-err, color='grey', linestyle='--', linewidth=1)\n",
    "    # ax.axvline(truth+err, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # ax.set_xlabel(xlabels[name])\n",
    "    # remove y ticks\n",
    "    ax.set_yticks([])\n",
    "    ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "    xmin = ax.get_xlim()[0]\n",
    "    xmax = ax.get_xlim()[1]\n",
    "    ax.set_xlim(min(xmin, truth - err), max(xmax, truth + err))\n",
    "    # add top ticks\n",
    "    ax.tick_params(axis=\"x\", top=True)\n",
    "    # make ticks point inwards\n",
    "    ax.tick_params(direction=\"in\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/chemical.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "particle_keys = sorted(\n",
    "    [f.split(\".\")[0] for f in os.listdir(\"data/pdg1970\") if f.endswith(\".csv\")]\n",
    ")\n",
    "print(particle_keys)\n",
    "units = {}\n",
    "for p in particle_keys:\n",
    "    path = f\"data/pdg1970/{p}.csv\"\n",
    "    datasets[p] = pd.read_csv(path, comment=\"#\")\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        nice_names[p] = lines[0].strip(\"# \").strip(\"\\n\")\n",
    "        units[p] = lines[1].strip(\"# \").strip(\"\\n\")\n",
    "        value = float(lines[2].split(\":\")[1].strip())\n",
    "        sigma = float(lines[3].split(\":\")[1].strip())\n",
    "        truths[p] = (value, sigma)\n",
    "print(len(particle_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_group_keys = {\n",
    "    \"T\": \"Lifetime\",\n",
    "    \"MM\": \"Mag.~moment\",\n",
    "    \"M\": \"Mass\",\n",
    "    \"M-\": \"Mass\",\n",
    "    \"DEL\": \"Decay param\",\n",
    "    \"D\": \"Mass diff\",\n",
    "    \"WR\": \"Branching rate\",\n",
    "    \"W\": \"Width\",\n",
    "    \"R\": \"Branching ratio\",\n",
    "    \"L+E\": \"Decay param\",\n",
    "    \"A\": \"Decay param\",\n",
    "    \"F+-\": \"Decay param\",\n",
    "    \"XI\": \"Decay param\",\n",
    "}\n",
    "from collections import defaultdict\n",
    "\n",
    "particle_group_map = {}\n",
    "remaining = set(particle_keys)\n",
    "for k, v in particle_group_keys.items():\n",
    "    for p in remaining:\n",
    "        if k in p[1:]:\n",
    "            particle_group_map[p] = v\n",
    "            remaining = remaining - set([p])\n",
    "assert len(remaining) == 0, print(remaining)\n",
    "nice_names.update({v: v for v in particle_group_keys.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest\n",
    "from methods import sign_rank_test\n",
    "\n",
    "names = list(datasets.keys())\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "def format_number(x):\n",
    "    if x == -np.inf:\n",
    "        return r\"$\\approx-\\infty$\"\n",
    "    if x == np.inf:\n",
    "        return r\"$\\approx\\infty$\"\n",
    "    if x >= 1e2 or x <= -1e2:\n",
    "        return f\"${str(int(x))}$\"\n",
    "    return r\"$\\num{{{0:.2g}}}$\".format(x)\n",
    "\n",
    "\n",
    "def fmt_result(result):\n",
    "    # result is a list\n",
    "    minmax = [min(result), max(result)]\n",
    "    result_fmt = [format_number(x) for x in minmax]\n",
    "    # get unique values (preserving order)\n",
    "    result_fmt = pd.unique(np.array(result_fmt))\n",
    "    if len(result_fmt) == 1:\n",
    "        return result_fmt[0]\n",
    "    else:\n",
    "        return f'[{\", \".join(result_fmt)}]'\n",
    "\n",
    "\n",
    "for n in names:\n",
    "    results[n] = {}\n",
    "    results[n][\"count\"] = [len(datasets[n])]\n",
    "\n",
    "    truth_vals = []\n",
    "    if hasattr(truths[n], \"__iter__\"):\n",
    "        truth_vals.append(truths[n][0] - truths[n][1])\n",
    "        truth_vals.append(truths[n][0] + truths[n][1])\n",
    "        truth_vals.append(truths[n][0])\n",
    "    else:\n",
    "        truth_vals.append(truths[n])\n",
    "\n",
    "    results[n][\"num_over\"] = [np.sum(datasets[n].value > t) for t in truth_vals]\n",
    "    results[n][\"prop_over\"] = [\n",
    "        np.sum(datasets[n].value > t) / len(datasets[n]) for t in truth_vals\n",
    "    ]\n",
    "\n",
    "    # binomial test\n",
    "    results[n][\"binom_p_value\"] = [\n",
    "        binomtest(\n",
    "            np.sum(datasets[n].value > t),\n",
    "            len(datasets[n]),\n",
    "            p=0.5,\n",
    "            alternative=\"two-sided\",\n",
    "        ).pvalue\n",
    "        for t in truth_vals\n",
    "    ]\n",
    "\n",
    "    results[n][\"sign_rank_p_value\"] = [\n",
    "        sign_rank_test(datasets[n].value, t) for t in truth_vals\n",
    "    ]\n",
    "\n",
    "results_tex = {}\n",
    "for n, res in results.items():\n",
    "    results_tex[n] = {}\n",
    "    for k, val in res.items():\n",
    "        results_tex[n][k] = fmt_result(val)\n",
    "\n",
    "# put all the results in a pandas dataframe\n",
    "results_df = pd.DataFrame(results_tex).T\n",
    "results_df = results_df.drop(index=particle_keys)\n",
    "latex_names = [nice_names[k] for k in results_df.index]\n",
    "\n",
    "\n",
    "# use np to save latex table with & separator\n",
    "rows = results_df.values.tolist()\n",
    "rows = [list(row) for row in rows]\n",
    "rows = [[latex_names[i]] + rows[i] for i in range(len(rows))]\n",
    "txt = \" \\\\\\\\\\n\".join([\" & \".join(row) for row in rows])\n",
    "print(txt)\n",
    "with open(\"tables/hist-sym.tex\", \"w\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(particle_keys))\n",
    "from methods import birge, random_effects_hksj, sign_rank\n",
    "\n",
    "# particle_keys = ['lambda-lifetime', 'sigma+-lifetime', 'pion-mass-diff', 'charged-kaon-lifetime', 'charged-pion-lifetime', 'eta-mass']\n",
    "\n",
    "\n",
    "def pval_to_stars(pval):\n",
    "    if pval < 0.001:\n",
    "        return \"***\"\n",
    "    elif pval < 0.01:\n",
    "        return \"**\"\n",
    "    elif pval < 0.05:\n",
    "        return \"*\"\n",
    "\n",
    "\n",
    "for plot in [\"model\", \"inference\"]:\n",
    "    fig, axs = plt.subplots(\n",
    "        8, 10, figsize=(8, 8), gridspec_kw={\"wspace\": 0, \"hspace\": 0.4}\n",
    "    )\n",
    "    assert len(particle_keys) <= len(axs.flatten())\n",
    "    if plot == \"inference\":\n",
    "        num_covs = defaultdict(int)\n",
    "    for i, name in enumerate(particle_keys):\n",
    "        ax = axs.flatten()[i]\n",
    "        # values = np.array(datasets[name].value - truths[name])\n",
    "        truth = truths[name][0]\n",
    "        err = truths[name][1]\n",
    "\n",
    "        values = np.array(datasets[name].value)\n",
    "        errs = np.array(datasets[name].uncertainty)\n",
    "        # sort by decreasing error\n",
    "        sort_idx = np.argsort(datasets[name].year)\n",
    "        values = values[sort_idx]\n",
    "        errs = errs[sort_idx]\n",
    "\n",
    "        ax.axvspan(truth - err, truth + err, color=\"black\", alpha=0.3)\n",
    "\n",
    "        ax.errorbar(\n",
    "            values,\n",
    "            -np.arange(len(values)),\n",
    "            xerr=errs,\n",
    "            fmt=\".\",\n",
    "            markersize=2,\n",
    "            linewidth=1,\n",
    "            color=\"black\",\n",
    "        )\n",
    "        # ax.invert_yaxis()\n",
    "        # xlim = max(abs(ax.get_xlim()[0]), abs(ax.get_xlim()[1]))\n",
    "        # ax.set_xlim(-xlim, xlim)\n",
    "\n",
    "        # ax.set_title(f'{nice_names[name]} {units[name]}')\n",
    "        ax.set_title(r\"\\texttt{\" + name + \"}\", pad=2, fontsize=10)\n",
    "\n",
    "        # ax.axvline(truth, color='black', linestyle='--', linewidth=1)\n",
    "        # ax.axvline(truth-err, color='grey', linestyle='--', linewidth=1)\n",
    "        # ax.axvline(truth+err, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "        # ax.set_xlabel(xlabels[name])\n",
    "        # remove y ticks\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        ax.set_xlim(min(xmin, truth - err), max(xmax, truth + err))\n",
    "        # add top ticks\n",
    "        ax.tick_params(axis=\"x\", top=True)\n",
    "        # make ticks point inwards\n",
    "        ax.tick_params(direction=\"in\")\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        yspan = ymax - ymin\n",
    "\n",
    "        if plot == \"model\":\n",
    "            pvals = [\n",
    "                min(results[name][\"binom_p_value\"]),\n",
    "                min(results[name][\"sign_rank_p_value\"]),\n",
    "            ]\n",
    "            for i, pval in enumerate(pvals):\n",
    "                if pval < 0.05:\n",
    "                    stars = pval_to_stars(pval)\n",
    "                    side = \"left\" if i == 0 else \"right\"\n",
    "                    x = 0.1 if i == 0 else 0.91\n",
    "                    ax.text(\n",
    "                        x,\n",
    "                        -0.02,\n",
    "                        stars,\n",
    "                        color=\"black\",\n",
    "                        ha=side,\n",
    "                        va=\"top\",\n",
    "                        transform=ax.transAxes,\n",
    "                    )\n",
    "\n",
    "        if plot == \"inference\":\n",
    "            interval_signrank, cov = sign_rank(values, coverage=0.6827)\n",
    "\n",
    "            interval_birge, _, _, _ = birge(values, errs, coverage=cov, pdg=True)\n",
    "            # ax.axvline(interval_birge[0], color='red', linestyle='--', linewidth=1)\n",
    "            # ax.axvline(interval_birge[1], color='red', linestyle='--', linewidth=1)\n",
    "            interval_re, _, _, _ = random_effects_hksj(values, errs, coverage=cov)\n",
    "            # ax.axvline(interval_re[0], color='blue', linestyle='--', linewidth=1)\n",
    "            # ax.axvline(interval_re[1], color='blue', linestyle='--', linewidth=1)\n",
    "\n",
    "            # ax.axvline(interval_signrank[0], color='green', linestyle='--', linewidth=1)\n",
    "            # ax.axvline(interval_signrank[1], color='green', linestyle='--', linewidth=1)\n",
    "            intervals = {\n",
    "                \"pdg\": interval_birge,\n",
    "                \"re_hksj\": interval_re,\n",
    "                \"signrank\": interval_signrank,\n",
    "            }\n",
    "\n",
    "            for j, (name, interval) in enumerate(intervals.items()):\n",
    "                # ax.axvspan(interval[0], interval[1])\n",
    "                covers = (interval[0] <= truth + 2 * err) & (\n",
    "                    truth - 2 * err <= interval[1]\n",
    "                )\n",
    "                color = \"green\" if covers else \"red\"\n",
    "                y0 = ymin + j * yspan / 3\n",
    "                y1 = ymin + (j + 1) * yspan / 3\n",
    "                x0 = interval[0]\n",
    "                x1 = interval[1]\n",
    "                # plot a rectangle\n",
    "                ax.add_patch(\n",
    "                    plt.Rectangle(\n",
    "                        (x0, y0), x1 - x0, y1 - y0, color=color, alpha=1, zorder=-1\n",
    "                    )\n",
    "                )\n",
    "                # ax.axhspan(y0, y1, xmin=x0, xmax=x1, color=color, alpha=0.3, zorder=-1)\n",
    "                num_covs[name] += covers\n",
    "            # print(num_covs)\n",
    "\n",
    "    # remove unused axes\n",
    "    for ax in axs.flatten()[len(particle_keys) :]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    filename = \"particles.pdf\" if plot == \"model\" else \"particles-inference.pdf\"\n",
    "    plt.savefig(f\"figs/{filename}\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "print(num_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# print(values)\n",
    "# res.fun\n",
    "chi2.ppf(0.05, df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import birge, random_effects_mle\n",
    "from scipy.stats import norm, chi2\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "def fmt_result(r, bold=False):\n",
    "    if bold:\n",
    "        start = r\"$\\mathbf{\"\n",
    "        end = r\"}$\"\n",
    "    else:\n",
    "        start = r\"$\"\n",
    "        end = r\"$\"\n",
    "    if r == -np.inf:\n",
    "        return r\"$\\approx-\\infty$\"\n",
    "    elif r < -100:\n",
    "        return start + str(int(r)) + end\n",
    "    else:\n",
    "        return start + r\"{:.3g}\".format(r) + end\n",
    "\n",
    "\n",
    "category_map = {k: \"chemistry\" for k in chemistry_keys}\n",
    "category_map.update({k: \"particle\" for k in particle_keys})\n",
    "category_map.update({k: \"historical\" for k in historical_keys})\n",
    "\n",
    "key_order = []\n",
    "\n",
    "exponent_grid = np.linspace(0, 1, 201)\n",
    "for n in tqdm(names):\n",
    "    values = np.array(datasets[n].value)\n",
    "    sigmas = np.array(datasets[n].uncertainty)\n",
    "    has_sigma = ~np.isnan(sigmas)\n",
    "    values = values[has_sigma]\n",
    "    sigmas = sigmas[has_sigma]\n",
    "    if len(values) < 2:\n",
    "        print(f\"{n} has less than 2 data points\")\n",
    "        continue\n",
    "\n",
    "    truth = truths[n][0] if hasattr(truths[n], \"__iter__\") else truths[n]\n",
    "    # scaler = truth\n",
    "    # values = values/scaler\n",
    "    # sigmas = sigmas/scaler\n",
    "    # truth = truth/scaler\n",
    "\n",
    "    _, muhat_birge, _, chat = birge(\n",
    "        values, sigmas, coverage=0.6827, truth=truth, mle=True\n",
    "    )\n",
    "    assert np.isclose(truth, muhat_birge)\n",
    "    # brs.append(chat)\n",
    "    # mean_sigma = np.mean(sigmas)\n",
    "    # muhat_re, _, tau = random_effects_dl_base(values, sigmas)\n",
    "    # taus.append(np.mean(tau/sigmas))\n",
    "    _, muhat_re, _, tau = random_effects_mle(\n",
    "        values, sigmas, coverage=0.6827, truth=truth\n",
    "    )\n",
    "    assert np.isclose(truth, muhat_re)\n",
    "    # taus.append(np.mean(tau/sigmas))\n",
    "    # I2s.append(I2(values, sigmas))\n",
    "\n",
    "    # # generate values with same sigmas but no unaccounted for errors.\n",
    "    # # to be used as a control when analyzing the distribution of chat and tau\n",
    "    # values_control = np.random.normal(loc=0, scale=sigmas)\n",
    "    # _, _, _, chat_cont = birge(values_control, sigmas, coverage=0.6827)\n",
    "    # brs_cont.append(chat_cont)\n",
    "    # _, _, _, tau_cont = random_effects_mle(values_control, sigmas, coverage=0.6827)\n",
    "    # taus_cont.append(np.mean(tau_cont/sigmas))\n",
    "    # I2s_cont.append(I2(values_control, sigmas))\n",
    "\n",
    "    # # errscale_ps.append(errscale_test(values, sigmas))\n",
    "    # # errscale_ps_cont.append(errscale_test(values_control, sigmas))\n",
    "    if n in particle_keys:\n",
    "        key = particle_group_map[n]\n",
    "    else:\n",
    "        key = n\n",
    "    if key not in key_order:\n",
    "        key_order.append(key)\n",
    "    if key not in results:\n",
    "        results[key] = {}\n",
    "        results[key][\"name\"] = nice_names[key]\n",
    "        results[key][\"ds_count\"] = 0\n",
    "        results[key][\"count\"] = 0\n",
    "        for r in [\"birge_loglike\", \"re_loglike\", \"fe_loglike\"]:\n",
    "            results[key][r] = 0\n",
    "        results[key][\"exponent_loglike\"] = np.zeros_like(exponent_grid)\n",
    "        results[key][\"exponent_loglike_iter\"] = np.zeros_like(exponent_grid)\n",
    "    results[key][\"ds_count\"] += 1\n",
    "    results[key][\"count\"] += len(values)\n",
    "    # print(norm.pdf(values, loc=muhat_birge, scale=sigmas*chat))\n",
    "    results[key][\"birge_loglike\"] += np.sum(\n",
    "        norm.logpdf(values, loc=muhat_birge, scale=sigmas * chat)\n",
    "    )\n",
    "    results[key][\"re_loglike\"] += np.sum(\n",
    "        norm.logpdf(values, loc=muhat_re, scale=np.sqrt(sigmas**2 + tau**2))\n",
    "    )\n",
    "    results[key][\"fe_loglike\"] += np.sum(\n",
    "        norm.logpdf(values, loc=muhat_birge, scale=sigmas)\n",
    "    )\n",
    "\n",
    "    # def to_minimize(params):\n",
    "    #     tau, exponent = params\n",
    "    #     return -np.sum(norm.logpdf(values, loc=truth, scale=np.sqrt(sigmas**2+(tau**2)*(sigmas**(2*exponent)))))\n",
    "    # res = minimize(to_minimize, x0=(0,0), bounds=[(0,max(sigmas)*100),(0,4)])\n",
    "    def to_minimize(params, exponent):\n",
    "        tau = params[0]\n",
    "        return -np.sum(\n",
    "            norm.logpdf(\n",
    "                values,\n",
    "                loc=truth,\n",
    "                scale=np.sqrt(sigmas**2 + (tau**2) * (sigmas ** (2 * exponent))),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for i, exponent in enumerate(exponent_grid):\n",
    "        res = minimize(\n",
    "            to_minimize,\n",
    "            x0=(0,),\n",
    "            bounds=[(0, max(sigmas) * 100)],\n",
    "            args=(exponent,),\n",
    "            method=\"Nelder-Mead\",\n",
    "        )\n",
    "        if not res.success:\n",
    "            print(res)\n",
    "            raise (ValueError())\n",
    "        results[key][\"exponent_loglike_iter\"][i] += -res.fun\n",
    "\n",
    "    #     _, muhat_re, _, tau = random_effects_mle(values, sigmas, coverage=0.6827, truth=truth, exponent=exponent)\n",
    "    #     loglike = np.sum(norm.logpdf(values, loc=muhat_re, scale=np.sqrt(sigmas**2+(tau**2)*sigmas**exponent)))\n",
    "    #     results[key]['exponent_loglike_iter'][i] += loglike\n",
    "    tau_grid = np.concatenate(\n",
    "        (np.logspace(-4, 4, 1000) * max(sigmas), np.logspace(-4, 4, 1000))\n",
    "    )\n",
    "    for i, exponent in enumerate(exponent_grid):\n",
    "        scale = np.sqrt(\n",
    "            sigmas[:, np.newaxis] ** 2\n",
    "            + ((tau_grid) ** 2) * (sigmas[:, np.newaxis] ** (2 * exponent))\n",
    "        )\n",
    "        loglike = norm.logpdf(values[:, np.newaxis], loc=muhat_re, scale=scale)\n",
    "        results[key][\"exponent_loglike\"][i] += np.max(np.sum(loglike, axis=0))\n",
    "\n",
    "    results[key][\"category\"] = category_map[n]\n",
    "\n",
    "for k, res in list(results.items()):\n",
    "    category = res[\"category\"]\n",
    "    if category not in results:\n",
    "        # dict with default value of zero\n",
    "        results[category] = defaultdict(float)\n",
    "    if \"total\" not in results:\n",
    "        results[\"total\"] = defaultdict(float)\n",
    "    for r in [\n",
    "        \"ds_count\",\n",
    "        \"count\",\n",
    "        \"birge_loglike\",\n",
    "        \"re_loglike\",\n",
    "        \"fe_loglike\",\n",
    "        \"exponent_loglike\",\n",
    "        \"exponent_loglike_iter\",\n",
    "    ]:\n",
    "        results[category][r] += res[r]\n",
    "        results[\"total\"][r] += res[r]\n",
    "\n",
    "for res in results.values():\n",
    "    res[\"ds_count\"] = fmt_result(res[\"ds_count\"])\n",
    "    res[\"count\"] = fmt_result(res[\"count\"])\n",
    "    if len(np.unique(res[\"exponent_loglike\"])) == 1:\n",
    "        print(\"all same loglike\")\n",
    "        res[\"best_exponent\"] = \"N/A\"\n",
    "    else:\n",
    "        best_exponent = exponent_grid[np.argmax(res[\"exponent_loglike\"])]\n",
    "        res[\"best_exponent\"] = fmt_result(best_exponent)\n",
    "        logratio = 2 * (np.max(res[\"exponent_loglike\"]) - res[\"exponent_loglike\"])\n",
    "        within = logratio < chi2.ppf(0.95, df=1)\n",
    "        idx_l = np.argmax(within)\n",
    "        idx_u = -np.argmax(within[::-1]) - 1\n",
    "        low = exponent_grid[idx_l]\n",
    "        high = exponent_grid[idx_u]\n",
    "        res[\"exponent_interval\"] = f\"$[{np.round(low,2)}, {np.round(high,2)}]$\"\n",
    "\n",
    "    loglike_keys = [\"birge_loglike\", \"re_loglike\", \"fe_loglike\"]\n",
    "    loglike_values = [res[k] for k in loglike_keys]\n",
    "    largest_idx = np.argmax(loglike_values)\n",
    "    if len(np.unique(loglike_values)) == 1:\n",
    "        largest_idx = np.nan\n",
    "    for i, r in enumerate(loglike_keys):\n",
    "        res[r] = fmt_result(res[r], bold=(i == largest_idx))\n",
    "\n",
    "results[\"total\"][\"name\"] = \"Total\"\n",
    "results[\"historical\"][\"name\"] = \"Total (historical)\"\n",
    "results[\"chemistry\"][\"name\"] = \"Total (chemistry)\"\n",
    "results[\"particle\"][\"name\"] = \"Total (particle)\"\n",
    "print(results)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "del results_df[\"category\"]\n",
    "del results_df[\"exponent_loglike\"]\n",
    "del results_df[\"exponent_loglike_iter\"]\n",
    "# results_df.index = [r['name'] for r in results.values()]\n",
    "# sort to be: chemistry, particle, total chemistry, total particle, total\n",
    "row_order = key_order + [\"historical\", \"chemistry\", \"particle\", \"total\"]\n",
    "results_df = results_df.loc[row_order]\n",
    "# print(results_df)\n",
    "\n",
    "# use np to save latex table with & separator\n",
    "rows = results_df.values.tolist()\n",
    "hline_idxs = [5, 5 + len(chemistry_keys), -4, -4, -1]\n",
    "for i in hline_idxs:\n",
    "    rows[i][0] = r\"\\hline \" + rows[i][0]\n",
    "print(rows)\n",
    "txt = \" \\\\\\\\\\n\".join([\" & \".join(row) for row in rows])\n",
    "print(txt)\n",
    "with open(\"tables/hist-syst.tex\", \"w\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from measurement_dist import measurement_dist\n",
    "\n",
    "group_keys = {\n",
    "    \"historical\": historical_keys,\n",
    "    \"chemical\": chemistry_keys,\n",
    "    \"particle\": particle_keys,\n",
    "}\n",
    "name_map = {\"historical\": \"Historical\", \"chemical\": \"Chemical\", \"particle\": \"PDG 1970\"}\n",
    "import json\n",
    "\n",
    "for name, keys in group_keys.items():\n",
    "    dfs = []\n",
    "    group_truths = []\n",
    "    group_truth_sigmas = []\n",
    "    for key in keys:\n",
    "        df = datasets[key]\n",
    "        df = df[~pd.isna(df[\"uncertainty\"])]\n",
    "        if len(df) < 3:\n",
    "            continue\n",
    "        dfs.append(df)\n",
    "        if hasattr(truths[key], \"__iter__\"):\n",
    "            truth = truths[key][0]\n",
    "            truth_sigma = truths[key][1]\n",
    "        else:\n",
    "            truth = truths[key]\n",
    "            truth_sigma = 0\n",
    "        group_truths.append(truth)\n",
    "        group_truth_sigmas.append(truth_sigma)\n",
    "    output = measurement_dist(dfs, group_truths, group_truth_sigmas, lists=True)\n",
    "    output[\"name\"] = name_map[name]\n",
    "    with open(f\"results/measurement_dist/{name}.json\", \"w\") as f:\n",
    "        json.dump(output, f)\n",
    "    plt.plot(output[\"zspace\"], output[\"hstar\"], label=name)\n",
    "    plt.plot(output[\"zspace\"], output[\"hprime\"], label=name)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the empirical probability of the next observation being on the same side of the ground truth as the current one.\n",
    "n_pairs = np.zeros(1000, dtype=int)\n",
    "same_side = np.zeros((1000, 3), dtype=int)\n",
    "for i in range(1000):\n",
    "    idxs = np.random.choice(len(particle_keys), size=60, replace=False)\n",
    "    for idx in idxs:\n",
    "        p = particle_keys[idx]\n",
    "        truth = truths[p][0]\n",
    "        truth_l = truths[p][0] - 1 * truths[p][1]\n",
    "        truth_u = truths[p][0] + 1 * truths[p][1]\n",
    "        df = datasets[p].sort_index()\n",
    "        sort_idx = np.argsort(datasets[p].year + np.random.rand(len(df)) * 0.1)\n",
    "        values = np.array(datasets[p][\"value\"])[sort_idx]\n",
    "\n",
    "        overs = np.array([values > truth, values > truth_l, values > truth_u])\n",
    "        # over = values > truth\n",
    "        xors = np.array([np.bitwise_xor(over[:-1], over[1:]) for over in overs])\n",
    "        n_pairs[i] += len(xors[0])\n",
    "        same_sides = np.sum(~xors, axis=1)\n",
    "        # print(same_sides)\n",
    "        same_side[i, 0] += same_sides[0]\n",
    "        same_side[i, 1] += min(same_sides)\n",
    "        same_side[i, 2] += max(same_sides)\n",
    "        # same_side[0] += np.sum(~xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(same_side)\n",
    "# print(n_pairs)\n",
    "from scipy.stats import binom\n",
    "\n",
    "p_mid = 1 - binom.cdf(k=same_side[:, 0], n=n_pairs, p=0.5)\n",
    "p_generous = 1 - binom.cdf(k=same_side[:, 1], n=n_pairs, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest\n",
    "\n",
    "intervals_mid = np.array(\n",
    "    [\n",
    "        list(\n",
    "            binomtest(\n",
    "                k=same_side[i, 0], n=n_pairs[i], p=0.5, alternative=\"two-sided\"\n",
    "            ).proportion_ci(confidence_level=0.90, method=\"exact\")\n",
    "        )\n",
    "        for i in range(same_side.shape[0])\n",
    "    ]\n",
    ")\n",
    "intervals_generous = np.array(\n",
    "    [\n",
    "        list(\n",
    "            binomtest(\n",
    "                k=same_side[i, 1], n=n_pairs[i], p=0.5, alternative=\"two-sided\"\n",
    "            ).proportion_ci(confidence_level=0.90, method=\"exact\")\n",
    "        )\n",
    "        for i in range(same_side.shape[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "axs[0].hist(\n",
    "    p_mid,\n",
    "    bins=np.linspace(0, 0.1, 21),\n",
    "    histtype=\"step\",\n",
    "    color=\"grey\",\n",
    "    density=True,\n",
    "    linewidth=2,\n",
    "    label=r\"$\\theta$ as truth\",\n",
    ")\n",
    "axs[0].hist(\n",
    "    p_generous,\n",
    "    bins=np.linspace(0, 0.1, 21),\n",
    "    histtype=\"step\",\n",
    "    color=\"black\",\n",
    "    density=True,\n",
    "    linewidth=2,\n",
    "    label=r\"$\\theta$, $\\theta-\\sigma$, or $\\theta+\\sigma$ as truth\",\n",
    ")\n",
    "axs[0].set_xlabel(\"$p$-value on $H_0$: $p=0.5$, $H_1$: $p>0.5$\")\n",
    "axs[0].set_ylabel(\"Density\")\n",
    "axs[0].set_xlim(0, 0.1)\n",
    "axs[0].legend(frameon=False)\n",
    "\n",
    "# histbins = np.linspace(0.45, 0.8, 41)\n",
    "# axs[1].hist(intervals_mid[:,0], bins=histbins, histtype='step', color='grey', density=True, linewidth=2)\n",
    "# axs[1].hist(intervals_mid[:,1], bins=histbins, histtype='step', color='grey', density=True, linewidth=2)\n",
    "\n",
    "colors = [\"grey\", \"black\"]\n",
    "for i, intervals in enumerate([intervals_mid, intervals_generous]):\n",
    "    midpoints = np.median(intervals, axis=0)\n",
    "    uppers = np.quantile(intervals, 0.75, axis=0)\n",
    "    lowers = np.quantile(intervals, 0.25, axis=0)\n",
    "    middle = np.mean(midpoints)\n",
    "    xerr0 = [middle - uppers[0], lowers[1] - middle]\n",
    "    xerr1 = [middle - midpoints[0], midpoints[1] - middle]\n",
    "    xerr2 = [middle - lowers[0], uppers[1] - middle]\n",
    "\n",
    "    axs[1].errorbar(\n",
    "        [middle],\n",
    "        [i],\n",
    "        xerr=np.array([xerr0]).T,\n",
    "        fmt=\"o\",\n",
    "        color=colors[i],\n",
    "        linewidth=2,\n",
    "        capsize=4,\n",
    "        markersize=0,\n",
    "    )\n",
    "    axs[1].errorbar(\n",
    "        [middle],\n",
    "        [i],\n",
    "        xerr=np.array([xerr1]).T,\n",
    "        fmt=\"o\",\n",
    "        color=colors[i],\n",
    "        linewidth=2,\n",
    "        capsize=4,\n",
    "        markersize=0,\n",
    "    )\n",
    "    axs[1].errorbar(\n",
    "        [middle],\n",
    "        [i],\n",
    "        xerr=np.array([xerr2]).T,\n",
    "        fmt=\"o\",\n",
    "        color=colors[i],\n",
    "        linewidth=2,\n",
    "        capsize=4,\n",
    "        markersize=0,\n",
    "    )\n",
    "\n",
    "axs[1].axvline(0.5, color=\"red\", linestyle=\"--\")\n",
    "axs[1].set_ylim(-0.5, 1.5)\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "axs[1].set_xlabel(r\"90\\% confidence intervals for $p$\")\n",
    "axs[1].set_xlim(0.45, 0.7)\n",
    "plt.suptitle(r\"Tests on $p=P(y_i,y_{i+1}\\,$on same side of truth)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/pdg_indep.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = truths[\"c\"]\n",
    "values = np.array(datasets[\"c\"][\"value\"])\n",
    "over = values > truth\n",
    "xor = np.bitwise_xor(over[:-1], over[1:])\n",
    "n_pairs = len(xor)\n",
    "same_side = np.sum(~xor)\n",
    "print(n_pairs, same_side, binom.cdf(k=same_side, n=n_pairs, p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "ns = [len(datasets[n]) for n in particle_keys]\n",
    "ks_mid = [np.sum(datasets[n][\"value\"] > truths[n][0]) for n in particle_keys]\n",
    "ks_generous = []\n",
    "for p in particle_keys:\n",
    "    n = len(datasets[p])\n",
    "    truth = truths[p][0]\n",
    "    sigma = truths[p][1]\n",
    "    contenders = [truth, truth - sigma, truth + sigma]\n",
    "    k_contenders = np.array(\n",
    "        [np.sum(datasets[p][\"value\"] > contender) for contender in contenders]\n",
    "    )\n",
    "    closest_to_half = np.argmin(np.abs(k_contenders / n - 0.5))\n",
    "    ks_generous.append(k_contenders[closest_to_half])\n",
    "rhos = []\n",
    "for ks in [ks_mid, ks_generous]:\n",
    "    with pm.Model() as model:\n",
    "        rho = pm.Uniform(\"rho\", lower=0, upper=1)\n",
    "        alphabeta = (1 / rho) - 1\n",
    "        alpha = alphabeta / 2\n",
    "        beta = alphabeta / 2\n",
    "        k = pm.BetaBinomial(\"k\", alpha=alpha, beta=beta, n=ns, observed=ks)\n",
    "        trace = pm.sample(20000, tune=1000)\n",
    "    rhos.append(trace.posterior[\"rho\"].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2))\n",
    "bins = np.linspace(0, 1, 401)\n",
    "plt.hist(\n",
    "    rhos[0],\n",
    "    color=\"grey\",\n",
    "    density=True,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    linewidth=1,\n",
    "    label=r\"$\\theta$ as truth\",\n",
    ")\n",
    "plt.hist(\n",
    "    rhos[1],\n",
    "    color=\"black\",\n",
    "    density=True,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    linewidth=1,\n",
    "    label=r\"$\\theta$, $\\theta-\\sigma$, or $\\theta+\\sigma$ as truth\",\n",
    ")\n",
    "plt.xlim(0, 0.5)\n",
    "plt.xlabel(r\"$\\rho$\")\n",
    "plt.ylabel(\"Posterior density\")\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(\"figs/pdg_rho.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate cdfs for correlated binomial\n",
    "\n",
    "binom_corr_cdfs = []\n",
    "p = 0.55\n",
    "for n in range(2, 11):\n",
    "    binom_corr_pmf = np.zeros(n + 1)\n",
    "    for i in range(100000):\n",
    "        same_side = np.random.choice([-1, 1], size=n - 1, p=[1 - p, p])\n",
    "        initial = np.random.choice([-1, 1])\n",
    "        seq = np.concatenate(([initial], np.cumprod(same_side) * initial))\n",
    "        seq = (seq + 1) // 2\n",
    "        binom_corr_pmf[np.sum(seq)] += 1\n",
    "    binom_corr_pmf /= np.sum(binom_corr_pmf)\n",
    "    binom_corr_cdfs.append(np.cumsum(binom_corr_pmf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_corr_cdfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(binom_corr_cdfs[2])\n",
    "plt.plot(binom.cdf(np.arange(5), n=4, p=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with the particle dataset\n",
    "from methods import (\n",
    "    birge,\n",
    "    random_effects_hksj,\n",
    "    binomial_method,\n",
    "    random_effects_dl,\n",
    "    vniim,\n",
    "    sign_rank,\n",
    "    flip_interval,\n",
    "    fixed_effect,\n",
    "    linear_pool,\n",
    "    birge_forecast,\n",
    "    boot,\n",
    "    random_effects_mle,\n",
    "    random_effects_pm,\n",
    "    interval_score,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from math import comb\n",
    "from itertools import combinations\n",
    "from scipy.stats import norm, betabinom\n",
    "\n",
    "# birge_covs = defaultdict(lambda: defaultdict(list))\n",
    "# re_covs = defaultdict(lambda: defaultdict(list))\n",
    "binom_target_covs = {}\n",
    "signrank_target_covs = {}\n",
    "target_covs = {}\n",
    "binom_corr_target_covs = {}\n",
    "# binom_covs = defaultdict(lambda: defaultdict(list))\n",
    "# method_covs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "n_combs = 100\n",
    "methods = [\n",
    "    \"fe\",\n",
    "    \"birge\",\n",
    "    \"birge-t\",\n",
    "    \"pdg\",\n",
    "    \"codata\",\n",
    "    \"re_hksj\",\n",
    "    \"re_mmhksj\",\n",
    "    \"re_dl\",\n",
    "    \"re_mle\",\n",
    "    \"re_pm\",\n",
    "    # 'vniim',\n",
    "    \"binom\",\n",
    "    \"signrank\",\n",
    "    # 'boot-normal',\n",
    "    # 'boot-student',\n",
    "    \"binom-corr\",\n",
    "    # 'birge-forecast'\n",
    "    # 'flip',\n",
    "    # \"lp\",\n",
    "]\n",
    "ns = np.arange(2, 11)\n",
    "result_shape = (len(ns), len(methods), len(particle_keys), n_combs)\n",
    "method_covs = np.full(result_shape, np.nan)\n",
    "method_widths = np.full(result_shape, np.nan)\n",
    "method_iscores = np.full(result_shape, np.nan)\n",
    "method_widths_rel_fe = np.full(result_shape, np.nan)\n",
    "# signrank_covs = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# target_cov = 1-2*binom_tail\n",
    "# print(target_cov)\n",
    "# target_cov = 0.75\n",
    "# tail_prob = (1-target_cov)/2\n",
    "\n",
    "\n",
    "def shrink_interval(interval, z_actual, z_target):\n",
    "    width = interval[1] - interval[0]\n",
    "    middle = (interval[0] + interval[1]) / 2\n",
    "    new_width = width * z_target / z_actual\n",
    "    return (middle - new_width / 2, middle + new_width / 2)\n",
    "\n",
    "\n",
    "# ns = [6]\n",
    "for i, n in enumerate(ns):\n",
    "    base_target_cov = 0.6827\n",
    "    if n > 2:\n",
    "        _, binom_target_cov, _ = binomial_method(\n",
    "            np.sort(np.arange(n)), coverage=base_target_cov\n",
    "        )\n",
    "        binom_target_covs[n] = binom_target_cov\n",
    "        _, signrank_target_cov = sign_rank(\n",
    "            np.sort(np.arange(n)), coverage=base_target_cov\n",
    "        )\n",
    "        signrank_target_covs[n] = signrank_target_cov\n",
    "        # z_signrank = -norm.ppf((1-signrank_target_cov)/2)\n",
    "        # _, binom_corr_target_cov = binomial_method(np.sort(np.arange(n)), cdf=binom_corr_cdfs[i], coverage=base_target_cov)\n",
    "        # binom_corr_target_covs[n] = binom_corr_target_cov\n",
    "        beta_binom_cdf = betabinom.cdf(np.arange(n + 1), n, a=4.5, b=4.5)\n",
    "        _, binom_corr_target_cov, _ = binomial_method(\n",
    "            np.sort(np.arange(n)), cdf=beta_binom_cdf, coverage=0.65\n",
    "        )\n",
    "        binom_corr_target_covs[n] = binom_corr_target_cov\n",
    "\n",
    "        target_cov = base_target_cov\n",
    "\n",
    "        z_binom = -norm.ppf((1 - binom_target_cov) / 2)\n",
    "        z_binom_corr = -norm.ppf((1 - binom_corr_target_cov) / 2)\n",
    "\n",
    "    else:\n",
    "        target_cov = base_target_cov\n",
    "    z_target_cov = -norm.ppf((1 - target_cov) / 2)\n",
    "    target_covs[n] = target_cov\n",
    "\n",
    "    for j, name in tqdm(enumerate(particle_keys)):\n",
    "        df = datasets[name]\n",
    "        if len(df) < n:\n",
    "            continue\n",
    "        value = np.array(df.value)\n",
    "        sigma = np.array(df.uncertainty)\n",
    "        truth_min = truths[name][0] - 3 * truths[name][1]\n",
    "        truth_max = truths[name][0] + 3 * truths[name][1]\n",
    "        truth = truths[name][0]\n",
    "\n",
    "        # if the number of combinations of n results is small, we use all combinations,\n",
    "        # otherwise we randomly sample\n",
    "        if comb(len(value), n) <= n_combs:\n",
    "            subset_idxs = [np.array(c) for c in combinations(range(len(value)), n)]\n",
    "        else:\n",
    "            subset_idxs = np.array(\n",
    "                [\n",
    "                    np.random.choice(len(value), size=n, replace=False)\n",
    "                    for _ in range(n_combs)\n",
    "                ]\n",
    "            )\n",
    "        for k, subset_idx in enumerate(subset_idxs):\n",
    "            value_s = value[subset_idx]\n",
    "            sigma_s = sigma[subset_idx]\n",
    "            intervals = {}\n",
    "            intervals[\"birge\"], wm, _, _ = birge(value_s, sigma_s, coverage=target_cov)\n",
    "            intervals[\"birge-t\"], wm, _, _ = birge(\n",
    "                value_s, sigma_s, coverage=target_cov, dist=\"t\"\n",
    "            )\n",
    "            intervals[\"pdg\"], wm, _, _ = birge(\n",
    "                value_s, sigma_s, coverage=target_cov, pdg=True\n",
    "            )\n",
    "            intervals[\"codata\"], wm, _, _ = birge(\n",
    "                value_s, sigma_s, coverage=target_cov, codata=True\n",
    "            )\n",
    "            intervals[\"re_hksj\"], muhat, _, _ = random_effects_hksj(\n",
    "                value_s, sigma_s, coverage=target_cov\n",
    "            )\n",
    "            intervals[\"re_mmhksj\"], muhat, _, _ = random_effects_hksj(\n",
    "                value_s, sigma_s, coverage=target_cov, trunc=\"talpha\"\n",
    "            )\n",
    "            intervals[\"re_dl\"], muhat, _, _ = random_effects_dl(\n",
    "                value_s, sigma_s, coverage=target_cov\n",
    "            )\n",
    "            intervals[\"re_pm\"], muhat, _, _ = random_effects_pm(\n",
    "                value_s, sigma_s, coverage=target_cov\n",
    "            )\n",
    "            intervals[\"re_mle\"], muhat, _, _ = random_effects_mle(\n",
    "                value_s, sigma_s, coverage=target_cov\n",
    "            )\n",
    "            # intervals['boot-normal'] = boot(value_s, sigma_s, coverage=target_cov, which='normal')\n",
    "            # intervals['boot-student'] = boot(value_s, sigma_s, coverage=target_cov, which='studentized')\n",
    "            # intervals['birge-forecast'], _, _, _, _ = birge_forecast(value_s, sigma_s, coverage=target_cov)\n",
    "            # intervals['vniim'], wm = vniim(value_s, sigma_s, coverage=target_cov)\n",
    "            if n > 2:\n",
    "                interval, _, interval_shrink = binomial_method(\n",
    "                    value_s, p=0.5, coverage=target_cov, shrink=\"cdf-interp\"\n",
    "                )\n",
    "                intervals[\"binom\"] = (\n",
    "                    interval  # shrink_interval(interval, z_binom, z_target_cov)\n",
    "                )\n",
    "                interval, _, interval_shrink = binomial_method(\n",
    "                    value_s, cdf=beta_binom_cdf, coverage=0.65, shrink=\"center\"\n",
    "                )\n",
    "                intervals[\"binom-corr\"] = (\n",
    "                    interval  # shrink_interval(interval, z_binom_corr, z_target_cov)\n",
    "                )\n",
    "                interval, _ = sign_rank(value_s, coverage=target_cov)\n",
    "                intervals[\"signrank\"] = (\n",
    "                    interval  # shrink_interval(interval, z_signrank, z_target_cov)\n",
    "                )\n",
    "                # fe intervals for different target coverages, for width comparison\n",
    "                fe_binom_interval, _, _ = fixed_effect(\n",
    "                    value_s, sigma_s, coverage=binom_target_cov\n",
    "                )\n",
    "                fe_binom_corr_interval, _, _ = fixed_effect(\n",
    "                    value_s, sigma_s, coverage=binom_corr_target_cov\n",
    "                )\n",
    "                fe_signrank_interval, _, _ = fixed_effect(\n",
    "                    value_s, sigma_s, coverage=signrank_target_cov\n",
    "                )\n",
    "            # intervals['flip'], _ = flip_interval(value_s, coverage=target_cov, max_iter=100, boot=False)\n",
    "            intervals[\"fe\"], _, _ = fixed_effect(value_s, sigma_s, coverage=target_cov)\n",
    "\n",
    "            # intervals[\"lp\"], _ = linear_pool(\n",
    "            #    value_s, sigma_s, coverage=target_cov, gridn=1000\n",
    "            # )\n",
    "\n",
    "            for l, method in enumerate(methods):\n",
    "                if method not in intervals:\n",
    "                    continue\n",
    "                interval = intervals[method]\n",
    "                width = interval[1] - interval[0]\n",
    "                if method == \"binom\":\n",
    "                    method_target_cov = binom_target_cov\n",
    "                    fe_width = fe_binom_interval[1] - fe_binom_interval[0]\n",
    "                elif method == \"binom-corr\":\n",
    "                    method_target_cov = binom_corr_target_cov\n",
    "                    fe_width = fe_binom_corr_interval[1] - fe_binom_corr_interval[0]\n",
    "                elif method == \"signrank\":\n",
    "                    method_target_cov = signrank_target_cov\n",
    "                    fe_width = fe_signrank_interval[1] - fe_signrank_interval[0]\n",
    "                else:\n",
    "                    method_target_cov = target_cov\n",
    "                    fe_width = intervals[\"fe\"][1] - intervals[\"fe\"][0]\n",
    "                method_widths_rel_fe[i, l, j, k] = width / fe_width\n",
    "                method_covs[i, l, j, k] = (interval[0] <= truth_max) & (\n",
    "                    truth_min <= interval[1]\n",
    "                )\n",
    "                method_widths[i, l, j, k] = interval[1] - interval[0]\n",
    "                method_iscores[i, l, j, k] = interval_score(\n",
    "                    interval, truth, coverage=method_target_cov, percent=True\n",
    "                )\n",
    "method_covs = np.nanmean(method_covs, axis=3)\n",
    "method_widths = np.nanmean(method_widths, axis=3)\n",
    "method_iscores = np.nanmean(method_iscores, axis=3)\n",
    "method_widths_rel_fe = np.nanmean(method_widths_rel_fe, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_covs.shape)\n",
    "n_datasets = np.sum(~np.isnan(method_covs[:, 0, :]), axis=1)\n",
    "print(n_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"binom\": \"orange\",\n",
    "    \"binom-corr\": \"goldenrod\",\n",
    "    \"signrank\": \"red\",\n",
    "    \"flip\": \"yellow\",\n",
    "    \"fe\": \"grey\",\n",
    "    \"lp\": \"lightgrey\",\n",
    "    \"re_hksj\": \"green\",\n",
    "    \"re_dl\": \"lightgreen\",\n",
    "    \"vniim\": \"lightblue\",\n",
    "    \"birge\": \"blue\",\n",
    "    \"pdg\": \"lightblue\",\n",
    "    \"codata\": \"plum\",\n",
    "    \"boot-normal\": \"brown\",\n",
    "    \"boot-student\": \"tan\",\n",
    "    # 'birge-forecast': 'pink'\n",
    "}\n",
    "\n",
    "method_widths_rel = method_widths / method_widths[:, 0, np.newaxis, :]\n",
    "\n",
    "# bootstrap over datasets\n",
    "B = 400\n",
    "method_covs_boot = np.full((len(ns), len(methods), len(particle_keys), B), np.nan)\n",
    "for b in range(B):\n",
    "    dataset_idxs = np.random.choice(\n",
    "        len(particle_keys), size=len(particle_keys), replace=True\n",
    "    )\n",
    "    method_covs_boot[:, :, :, b] = method_covs[:, :, dataset_idxs]\n",
    "method_covs_mean = np.nanmean(method_covs, axis=2)\n",
    "method_covs_mean_rel = (\n",
    "    method_covs_mean / method_covs_mean[:, methods.index(\"signrank\"), np.newaxis]\n",
    ")\n",
    "method_covs_mean_boot = np.nanmean(method_covs_boot, axis=2)\n",
    "method_covs_mean_boot_rel = (\n",
    "    method_covs_mean_boot\n",
    "    / method_covs_mean_boot[:, methods.index(\"signrank\"), np.newaxis, :]\n",
    ")\n",
    "\n",
    "uppers_rel = 2 * method_covs_mean_rel - np.nanquantile(\n",
    "    method_covs_mean_boot_rel, 0.1, axis=2\n",
    ")\n",
    "lowers_rel = 2 * method_covs_mean_rel - np.nanquantile(\n",
    "    method_covs_mean_boot_rel, 0.9, axis=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_covs_mean_rel[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hspace = 0\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 7), sharex=True, gridspec_kw={\"hspace\": 0})\n",
    "for i, method in enumerate(methods):\n",
    "    axs[0].plot(\n",
    "        ns, np.nanmean(method_covs[:, i, :], axis=1), label=method, color=colors[method]\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        ns,\n",
    "        np.nanmean(method_widths_rel[:, i, :], axis=1),\n",
    "        label=method,\n",
    "        color=colors[method],\n",
    "    )\n",
    "    axs[2].plot(ns, method_covs_mean_rel[:, i], label=method, color=colors[method])\n",
    "    axs[2].plot(\n",
    "        ns, uppers_rel[:, i], color=colors[method], linestyle=\"--\", linewidth=0.5\n",
    "    )\n",
    "    axs[2].plot(\n",
    "        ns, lowers_rel[:, i], color=colors[method], linestyle=\"--\", linewidth=0.5\n",
    "    )\n",
    "# axs[0].plot(signrank_target_covs.keys(), signrank_target_covs.values(), label='target (signrank)', color='grey', linestyle='--')\n",
    "# axs[0].plot(binom_target_covs.keys(), binom_target_covs.values(), label='target (binom)', color='grey', linestyle=':')\n",
    "# axs[0].plot(binom_corr_target_covs.keys(), binom_corr_target_covs.values(), label='target (binom-corr)', color='grey', linestyle='--')\n",
    "axs[0].plot(\n",
    "    target_covs.keys(),\n",
    "    target_covs.values(),\n",
    "    label=\"target\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    ")\n",
    "axs[0].set_ylabel(\"Coverage\")\n",
    "# axs[0].legend()\n",
    "axs[1].set_ylabel(\"Width relative to fixed effect\")\n",
    "axs[2].legend(frameon=False)\n",
    "\n",
    "axs[2].set_xlabel(\"Number of results\")\n",
    "axs[2].set_ylabel(\"Coverage relative to signrank\")\n",
    "\n",
    "# point ticks inwards and add top and right ticks\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction=\"in\", top=True, right=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 3.5), sharex=True, gridspec_kw={\"hspace\": 0})\n",
    "linewidth = 1.5\n",
    "\n",
    "for i, result in enumerate([method_covs, method_widths_rel_fe]):\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"re_dl\"), :], axis=1),\n",
    "        label=\"RE (DL)\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"re_hksj\"), :], axis=1),\n",
    "        label=\"RE (HKSJ)\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\":\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"re_mmhksj\"), :], axis=1),\n",
    "        label=\"RE (mmHKSJ)\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=(0, (1, 8)),\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"re_mle\"), :], axis=1),\n",
    "        label=\"RE (MLE)\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"re_pm\"), :], axis=1),\n",
    "        label=\"RE (PM)\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\"-.\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"fe\"), :], axis=1),\n",
    "        label=\"Fixed Effect\",\n",
    "        color=\"grey\",\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "\n",
    "axs[0].set_ylabel(\"Coverage\")\n",
    "axs[0].plot(\n",
    "    target_covs.keys(), target_covs.values(), color=\"black\", linestyle=\"-\", linewidth=1\n",
    ")\n",
    "axs[0].text(\n",
    "    9,\n",
    "    list(target_covs.values())[-1] * 0.98,\n",
    "    r\"$\\uparrow$ Target coverage\",\n",
    "    va=\"top\",\n",
    "    ha=\"center\",\n",
    ")\n",
    "axs[0].text(8, 0.45, r\"Achieved coverage\", va=\"bottom\", ha=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of results $n$\")\n",
    "axs[1].set_ylabel(r\"Average width\\\\relative to Fixed Effect\")\n",
    "\n",
    "axs[0].legend(\n",
    "    frameon=False,\n",
    "    ncol=6,\n",
    "    prop={\"size\": 8},\n",
    "    bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n",
    "    loc=\"lower left\",\n",
    "    mode=\"expand\",\n",
    "    borderaxespad=0.0,\n",
    ")\n",
    "\n",
    "plt.xlim(2, 10)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction=\"in\", top=True, right=True)\n",
    "plt.savefig(\"figs/pdg1970_cov_re.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 3.5), sharex=True, gridspec_kw={\"hspace\": 0})\n",
    "linewidth = 1.5\n",
    "\n",
    "for i, result in enumerate([method_covs, method_widths_rel_fe]):\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"pdg\"), :], axis=1),\n",
    "        label=\"PDG\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"re_pm\"), :], axis=1),\n",
    "        label=\"RE (PM)\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"fe\"), :], axis=1),\n",
    "        label=\"Fixed Effect\",\n",
    "        color=\"grey\",\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "\n",
    "axs[0].set_ylabel(\"Coverage\")\n",
    "axs[0].plot(\n",
    "    target_covs.keys(), target_covs.values(), color=\"black\", linestyle=\"-\", linewidth=1\n",
    ")\n",
    "axs[0].text(\n",
    "    9,\n",
    "    list(target_covs.values())[-1] * 0.98,\n",
    "    r\"$\\uparrow$ Target coverage\",\n",
    "    va=\"top\",\n",
    "    ha=\"center\",\n",
    ")\n",
    "# axs[0].text(8, 0.45, r'Achieved coverage', va='bottom', ha='center')\n",
    "\n",
    "plt.xlabel(\"Number of results $n$\")\n",
    "axs[1].set_ylabel(r\"Average width\\\\relative to Fixed Effect\")\n",
    "\n",
    "axs[0].legend(\n",
    "    frameon=False,\n",
    "    ncol=6,\n",
    "    prop={\"size\": 8},\n",
    "    bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n",
    "    loc=\"lower left\",\n",
    "    mode=\"expand\",\n",
    "    borderaxespad=0.0,\n",
    ")\n",
    "\n",
    "plt.xlim(2, 10)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction=\"in\", top=True, right=True)\n",
    "plt.savefig(\"figs/pdg1970_cov_re-pdg.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(\"figs/pdg1970_cov_re-pdg.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(method_covs[:, methods.index(\"re_hksj\"), :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_effects_hksj(value, sigma, coverage=0.6827, trunc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 3.5), sharex=True, gridspec_kw={\"hspace\": 0})\n",
    "linewidth = 1.5\n",
    "\n",
    "for i, result in enumerate([method_covs, method_widths_rel_fe]):\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"birge\"), :], axis=1),\n",
    "        label=\"Birge Ratio\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"birge-t\"), :], axis=1),\n",
    "        label=\"Birge Ratio ($t$)\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\"-.\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"pdg\"), :], axis=1),\n",
    "        label=\"PDG\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\":\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"codata\"), :], axis=1),\n",
    "        label=\"CODATA\",\n",
    "        color=\"black\",\n",
    "        linewidth=linewidth,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        ns,\n",
    "        np.nanmean(result[:, methods.index(\"fe\"), :], axis=1),\n",
    "        label=\"Fixed Effect\",\n",
    "        color=\"grey\",\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "\n",
    "axs[0].set_ylabel(\"Coverage\")\n",
    "axs[0].plot(\n",
    "    target_covs.keys(), target_covs.values(), color=\"black\", linestyle=\"-\", linewidth=1\n",
    ")\n",
    "axs[0].text(\n",
    "    9,\n",
    "    list(target_covs.values())[-1] * 0.98,\n",
    "    r\"$\\uparrow$ Target coverage\",\n",
    "    va=\"top\",\n",
    "    ha=\"center\",\n",
    ")\n",
    "axs[0].text(8, 0.49, r\"Achieved coverage\", va=\"bottom\", ha=\"center\")\n",
    "axs[0].legend(frameon=False)\n",
    "\n",
    "plt.xlabel(\"Number of results $n$\")\n",
    "axs[1].set_ylabel(r\"Average width\\\\relative to Fixed Effect\")\n",
    "\n",
    "axs[0].legend(\n",
    "    frameon=False,\n",
    "    ncol=5,\n",
    "    prop={\"size\": 8},\n",
    "    bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n",
    "    loc=\"lower left\",\n",
    "    mode=\"expand\",\n",
    "    borderaxespad=0.0,\n",
    ")\n",
    "\n",
    "plt.xlim(2, 10)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction=\"in\", top=True, right=True)\n",
    "plt.savefig(\"figs/pdg1970_cov_birge.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 5.6), sharex=True, gridspec_kw={\"hspace\": 0})\n",
    "\n",
    "nonparam_target_covs = {\n",
    "    'binom': binom_target_covs,\n",
    "    'binom-corr': binom_corr_target_covs,\n",
    "    'signrank': signrank_target_covs,\n",
    "}\n",
    "colors = {\n",
    "    'binom': 'black',\n",
    "    'binom-corr': 'red',\n",
    "    'signrank': 'silver',\n",
    "}\n",
    "labels = {\n",
    "    'binom': 'ST',\n",
    "    'binom-corr': r\"ST$_{\\rho=0.1}$\",\n",
    "    'signrank': r\"SRT\",\n",
    "}\n",
    "linewidths = defaultdict(lambda: 3)\n",
    "linewidths['binom-corr'] = 1.5\n",
    "\n",
    "axs[0].set_ylabel(\"Coverage\")\n",
    "axs[1].set_ylabel(\"Width relative to FE\")\n",
    "plt.xlabel(\"Number of results $n$\")\n",
    "\n",
    "# plt.plot(target_covs.keys(), target_covs.values(), label='target', color='black', linestyle='--', linewidth=2)\n",
    "# print(binom_corr_target_covs[3])\n",
    "plt.xlim(3, 10)\n",
    "axs[1].set_ylim(1.5, 4.2)\n",
    "axs[0].tick_params(direction=\"in\", top=True, right=True)\n",
    "axs[1].tick_params(direction=\"in\", top=True, right=True)\n",
    "for method in ['binom', 'signrank', 'binom-corr']:\n",
    "    for i, result in enumerate([method_covs, method_widths_rel_fe]):\n",
    "        axs[i].plot(\n",
    "            ns,\n",
    "            np.nanmean(result[:, methods.index(method), :], axis=1),\n",
    "            label=labels[method],\n",
    "            color=colors[method],\n",
    "            linewidth=linewidths[method],\n",
    "        )\n",
    "    axs[0].plot(\n",
    "        nonparam_target_covs[method].keys(),\n",
    "        nonparam_target_covs[method].values(),\n",
    "        color=colors[method],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    plt.legend(frameon=False)\n",
    "    if method == 'signrank':\n",
    "        plt.savefig(\"figs/pdg1970_cov_nonparam_less.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.savefig(\"figs/pdg1970_cov_nonparam.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(\"figs/pdg1970_cov_nonparam.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 5.6), sharex=True, gridspec_kw={\"hspace\": 0})\n",
    "\n",
    "nonparam_target_covs = {\n",
    "    'binom': binom_target_covs,\n",
    "    'binom-corr': binom_corr_target_covs,\n",
    "    'signrank': signrank_target_covs,\n",
    "}\n",
    "colors = {\n",
    "    're_pm': 'blue',\n",
    "    'pdg': 'green',\n",
    "    'fe': 'grey',\n",
    "    'binom': 'black',\n",
    "    'binom-corr': 'red',\n",
    "    'signrank': 'red',\n",
    "}\n",
    "labels = {\n",
    "    'binom': 'ST',\n",
    "    'binom-corr': r\"ST$_{\\rho=0.1}$\",\n",
    "    'signrank': r\"SRT\",\n",
    "    're_pm': r\"Random Effects (PM)\",\n",
    "    'pdg': r\"Birge (PDG)\",\n",
    "    'fe': r\"Fixed Effect\",\n",
    "}\n",
    "linewidths = defaultdict(lambda: 3)\n",
    "linewidths['binom-corr'] = 1.5\n",
    "\n",
    "axs[0].set_ylabel(\"Coverage\")\n",
    "axs[1].set_ylabel(\"Coverage\")\n",
    "plt.xlabel(\"Number $n$ of studies in meta-analysis\")\n",
    "\n",
    "# plt.plot(target_covs.keys(), target_covs.values(), label='target', color='black', linestyle='--', linewidth=2)\n",
    "# print(binom_corr_target_covs[3])\n",
    "plt.xlim(2, 10)\n",
    "# axs[1].set_ylim(1.5, 4.2)\n",
    "for ax in axs:\n",
    "    ax.tick_params(direction=\"in\", top=True, right=True)\n",
    "    ax.set_ylim(0.3, 0.95)\n",
    "which_ax = {\n",
    "    'fe': 0,\n",
    "    're_pm': 0,\n",
    "    'pdg': 0,\n",
    "    'binom': 1,\n",
    "    'signrank': 1,\n",
    "    'binom-corr': 1,\n",
    "}\n",
    "axs[0].axhline(0.6827, color='black', linestyle='--', linewidth=1.5)\n",
    "# axs[0].text(\n",
    "#     7.5,\n",
    "#     0.6827+0.01,\n",
    "#     r\"Target coverage\",\n",
    "#     va=\"bottom\",\n",
    "#     ha=\"center\",\n",
    "# )\n",
    "for method in ['fe', 're_pm', 'pdg', 'binom', 'signrank']:\n",
    "    ax = axs[which_ax[method]]\n",
    "    ax.plot(ns, np.nanmean(method_covs[:, methods.index(method), :], axis=1),\n",
    "            label=labels[method],\n",
    "            color=colors[method],\n",
    "            linewidth=linewidths[method],\n",
    "        )\n",
    "    if method in ['binom', 'signrank', 'binom-corr']:\n",
    "        ax.plot(\n",
    "            nonparam_target_covs[method].keys(),\n",
    "            nonparam_target_covs[method].values(),\n",
    "            color=colors[method],\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "        )\n",
    "\n",
    "axs[0].legend(frameon=False, loc='upper left')\n",
    "axs[1].legend(frameon=False, loc='lower left')\n",
    "\n",
    "# plt.savefig(\"figs/pdg1970_cov_nonparam.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(\"figs/pdg1970_cov_poster.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha = 1 / 3\n",
    "sigma1 = np.linspace(0.1, 2.5, 1000)\n",
    "zalpha = norm.ppf(1 - alpha / 2)\n",
    "scores = 2 * sigma1 * zalpha + (4 / alpha) * (\n",
    "    norm.pdf(sigma1 * zalpha) / norm.cdf(-sigma1 * zalpha) - sigma1 * zalpha\n",
    ") * norm.cdf(-sigma1 * zalpha)\n",
    "covs = 1 - 2 * norm.cdf(-sigma1 * zalpha)\n",
    "plt.plot(sigma1, scores)\n",
    "plt.plot(sigma1, covs)\n",
    "plt.axvline(sigma1[np.argmin(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap over datasets\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "method_covs_boot_mean = np.nanmean(method_covs_ds_boot, axis=2)\n",
    "method_covs_mean = np.nanmean(method_covs, axis=(2, 3))\n",
    "uppers = 2 * method_covs_mean - np.nanquantile(method_covs_boot_mean, 0.1, axis=2)\n",
    "lowers = 2 * method_covs_mean - np.nanquantile(method_covs_boot_mean, 0.9, axis=2)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.plot(\n",
    "        ns,\n",
    "        np.nanmean(method_covs[:, i, :, :], axis=(1, 2)),\n",
    "        label=method,\n",
    "        color=colors[method],\n",
    "    )\n",
    "    plt.plot(ns, uppers[:, i], color=colors[method], linestyle=\":\", linewidth=0.5)\n",
    "    plt.plot(ns, lowers[:, i], color=colors[method], linestyle=\":\", linewidth=0.5)\n",
    "plt.plot(ns, target_covs, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(datasets[\"agi\"].value)\n",
    "sigma = np.array(datasets[\"agi\"].uncertainty)\n",
    "\n",
    "import pymc as pm\n",
    "\n",
    "print(y)\n",
    "n = len(y)\n",
    "width = np.max(y) - np.min(y)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    theta = pm.Normal(\"theta\", np.mean(y), np.mean(y))\n",
    "    # scaler_rate = pm.Exponential('scaler_rate', 0.2)\n",
    "    # scalers = pm.Exponential('scalers', scaler_rate, shape=n)+1\n",
    "    # y_pred = pm.Normal('y_pred', theta, sigma*scalers, observed=y)\n",
    "\n",
    "    # adder_rate = pm.Exponential('adder_rate', 10/width)\n",
    "    adders = pm.Exponential(\"adders\", 5 / width, shape=n)\n",
    "    y_pred = pm.Normal(\"y_pred\", theta, np.sqrt(sigma**2 + adders**2), observed=y)\n",
    "\n",
    "    trace = pm.sample(draws=1000, tune=1000, chains=4, target_accept=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    plt.hist(\n",
    "        trace.posterior[\"scalers\"].values[:, :, i].flatten() + 1,\n",
    "        bins=100,\n",
    "        histtype=\"step\",\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.posterior[\"scalers\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trace.posterior[\"theta\"].values.flatten(), bins=100)\n",
    "qs = np.quantile(trace.posterior[\"theta\"].values.flatten(), [0.16, 0.84])\n",
    "plt.axvline(qs[0], color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.axvline(qs[1], color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "ymax = plt.ylim()[1]\n",
    "if \"scalers\" in trace.posterior:\n",
    "    scaler_qs = (\n",
    "        np.quantile(trace.posterior[\"scalers\"].values, [0.25, 0.5, 0.75], axis=(0, 1))\n",
    "        + 1\n",
    "    )\n",
    "    print(scaler_qs.shape)\n",
    "    for i in range(3):\n",
    "        plt.errorbar(\n",
    "            y,\n",
    "            np.linspace(ymax / len(y), ymax, len(y)),\n",
    "            xerr=sigma * scaler_qs[i],\n",
    "            fmt=\".\",\n",
    "            markersize=2,\n",
    "            linewidth=1,\n",
    "            color=\"black\",\n",
    "            capsize=2,\n",
    "        )\n",
    "if \"adders\" in trace.posterior:\n",
    "    adder_qs = np.quantile(\n",
    "        trace.posterior[\"adders\"].values, [0.25, 0.5, 0.75], axis=(0, 1)\n",
    "    )\n",
    "    print(adder_qs.shape)\n",
    "    for i in range(3):\n",
    "        plt.errorbar(\n",
    "            y,\n",
    "            np.linspace(ymax / len(y), ymax, len(y)),\n",
    "            xerr=sigma + adder_qs[i],\n",
    "            fmt=\".\",\n",
    "            markersize=2,\n",
    "            linewidth=1,\n",
    "            color=\"black\",\n",
    "            capsize=2,\n",
    "        )\n",
    "# sigmas_adjust = sigma * np.median(trace.posterior['scalers'].values, axis=(0,1))\n",
    "\n",
    "plt.errorbar(\n",
    "    y,\n",
    "    np.linspace(ymax / len(y), ymax, len(y)),\n",
    "    xerr=sigma,\n",
    "    fmt=\".\",\n",
    "    markersize=2,\n",
    "    linewidth=2,\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# plt.ylim(0, ymax)\n",
    "# plt.xlim(qs[0], qs[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
